<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Supervised classification using naive Bayes — mlNaiveBayes • mlearning</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="152x152" href="../apple-touch-icon-152x152.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.1/css/all.min.css" integrity="sha256-PbSmjxuVAzJ6FPvNYsrXygfGhNJYyZ2GktDbkMBqQZg=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.1/css/v4-shims.min.css" integrity="sha256-A6jcAdwFD48VMjlI3GDxUd+eCQa7/KWy6G9oe/ovaPA=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/headroom.min.js" integrity="sha256-DJFC1kqIhelURkuza0AvYal5RxMtpzLjFhsnVIeuk+U=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.9.4/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><!-- svPkgdown --><script src="../script.js"></script><!-- themify icon --><link rel="stylesheet" href="../themify-icons.css?nocache=123"><!--<link rel="stylesheet" href="../style.css?nocache=123">--><!-- bootstrat v4.1.1, required CSS from more recent version --><link rel="stylesheet" href="../bootstrap.more.css?nocache=123"><link href="../style.css?nocache=123" rel="stylesheet"><!-- end of addition for svPkgdown --><meta property="og:title" content="Supervised classification using naive Bayes — mlNaiveBayes"><meta property="og:description" content="Unified (formula-based) interface version of the naive Bayes algorithm
provided by e1071::naiveBayes()."><meta name="twitter:card" content="summary"><!-- Mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js" async integrity="sha256-nlrDrBTHxJJlDDX22AS33xYI1OJHnGMDhiYMSe2U0e0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/config/TeX-AMS-MML_HTMLorMML.js" async integrity="sha256-4zys9A4hMQmtq2EUL+JRoXc0NZi8jVJMzb8onewOaSQ=" crossorigin="anonymous"></script><!-- Global site tag (gtag.js) - Google Analytics --><!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-462421-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-462421-8');
</script>--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <!-- preloader start -->
<div class="preloader">
  <img src="../preloader.gif" alt="loading..."></div>
<!-- preloader end -->

<div class="navbar navbar-default navbar-fixed-top bg-secondary" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <div class="navbar-brand-container">
       <div class="navbar-brand-logo">
         <a class="external-link navbar-brand" href="https://www.sciviews.org">
          <img src="../logo_full.png" alt="SciViews"></a>
       </div>
       <div class="navbar-brand-package">
      </div>
      </div>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <!-- start svPkgdown global site search with Google -->
      <form action="https://www.google.com/search" class="navbar-form navbar-right search-form-global">
        <div class="form-group has-search">
          <span class="fa fa-search form-control-feedback-global"></span>
          <input type="text" class="form-control-global search-form-input" name="q" id="q" placeholder="Search..." required><input type="hidden" name="sitesearch" value="https://www.sciviews.org"></div>
      </form>
      <!-- end svPkgdown global site search with Google -->
      <ul class="nav navbar-nav navbar-right"><li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
        <li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/SciViews/mlearning/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul><div class="navbar-title">
        <a class="navbar-brand" href="../index.html"><p class="h2">mlearning <small>v1.2.1</small></p></a>
      </div>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
  <div class="navbar-spacer bg-light">
  </div>
</div><!--/.navbar -->

    <div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Supervised classification using naive Bayes</h1>
      <small class="dont-index">Source: <a href="https://github.com/SciViews/mlearning/blob/HEAD/R/ml_naive_bayes.R" class="external-link"><code>R/ml_naive_bayes.R</code></a></small>
      <div class="d-none name"><code>mlNaiveBayes.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Unified (formula-based) interface version of the naive Bayes algorithm
provided by <code><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html" class="external-link">e1071::naiveBayes()</a></code>.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">mlNaiveBayes</span><span class="op">(</span><span class="va">train</span>, <span class="va">...</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ml_naive_bayes</span><span class="op">(</span><span class="va">train</span>, <span class="va">...</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for formula</span></span>
<span><span class="fu">mlNaiveBayes</span><span class="op">(</span><span class="va">formula</span>, <span class="va">data</span>, laplace <span class="op">=</span> <span class="fl">0</span>, <span class="va">...</span>, <span class="va">subset</span>, <span class="va">na.action</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for default</span></span>
<span><span class="fu">mlNaiveBayes</span><span class="op">(</span><span class="va">train</span>, <span class="va">response</span>, laplace <span class="op">=</span> <span class="fl">0</span>, <span class="va">...</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># S3 method for mlNaiveBayes</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span></span>
<span>  <span class="va">object</span>,</span>
<span>  <span class="va">newdata</span>,</span>
<span>  type <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"class"</span>, <span class="st">"membership"</span>, <span class="st">"both"</span><span class="op">)</span>,</span>
<span>  method <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"direct"</span>, <span class="st">"cv"</span><span class="op">)</span>,</span>
<span>  na.action <span class="op">=</span> <span class="va">na.exclude</span>,</span>
<span>  threshold <span class="op">=</span> <span class="fl">0.001</span>,</span>
<span>  eps <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>train</dt>
<dd><p>a matrix or data frame with predictors.</p></dd>


<dt>...</dt>
<dd><p>further arguments passed to the classification method or its
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> method (not used here for now).</p></dd>


<dt>formula</dt>
<dd><p>a formula with left term being the factor variable to predict
and the right term with the list of independent, predictive variables,
separated with a plus sign. If the data frame provided contains only the
dependent and independent variables, one can use the <code>class ~ .</code> short
version (that one is strongly encouraged). Variables with minus sign are
eliminated. Calculations on variables are possible according to usual formula
convention (possibly protected by using <code><a href="https://rdrr.io/r/base/AsIs.html" class="external-link">I()</a></code>).</p></dd>


<dt>data</dt>
<dd><p>a data.frame to use as a training set.</p></dd>


<dt>laplace</dt>
<dd><p>positive number controlling Laplace smoothing for the naive
Bayes classifier. The default (0) disables Laplace smoothing.</p></dd>


<dt>subset</dt>
<dd><p>index vector with the cases to define the training set in use
(this argument must be named, if provided).</p></dd>


<dt>na.action</dt>
<dd><p>function to specify the action to be taken if <code>NA</code>s are
found. For <code>ml_naive_bayes()</code> <code>na.fail</code> is used by default. The calculation is
stopped if there is any <code>NA</code> in the data. Another option is <code>na.omit</code>,
where cases with missing values on any required variable are dropped (this
argument must be named, if provided). For the <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> method, the
default, and most suitable option, is <code>na.exclude</code>. In that case, rows with
<code>NA</code>s in <code>newdata=</code> are excluded from prediction, but reinjected in the
final results so that the number of items is still the same (and in the
same order as <code>newdata=</code>).</p></dd>


<dt>response</dt>
<dd><p>a vector of factor with the classes.</p></dd>


<dt>object</dt>
<dd><p>an <strong>mlNaiveBayes</strong> object</p></dd>


<dt>newdata</dt>
<dd><p>a new dataset with same conformation as the training set (same
variables, except may by the class for classification or dependent variable
for regression). Usually a test set, or a new dataset to be predicted.</p></dd>


<dt>type</dt>
<dd><p>the type of prediction to return. <code>"class"</code> by default, the
predicted classes. Other options are <code>"membership"</code>, the posterior
probability or <code>"both"</code> to return classes and memberships,</p></dd>


<dt>method</dt>
<dd><p><code>"direct"</code> (default) or <code>"cv"</code>. <code>"direct"</code> predicts new cases in
<code>newdata=</code> if this argument is provided, or the cases in the training set
if not. Take care that not providing <code>newdata=</code> means that you just
calculate the <strong>self-consistency</strong> of the classifier but cannot use the
metrics derived from these results for the assessment of its performances.
Either use a different dataset in <code>newdata=</code> or use the alternate
cross-validation ("cv") technique. If you specify <code>method = "cv"</code> then
<code><a href="mlearning.html">cvpredict()</a></code> is used and you cannot provide <code>newdata=</code> in that case.</p></dd>


<dt>threshold</dt>
<dd><p>value replacing cells with probabilities within 'eps' range.</p></dd>


<dt>eps</dt>
<dd><p>number for specifying an epsilon-range to apply Laplace smoothing
(to replace zero or close-zero probabilities by 'threshold').</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p><code>ml_naive_bayes()</code>/<code>mlNaiveBayes()</code> creates an <strong>mlNaiveBayes</strong>,
<strong>mlearning</strong> object containing the classifier and a lot of additional
metadata used by the functions and methods you can apply to it like
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> or <code><a href="mlearning.html">cvpredict()</a></code>. In case you want to program new functions or
extract specific components, inspect the "unclassed" object using
<code><a href="https://rdrr.io/r/base/class.html" class="external-link">unclass()</a></code>.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p><code><a href="mlearning.html">mlearning()</a></code>, <code><a href="mlearning.html">cvpredict()</a></code>, <code><a href="confusion.html">confusion()</a></code>, also
<code><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html" class="external-link">e1071::naiveBayes()</a></code> that actually does the classification.</p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="co"># Prepare data: split into training set (2/3) and test set (1/3)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"iris"</span>, package <span class="op">=</span> <span class="st">"datasets"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">34</span>, <span class="fl">51</span><span class="op">:</span><span class="fl">83</span>, <span class="fl">101</span><span class="op">:</span><span class="fl">133</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">iris_train</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="va">train</span>, <span class="op">]</span></span></span>
<span class="r-in"><span><span class="va">iris_test</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>, <span class="op">]</span></span></span>
<span class="r-in"><span><span class="co"># One case with missing data in train set, and another case in test set</span></span></span>
<span class="r-in"><span><span class="va">iris_train</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span></span>
<span class="r-in"><span><span class="va">iris_test</span><span class="op">[</span><span class="fl">25</span>, <span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">NA</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">iris_nb</span> <span class="op">&lt;-</span> <span class="fu">ml_naive_bayes</span><span class="op">(</span>data <span class="op">=</span> <span class="va">iris_train</span>, <span class="va">Species</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">iris_nb</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> A mlearning object of class mlNaiveBayes (naive Bayes classifier):</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Initial call: mlNaiveBayes.formula(formula = Species ~ ., data = iris_train)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Naive Bayes Classifier for Discrete Predictors</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Call:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> naiveBayes.default(x = train, y = response, laplace = laplace, </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>     .args. = ..1)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> A-priori probabilities:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>     setosa versicolor  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  0.3333333  0.3333333  0.3333333 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Conditional probabilities:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             Sepal.Length</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response         [,1]      [,2]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   setosa     5.048485 0.3725933</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   versicolor 6.027273 0.5392545</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   virginica  6.642424 0.7088857</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             Sepal.Width</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response         [,1]      [,2]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   setosa     3.478788 0.3805897</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   versicolor 2.763636 0.3267436</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   virginica  2.951515 0.3545430</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             Petal.Length</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response         [,1]      [,2]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   setosa     1.478788 0.1781109</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   versicolor 4.284848 0.4651083</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   virginica  5.642424 0.6179757</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             Petal.Width</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response          [,1]      [,2]</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   setosa     0.2454545 0.1033529</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   versicolor 1.3303030 0.2157615</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   virginica  2.0090909 0.2466825</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">iris_nb</span><span class="op">)</span> <span class="co"># Default type is class</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [1] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [7] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [13] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [19] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [25] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [31] setosa     setosa     setosa     versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [37] versicolor versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [43] versicolor versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [49] versicolor versicolor versicolor versicolor versicolor virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [55] versicolor versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [61] virginica  versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [67] virginica  virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [73] versicolor virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [79] virginica  virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [85] virginica  versicolor virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [91] virginica  virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [97] virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Levels: setosa versicolor virginica</span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">iris_nb</span>, type <span class="op">=</span> <span class="st">"membership"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>               setosa   versicolor    virginica</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [1,]  1.000000e+00 1.803036e-16 1.167697e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [2,]  1.000000e+00 1.352715e-17 1.831338e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [3,]  1.000000e+00 1.772241e-16 1.353873e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [4,]  1.000000e+00 5.692804e-18 1.376409e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [5,]  1.000000e+00 6.093062e-14 6.531376e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [6,]  1.000000e+00 8.826228e-17 2.837739e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [7,]  1.000000e+00 7.437194e-17 8.757620e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [8,]  1.000000e+00 1.216747e-16 9.720500e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [9,]  1.000000e+00 6.724341e-17 2.531533e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [10,]  1.000000e+00 4.566759e-17 1.186716e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [11,]  1.000000e+00 1.879913e-16 1.981529e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [12,]  1.000000e+00 2.686953e-17 1.085926e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [13,]  1.000000e+00 1.650638e-18 2.251328e-26</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [14,]  1.000000e+00 2.368402e-18 3.908159e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [15,]  1.000000e+00 1.761543e-16 1.792568e-22</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [16,]  1.000000e+00 2.706070e-16 6.524103e-23</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [17,]  1.000000e+00 1.684941e-16 5.447395e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [18,]  1.000000e+00 2.886607e-14 1.441825e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [19,]  1.000000e+00 6.739186e-17 4.013941e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [20,]  1.000000e+00 9.227945e-15 7.619346e-23</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [21,]  1.000000e+00 3.076912e-15 2.554200e-22</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [22,]  1.000000e+00 2.412671e-19 2.082673e-26</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [23,]  1.000000e+00 5.291974e-11 2.164940e-18</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [24,]  1.000000e+00 8.216570e-14 5.480528e-22</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [25,]  1.000000e+00 3.650319e-15 1.524151e-23</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [26,]  1.000000e+00 7.652882e-14 2.445210e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [27,]  1.000000e+00 7.812216e-17 1.149116e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [28,]  1.000000e+00 4.476095e-17 6.365960e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [29,]  1.000000e+00 5.252216e-16 3.756987e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [30,]  1.000000e+00 1.184327e-15 6.488869e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [31,]  1.000000e+00 8.338509e-14 3.263362e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [32,]  1.000000e+00 1.690882e-19 8.721554e-27</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [33,]  1.000000e+00 4.316419e-19 7.033715e-26</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [34,] 7.689655e-103 9.159085e-01 8.409150e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [35,]  1.096673e-96 9.667123e-01 3.328772e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [36,] 1.930514e-116 6.802159e-01 3.197841e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [37,]  1.102081e-67 9.999412e-01 5.879818e-05</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [38,] 3.602267e-102 9.709066e-01 2.909345e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [39,]  3.335613e-86 9.993121e-01 6.878536e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [40,] 1.129911e-109 7.783585e-01 2.216415e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [41,]  2.982269e-33 9.999994e-01 5.938056e-07</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [42,]  4.546204e-93 9.957187e-01 4.281285e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [43,]  2.245740e-67 9.998022e-01 1.977854e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [44,]  1.295311e-39 9.999994e-01 6.198654e-07</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [45,]  8.639091e-84 9.962009e-01 3.799095e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [46,]  1.306052e-57 9.999962e-01 3.800063e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [47,] 5.129110e-100 9.913456e-01 8.654381e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [48,]  1.247004e-53 9.999529e-01 4.710380e-05</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [49,]  2.751640e-89 9.894678e-01 1.053225e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [50,]  9.204587e-95 9.910781e-01 8.921933e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [51,]  1.056753e-59 9.999929e-01 7.109959e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [52,]  4.759450e-98 9.939246e-01 6.075414e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [53,]  4.172627e-56 9.999931e-01 6.930611e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [54,] 1.147499e-124 2.598343e-01 7.401657e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [55,]  2.430851e-68 9.998174e-01 1.826315e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [56,] 3.305367e-115 9.485689e-01 5.143106e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [57,]  1.322349e-91 9.991780e-01 8.219604e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [58,]  3.383621e-80 9.990860e-01 9.139928e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [59,]  4.018503e-89 9.929331e-01 7.066888e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [60,] 2.885304e-107 9.596243e-01 4.037565e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [61,] 1.500417e-131 1.709929e-01 8.290071e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [62,]  4.494759e-96 9.893143e-01 1.068568e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [63,]  5.408043e-40 9.999988e-01 1.216458e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [64,]  1.002758e-52 9.999955e-01 4.522882e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [65,]  5.628947e-46 9.999987e-01 1.332260e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [66,]  4.602589e-60 9.999711e-01 2.887912e-05</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [67,] 4.079871e-244 3.576071e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [68,] 3.689196e-146 5.139596e-02 9.486040e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [69,] 4.834773e-210 1.388701e-06 9.999986e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [70,] 1.096501e-167 4.868244e-03 9.951318e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [71,] 1.614505e-208 2.397766e-06 9.999976e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [72,] 1.416791e-258 1.522784e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [73,] 4.215784e-105 9.531740e-01 4.682597e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [74,] 1.247937e-215 3.770575e-06 9.999962e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [75,] 1.109378e-181 1.112506e-03 9.988875e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [76,] 1.716419e-254 1.088139e-10 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [77,] 4.973153e-155 2.017456e-03 9.979825e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [78,] 1.101493e-158 8.236383e-03 9.917636e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [79,] 3.008552e-185 3.859199e-05 9.999614e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [80,] 3.744500e-148 2.931546e-02 9.706845e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [81,] 3.347013e-184 2.234062e-05 9.999777e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [82,] 8.606627e-188 8.413290e-06 9.999916e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [83,] 7.840619e-163 5.273864e-03 9.947261e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [84,] 1.759992e-272 1.484798e-11 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [85,] 8.472771e-297 6.900804e-12 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [86,] 2.360925e-119 9.577011e-01 4.229886e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [87,] 2.229069e-212 2.646252e-07 9.999997e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [88,] 1.285230e-142 3.410444e-02 9.658956e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [89,] 2.856722e-259 2.856759e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [90,] 1.186200e-131 2.371171e-01 7.628829e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [91,] 1.619220e-195 7.614926e-06 9.999924e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [92,] 1.704706e-195 2.566160e-05 9.999743e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [93,] 1.903352e-126 3.334420e-01 6.665580e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [94,] 2.236975e-130 2.044628e-01 7.955372e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [95,] 3.459428e-189 6.688505e-05 9.999331e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [96,] 1.646362e-171 1.997026e-03 9.980030e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [97,] 2.796148e-210 3.580998e-06 9.999964e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [98,] 1.557094e-238 1.596656e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [99,] 7.874332e-197 1.449503e-05 9.999855e-01</span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">iris_nb</span>, type <span class="op">=</span> <span class="st">"both"</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $class</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [1] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [7] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [13] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [19] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [25] setosa     setosa     setosa     setosa     setosa     setosa    </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [31] setosa     setosa     setosa     versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [37] versicolor versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [43] versicolor versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [49] versicolor versicolor versicolor versicolor versicolor virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [55] versicolor versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [61] virginica  versicolor versicolor versicolor versicolor versicolor</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [67] virginica  virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [73] versicolor virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [79] virginica  virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [85] virginica  versicolor virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [91] virginica  virginica  virginica  virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [97] virginica  virginica  virginica </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Levels: setosa versicolor virginica</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> $membership</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>               setosa   versicolor    virginica</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [1,]  1.000000e+00 1.803036e-16 1.167697e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [2,]  1.000000e+00 1.352715e-17 1.831338e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [3,]  1.000000e+00 1.772241e-16 1.353873e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [4,]  1.000000e+00 5.692804e-18 1.376409e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [5,]  1.000000e+00 6.093062e-14 6.531376e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [6,]  1.000000e+00 8.826228e-17 2.837739e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [7,]  1.000000e+00 7.437194e-17 8.757620e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [8,]  1.000000e+00 1.216747e-16 9.720500e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   [9,]  1.000000e+00 6.724341e-17 2.531533e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [10,]  1.000000e+00 4.566759e-17 1.186716e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [11,]  1.000000e+00 1.879913e-16 1.981529e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [12,]  1.000000e+00 2.686953e-17 1.085926e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [13,]  1.000000e+00 1.650638e-18 2.251328e-26</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [14,]  1.000000e+00 2.368402e-18 3.908159e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [15,]  1.000000e+00 1.761543e-16 1.792568e-22</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [16,]  1.000000e+00 2.706070e-16 6.524103e-23</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [17,]  1.000000e+00 1.684941e-16 5.447395e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [18,]  1.000000e+00 2.886607e-14 1.441825e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [19,]  1.000000e+00 6.739186e-17 4.013941e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [20,]  1.000000e+00 9.227945e-15 7.619346e-23</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [21,]  1.000000e+00 3.076912e-15 2.554200e-22</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [22,]  1.000000e+00 2.412671e-19 2.082673e-26</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [23,]  1.000000e+00 5.291974e-11 2.164940e-18</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [24,]  1.000000e+00 8.216570e-14 5.480528e-22</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [25,]  1.000000e+00 3.650319e-15 1.524151e-23</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [26,]  1.000000e+00 7.652882e-14 2.445210e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [27,]  1.000000e+00 7.812216e-17 1.149116e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [28,]  1.000000e+00 4.476095e-17 6.365960e-25</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [29,]  1.000000e+00 5.252216e-16 3.756987e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [30,]  1.000000e+00 1.184327e-15 6.488869e-24</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [31,]  1.000000e+00 8.338509e-14 3.263362e-21</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [32,]  1.000000e+00 1.690882e-19 8.721554e-27</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [33,]  1.000000e+00 4.316419e-19 7.033715e-26</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [34,] 7.689655e-103 9.159085e-01 8.409150e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [35,]  1.096673e-96 9.667123e-01 3.328772e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [36,] 1.930514e-116 6.802159e-01 3.197841e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [37,]  1.102081e-67 9.999412e-01 5.879818e-05</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [38,] 3.602267e-102 9.709066e-01 2.909345e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [39,]  3.335613e-86 9.993121e-01 6.878536e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [40,] 1.129911e-109 7.783585e-01 2.216415e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [41,]  2.982269e-33 9.999994e-01 5.938056e-07</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [42,]  4.546204e-93 9.957187e-01 4.281285e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [43,]  2.245740e-67 9.998022e-01 1.977854e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [44,]  1.295311e-39 9.999994e-01 6.198654e-07</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [45,]  8.639091e-84 9.962009e-01 3.799095e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [46,]  1.306052e-57 9.999962e-01 3.800063e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [47,] 5.129110e-100 9.913456e-01 8.654381e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [48,]  1.247004e-53 9.999529e-01 4.710380e-05</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [49,]  2.751640e-89 9.894678e-01 1.053225e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [50,]  9.204587e-95 9.910781e-01 8.921933e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [51,]  1.056753e-59 9.999929e-01 7.109959e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [52,]  4.759450e-98 9.939246e-01 6.075414e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [53,]  4.172627e-56 9.999931e-01 6.930611e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [54,] 1.147499e-124 2.598343e-01 7.401657e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [55,]  2.430851e-68 9.998174e-01 1.826315e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [56,] 3.305367e-115 9.485689e-01 5.143106e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [57,]  1.322349e-91 9.991780e-01 8.219604e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [58,]  3.383621e-80 9.990860e-01 9.139928e-04</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [59,]  4.018503e-89 9.929331e-01 7.066888e-03</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [60,] 2.885304e-107 9.596243e-01 4.037565e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [61,] 1.500417e-131 1.709929e-01 8.290071e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [62,]  4.494759e-96 9.893143e-01 1.068568e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [63,]  5.408043e-40 9.999988e-01 1.216458e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [64,]  1.002758e-52 9.999955e-01 4.522882e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [65,]  5.628947e-46 9.999987e-01 1.332260e-06</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [66,]  4.602589e-60 9.999711e-01 2.887912e-05</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [67,] 4.079871e-244 3.576071e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [68,] 3.689196e-146 5.139596e-02 9.486040e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [69,] 4.834773e-210 1.388701e-06 9.999986e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [70,] 1.096501e-167 4.868244e-03 9.951318e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [71,] 1.614505e-208 2.397766e-06 9.999976e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [72,] 1.416791e-258 1.522784e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [73,] 4.215784e-105 9.531740e-01 4.682597e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [74,] 1.247937e-215 3.770575e-06 9.999962e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [75,] 1.109378e-181 1.112506e-03 9.988875e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [76,] 1.716419e-254 1.088139e-10 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [77,] 4.973153e-155 2.017456e-03 9.979825e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [78,] 1.101493e-158 8.236383e-03 9.917636e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [79,] 3.008552e-185 3.859199e-05 9.999614e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [80,] 3.744500e-148 2.931546e-02 9.706845e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [81,] 3.347013e-184 2.234062e-05 9.999777e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [82,] 8.606627e-188 8.413290e-06 9.999916e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [83,] 7.840619e-163 5.273864e-03 9.947261e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [84,] 1.759992e-272 1.484798e-11 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [85,] 8.472771e-297 6.900804e-12 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [86,] 2.360925e-119 9.577011e-01 4.229886e-02</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [87,] 2.229069e-212 2.646252e-07 9.999997e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [88,] 1.285230e-142 3.410444e-02 9.658956e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [89,] 2.856722e-259 2.856759e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [90,] 1.186200e-131 2.371171e-01 7.628829e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [91,] 1.619220e-195 7.614926e-06 9.999924e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [92,] 1.704706e-195 2.566160e-05 9.999743e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [93,] 1.903352e-126 3.334420e-01 6.665580e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [94,] 2.236975e-130 2.044628e-01 7.955372e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [95,] 3.459428e-189 6.688505e-05 9.999331e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [96,] 1.646362e-171 1.997026e-03 9.980030e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [97,] 2.796148e-210 3.580998e-06 9.999964e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [98,] 1.557094e-238 1.596656e-09 1.000000e+00</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  [99,] 7.874332e-197 1.449503e-05 9.999855e-01</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-in"><span><span class="co"># Self-consistency, do not use for assessing classifier performances!</span></span></span>
<span class="r-in"><span><span class="fu"><a href="confusion.html">confusion</a></span><span class="op">(</span><span class="va">iris_nb</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 99 items classified with 95 true positives (error rate = 4%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>                Predicted</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Actual          01 02 03 (sum) (FNR%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   01 setosa     33  0  0    33      0</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   02 versicolor  0 31  2    33      6</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   03 virginica   0  2 31    33      6</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   (sum)         33 33 33    99      4</span>
<span class="r-in"><span><span class="co"># Use an independent test set instead</span></span></span>
<span class="r-in"><span><span class="fu"><a href="confusion.html">confusion</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">iris_nb</span>, newdata <span class="op">=</span> <span class="va">iris_test</span><span class="op">)</span>, <span class="va">iris_test</span><span class="op">$</span><span class="va">Species</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 50 items classified with 47 true positives (error rate = 6%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>                Predicted</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Actual          01 02 03 04 (sum) (FNR%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   01 setosa     16  0  0  0    16      0</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   02 NA          0  0  0  0     0       </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   03 versicolor  0  1 16  0    17      6</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   04 virginica   0  0  2 15    17     12</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   (sum)         16  1 18 15    50      6</span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Another dataset</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"HouseVotes84"</span>, package <span class="op">=</span> <span class="st">"mlbench"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">house_nb</span> <span class="op">&lt;-</span> <span class="fu">ml_naive_bayes</span><span class="op">(</span>data <span class="op">=</span> <span class="va">HouseVotes84</span>, <span class="va">Class</span> <span class="op">~</span> <span class="va">.</span>,</span></span>
<span class="r-in"><span>  na.action <span class="op">=</span> <span class="va">na.omit</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">house_nb</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> A mlearning object of class mlNaiveBayes (naive Bayes classifier):</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Initial call: mlNaiveBayes.formula(formula = Class ~ ., data = HouseVotes84,     na.action = na.omit)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Naive Bayes Classifier for Discrete Predictors</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Call:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> naiveBayes.default(x = train, y = response, laplace = laplace, </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>     .args. = ..1)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> A-priori probabilities:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat republican </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>  0.5344828  0.4655172 </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Conditional probabilities:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V1</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.4112903 0.5887097</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.7870370 0.2129630</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V2</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.5483871 0.4516129</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.5277778 0.4722222</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V3</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.1451613 0.8548387</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.8425926 0.1574074</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V4</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response               n           y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.951612903 0.048387097</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.009259259 0.990740741</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V5</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.7983871 0.2016129</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.0462963 0.9537037</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V6</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.5564516 0.4435484</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.1296296 0.8703704</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V7</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.2338710 0.7661290</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.7314815 0.2685185</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V8</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.1693548 0.8306452</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.8518519 0.1481481</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V9</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.2096774 0.7903226</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.8611111 0.1388889</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V10</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.4677419 0.5322581</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.4259259 0.5740741</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V11</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.4919355 0.5080645</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.8425926 0.1574074</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V12</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.8709677 0.1290323</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.1481481 0.8518519</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V13</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.7096774 0.2903226</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.1574074 0.8425926</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V14</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response              n          y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.65322581 0.34677419</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.01851852 0.98148148</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V15</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response             n         y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.4032258 0.5967742</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.8888889 0.1111111</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span>             V16</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> response              n          y</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   democrat   0.05645161 0.94354839</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   republican 0.33333333 0.66666667</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-in"><span><span class="fu"><a href="confusion.html">confusion</a></span><span class="op">(</span><span class="va">house_nb</span><span class="op">)</span> <span class="co"># Self-consistency</span></span></span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 232 items classified with 218 true positives (error rate = 6%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>                Predicted</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Actual           01  02 (sum) (FNR%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   01 democrat   111  13   124     10</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   02 republican   1 107   108      1</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   (sum)         112 120   232      6</span>
<span class="r-in"><span><span class="fu"><a href="confusion.html">confusion</a></span><span class="op">(</span><span class="fu"><a href="mlearning.html">cvpredict</a></span><span class="op">(</span><span class="va">house_nb</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/na.fail.html" class="external-link">na.omit</a></span><span class="op">(</span><span class="va">HouseVotes84</span><span class="op">)</span><span class="op">$</span><span class="va">Class</span><span class="op">)</span></span></span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data?</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> 232 items classified with 218 true positives (error rate = 6%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>                Predicted</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Actual           01  02 (sum) (FNR%)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   01 democrat   111  13   124     10</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   02 republican   1 107   108      1</span>
<span class="r-out co"><span class="r-pr">#&gt;</span>   (sum)         112 120   232      6</span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><!-- footer --><!--<footer class="bg-secondary">--><div class="bg-secondary full-width">
    <div class="py-100 border-bottom" style="border-color: #454547 !important">
        <div class="container">
            <div class="row">
                <div class="col-lg-4 col-md-4">
                    <div class="mb-5 mb-md-0 text-center text-md-left">
                        <!-- logo -->
                        <img class="mb-30" src="https://www.sciviews.org/images/logo-footer.png" alt="logo"><p class="text-white mb-30"> We build software for reproducible research with R</p>
                        <!-- social icon -->
                        <ul class="list-inline"><li class="list-inline-item">
                                <a class="external-link social-icon-outline" href="https://github.com/SciViews">
                                    <i class="ti-github"></i>
                                </a>
                            </li>

                            <li class="list-inline-item">
                                <a class="external-link social-icon-outline" href="https://twitter.com/PhilGrosjean">
                                    <i class="ti-twitter-alt"></i>
                                </a>
                            </li>

                            <li class="list-inline-item">
                                <a class="external-link social-icon-outline" href="https://www.linkedin.com/in/philippe-grosjean-5b6b29a2/">
                                    <i class="ti-linkedin"></i>
                                </a>
                            </li>

                        </ul></div>
                </div>
                <!-- footer links -->
                <div class="col-lg-2 col-md-4 col-6">
                    <p class="h4 text-white mb-4">Related</p>
                    <ul class="footer-links"><li>
                            <a href="http://econum.umons.ac.be" class="external-link">Our Lab</a>
                        </li>

                        <li>
                            <a href="https://web.umons.ac.be/fs/en/" class="external-link">Our Faculty</a>
                        </li>

                        <li>
                            <a href="https://web.umons.ac.be/complexys/en/" class="external-link">Complexys</a>
                        </li>

                        <li>
                            <a href="https://web.umons.ac.be/infortech/en/" class="external-link">InforTech</a>
                        </li>

                        <li>
                            <a href="https://web.umons.ac.be/en/" class="external-link">UMONS</a>
                        </li>

                    </ul></div>
                <!-- footer links -->
                <div class="col-lg-2 col-md-4 col-6">
                    <p class="h4 text-white mb-4">Quick Link</p>
                    <ul class="footer-links"><li>
                            <a href="https://www.sciviews.org/software/svbox" class="external-link">SciViews Box</a>
                        </li>

                        <li>
                            <a href="https://www.sciviews.org/software/sciviews-r" class="external-link">SciViews::R</a>
                        </li>

                        <li>
                            <a href="https://www.sciviews.org/software/zooimage" class="external-link">Zoo/PhytoImage</a>
                        </li>

                        <li>
                            <a href="https://www.sciviews.org/blog" class="external-link">Blog</a>
                        </li>

                        <li>
                            <a href="https://www.sciviews.org/about" class="external-link">About</a>
                        </li>

                    </ul></div>
                <!-- subscribe form -->
                <div class="col-lg-3 col-md-12 offset-lg-1">
                    <div class="mt-5 mt-lg-0 text-center text-md-left">
                             <p class="h4 mb-4 text-white">Contact Us</p>
                             <p class="text-white mb-4">Need more info?<br><a href="mailto:info@sciviews.org"><span class="text-primary">Leave us a mail...</span></a></p>

                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- copyright -->
    <div class="pt-4 pb-3 position-relative">
        <div class="container">
            <div class="row">
                <div class="col-lg-6 col-md-5">
                    <p class="text-white text-center text-md-left">
                        <span class="text-primary">SciViews</span> © All Right Reserved</p>
                </div>
                <div class="col-lg-6 col-md-7">
                    <ul class="list-inline text-center text-md-right"><li class="list-inline-item mx-lg-3 my-lg-0 mx-2 my-2">
                            <a class="external-link font-secondary text-white" href="https://creativecommons.org/licenses/by/4.0/">Site content under CC-BY 4.0 license</a>
                        </li>

                    </ul></div>
            </div>
        </div>
        <!-- back to top -->
        <!--<button class="back-to-top">
            <i class="ti-angle-up"></i>
        </button>-->
    </div>
</div>
<!--</footer>-->
<!-- /footer -->

    </footer></div>

  

  

  </body></html>

