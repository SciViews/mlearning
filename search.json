[{"path":[]},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement phgrosjean@sciviews.org. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://www.sciviews.org/mlearning/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://www.sciviews.org/mlearning/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://www.sciviews.org/mlearning/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"https://www.sciviews.org/mlearning/LICENSE.html","id":"gnu-general-public-license-1","dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Terms conditions copying, distributing modification (0.) License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . (1.) may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. (2.) may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: must cause modified files carry prominent notices stating changed files date change. must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. (3.) may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. (4.) may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. (5.) required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . (6.) time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. (7.) , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. (8.) distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. (9.) Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. (10.) wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"https://www.sciviews.org/mlearning/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"No warranty","title":"GNU General Public License","text":"(11.) program licensed free charge, warranty program, extent permitted applicable law. Except otherwise stated writing copyright holders /parties provide program “” without warranty kind, either expressed implied, including, limited , implied warranties merchantability fitness particular purpose. entire risk quality performance program . program prove defective, assume cost necessary servicing, repair correction. (12.) event unless required applicable law agreed writing copyright holder, party may modify /redistribute program permitted , liable damages, including general, special, incidental consequential damages arising use inability use program (including limited loss data data rendered inaccurate losses sustained third parties failure program operate programs), even holder party advised possibility damages.","code":""},{"path":"https://www.sciviews.org/mlearning/LICENSE.html","id":"end-of-terms-and-conditions","dir":"","previous_headings":"","what":"End of terms and conditions","title":"GNU General Public License","text":"Apply Terms New Programs develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. <one line give program’s name brief idea .> Copyright (C) program free software; can redistribute /modify terms GNU General Public License published Free Software Foundation; either version 2 License, (option) later version. program distributed hope useful, WITHOUT WARRANTY; without even implied warranty MERCHANTABILITY FITNESS PARTICULAR PURPOSE. See GNU General Public License details. received copy GNU General Public License along program; , write Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: Gnomovision version 69, Copyright (C) year name author Gnomovision comes ABSOLUTELY WARRANTY; details type ‘show w’. free software, welcome redistribute certain conditions; type ‘show c’ details. hypothetical commands ‘show w’ ‘show c’ show appropriate parts General Public License. course, commands use may called something ‘show w’ ‘show c’; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: Yoyodyne, Inc., hereby disclaims copyright interest program ‘Gnomovision’ (makes passes compilers) written James Hacker. <signature Ty Coon>, 1 April 1989 Ty Coon, President Vice General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":""},{"path":"https://www.sciviews.org/mlearning/TODO.html","id":null,"dir":"","previous_headings":"","what":"mlearning To Do list","title":"mlearning To Do list","text":"Vignette. Use lattice ggplot2 plots instead base plots. Parallel versions, like ranger <-> randomForest. Tests.","code":""},{"path":"https://www.sciviews.org/mlearning/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Philippe Grosjean. Author, maintainer. Kevin Denis. Author.","code":""},{"path":"https://www.sciviews.org/mlearning/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Grosjean P, Denis K (2023). mlearning: Machine Learning Algorithms Unified Interface Confusion Matrices. R package version 1.2.1, https://www.sciviews.org/mlearning/.","code":"@Manual{,   title = {mlearning: Machine Learning Algorithms with Unified Interface and Confusion Matrices},   author = {Philippe Grosjean and Kevin Denis},   year = {2023},   note = {R package version 1.2.1},   url = {https://www.sciviews.org/mlearning/}, }"},{"path":"https://www.sciviews.org/mlearning/index.html","id":null,"dir":"","previous_headings":"","what":"Machine Learning Algorithms with Unified Interface and Confusion Matrices","title":"Machine Learning Algorithms with Unified Interface and Confusion Matrices","text":"unified interface provided various machine learning algorithms like linear quadratic discriminant analysis, k-nearest neighbor, learning vector quantization, random forest, support vector machine, … allows train, test, apply cross-validation using similar functions function arguments minimalist clean, formula-based interface. Missing data processed way base stats R functions algorithms, training testing. Confusion matrices also provided rich set metrics calculated specific plots.","code":""},{"path":"https://www.sciviews.org/mlearning/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Machine Learning Algorithms with Unified Interface and Confusion Matrices","text":"can install released version {mlearning} CRAN : can also install latest development version. Make sure {remotes} R package installed: Use install_github() install {mlearning} package GitHub (source master branch recompiled machine): R install required dependencies automatically, compile install {mlearning}. Latest devel version {mlearning} (source + Windows binaries latest stable version R time compilation) also available appveyor.","code":"install.packages(\"mlearning\") install.packages(\"remotes\") remotes::install_github(\"SciViews/mlearning\")"},{"path":"https://www.sciviews.org/mlearning/index.html","id":"further-explore-mlearning","dir":"","previous_headings":"","what":"Further explore {mlearning}","title":"Machine Learning Algorithms with Unified Interface and Confusion Matrices","text":"can get help package way: Make {mlearning} package available R session: Get help package: instructions, please, refer help pages https://www.sciviews.org/mlearning/.","code":"library(\"mlearning\") library(help = \"mlearning\") help(\"mlearning-package\")"},{"path":"https://www.sciviews.org/mlearning/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Machine Learning Algorithms with Unified Interface and Confusion Matrices","text":"Please note {mlearning} package released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/confusion.html","id":null,"dir":"Reference","previous_headings":"","what":"Construct and analyze confusion matrices — confusion","title":"Construct and analyze confusion matrices — confusion","text":"Confusion matrices compare two classifications (usually one done automatically using machine learning algorithm versus true classification done specialist... one can also compare two automatic two manual classifications ).","code":""},{"path":"https://www.sciviews.org/mlearning/reference/confusion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Construct and analyze confusion matrices — confusion","text":"","code":"confusion(x, ...)  # S3 method for default confusion(   x,   y = NULL,   vars = c(\"Actual\", \"Predicted\"),   labels = vars,   merge.by = \"Id\",   useNA = \"ifany\",   prior,   ... )  # S3 method for mlearning confusion(   x,   y = response(x),   labels = c(\"Actual\", \"Predicted\"),   useNA = \"ifany\",   prior,   ... )  # S3 method for confusion print(x, sums = TRUE, error.col = sums, digits = 0, sort = \"ward.D2\", ...)  # S3 method for confusion summary(object, type = \"all\", sort.by = \"Fscore\", decreasing = TRUE, ...)  # S3 method for summary.confusion print(x, ...)"},{"path":"https://www.sciviews.org/mlearning/reference/confusion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Construct and analyze confusion matrices — confusion","text":"x object confusion() method implemented. ... arguments passed method. y another object, extract second classification, NULL used. vars variables interest first second classification case objects lists data frames. Otherwise, argument ignored x y must factors length levels. labels labels use two classifications. default, vars, one confusion matrix. merge.character string name variables use merge two data frames, NULL. useNA keep NAs separate category? default \"ifany\" creates category missing values. possibilities \"\", \"always\". prior class frequencies use first classifier tabulated rows confusion matrix. value, see , value= argument. sums confusion matrix printed rows columns sums? error.col column class error first classifier added (equivalent false negative rate FNR)? digits number digits decimal point print confusion matrix. default zero leads compact presentation suitable frequencies, relative frequencies. sort rows columns confusion matrix sorted classes larger confusion closer together? Sorting done using hierarchical clustering hclust(). clustering method \"ward.D2\" default, see hclust() help options). FALSE NULL, sorting done. object confusion object type either \"\" (default), considering TP true positives, FP false positives, TN true negatives FN false negatives, one can also specify: \"Fscore\" (F-score = F-measure = F1 score = harmonic mean Precision recall), \"Recall\" (TP / (TP + FN) = 1 - FNR), \"Precision\" (TP / (TP + FP) = 1 - FDR), \"Specificity\" (TN / (TN + FP) = 1 - FPR), \"NPV\" (Negative predicted value = TN / (TN + FN) = 1 - ), \"FPR\" (False positive rate = 1 - Specificity = FP / (FP + TN)), \"FNR\" (False negative rate = 1 - Recall = FN / (TP + FN)), \"FDR\" (False Discovery Rate = 1 - Precision = FP / (TP + FP)), \"\" (False omission rate = 1 - NPV = FN / (FN + TN)), \"LRPT\" (Likelihood Ratio Positive Tests = Recall / FPR = Recall / (1 - Specificity)), \"LRNT\" Likelihood Ratio Negative Tests = FNR / Specificity = (1 - Recall) / Specificity, \"LRPS\" (Likelihood Ratio Positive Subjects = Precision / = Precision / (1 - NPV)), \"LRNS\" (Likelihood Ratio Negative Subjects = FDR / NPV = (1 - Precision) / (1 - )), \"BalAcc\" (Balanced accuracy = (Sensitivity + Specificity) / 2), \"MCC\" (Matthews correlation coefficient), \"Chisq\" (Chisq metric), \"Bray\" (Bray-Curtis metric) sort.statistics use sort table (default, Fmeasure, F1 score class = 2 * recall * precision / (recall + precision)). decreasing sort increasing decreasing order?","code":""},{"path":"https://www.sciviews.org/mlearning/reference/confusion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Construct and analyze confusion matrices — confusion","text":"confusion matrix confusion object.","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/confusion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Construct and analyze confusion matrices — confusion","text":"","code":"data(\"Glass\", package = \"mlbench\") # Use a little bit more informative labels for Type Glass$Type <- as.factor(paste(\"Glass\", Glass$Type))  # Use learning vector quantization to classify the glass types # (using default parameters) summary(glass_lvq <- ml_lvq(Type ~ ., data = Glass)) #> Codebook: #>       Class       RI       Na           Mg        Al       Si           K #> 63  Glass 1 1.521694 13.59043  3.990725793 0.9094547 72.01523  0.14579391 #> 7   Glass 1 1.521059 13.07075  4.103372783 0.8473620 73.29846  0.37743988 #> 34  Glass 1 1.517929 12.58665  3.629106606 1.2936618 73.32089  0.55333957 #> 68  Glass 1 1.521414 12.99878  3.889617882 0.7934280 72.35330  0.16995566 #> 67  Glass 1 1.520088 12.65700  4.045400000 0.9843600 72.64360  0.34576000 #> 35  Glass 1 1.517406 12.90335  3.513231262 1.2498535 73.02203  0.59219507 #> 59  Glass 1 1.518949 13.32596  3.988736000 1.0178040 73.38206  0.25774400 #> 22  Glass 1 1.520734 14.40385  3.997666027 0.1974609 72.09925  0.04850002 #> 121 Glass 2 1.518000 12.74152  3.944281216 1.0276983 72.40564  0.65811726 #> 114 Glass 2 1.517349 13.30204  3.962854978 1.3667218 72.46829  0.45627804 #> 99  Glass 2 1.516487 12.51203  3.563313952 1.1265846 73.82137  0.46425959 #> 80  Glass 2 1.516699 12.77170  4.540900000 1.6672000 73.53110 -0.94650000 #> 111 Glass 2 1.526975 11.85964  0.000000000 1.0945690 72.34003  0.20628403 #> 101 Glass 2 1.517469 12.42232  3.110797958 0.9764117 73.55289  0.50124441 #> 106 Glass 2 1.529429 10.64972  0.000000000 2.0907286 70.65190  0.74569286 #> 76  Glass 2 1.516460 13.01526  3.514944907 1.5919963 73.09375  0.46848265 #> 147 Glass 3 1.517611 13.51991  3.759109610 0.7179009 72.97630  0.04365230 #> 148 Glass 3 1.444993 13.39231  4.770239100 0.2944106 72.97367  0.45442414 #> 167 Glass 5 1.520825 11.84560  0.952847971 1.9327272 72.94806  0.56815986 #> 177 Glass 6 1.517433 14.32028  2.843336175 1.3961537 73.06737 -0.51989687 #> 199 Glass 7 1.515535 14.15913 -0.098719684 2.8584273 73.11888 -0.19135965 #> 200 Glass 7 1.517464 14.74311  0.167159642 2.1239741 73.17683 -0.17697378 #> 193 Glass 7 1.515671 13.53272 -0.004379341 2.6895681 74.40964 -0.15323070 #>            Ca           Ba           Fe #> 63   9.206825  0.010724617 -0.015875870 #> 7    8.656951 -0.274615385 -0.085258413 #> 34   8.547727 -0.006000823  0.025968176 #> 68   9.730704 -0.019643909  0.117945793 #> 67   9.147760  0.000000000  0.255320000 #> 35   8.574619 -0.010510399  0.033240025 #> 59   8.564840 -0.547680000 -0.042000000 #> 22   9.311891 -0.125092909 -0.059868953 #> 121  8.813795  0.000000000  0.289088634 #> 114  8.302524 -0.187533268  0.152725268 #> 99   8.684371 -0.417412231 -0.025376536 #> 80   8.234000  0.000000000 -0.101400000 #> 111 14.303707  0.000000000  0.102835753 #> 101  9.212638  0.027445782  0.180055196 #> 106 13.425721  2.025000000  0.360078571 #> 76   8.209506 -0.117284646  0.008686078 #> 147  8.811514  0.054603120  0.072752345 #> 148  8.426168 -0.502962704 -0.695615193 #> 167 11.588343  0.014503215  0.052593840 #> 177  9.236636 -0.353777421 -0.136577820 #> 199  9.217433  0.900649271  0.007556161 #> 200  8.597607  1.296505961  0.019066682 #> 193  8.890591  0.644277876  0.075870632  # Calculate cross-validated confusion matrix (glass_conf <- confusion(cvpredict(glass_lvq), Glass$Type)) #> 214 items classified with 138 true positives (error rate = 35.5%) #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 3   1  10   6   0   0   0    17     94 #>   02 Glass 1   2  53  15   0   0   0    70     24 #>   03 Glass 2   0  19  51   4   1   1    76     33 #>   04 Glass 5   0   0   5   6   0   2    13     54 #>   05 Glass 6   0   1   0   1   4   3     9     56 #>   06 Glass 7   0   1   2   1   2  23    29     21 #>   (sum)        3  84  79  12   7  29   214     36 # Raw confusion matrix: no sort and no margins print(glass_conf, sums = FALSE, sort = FALSE) #> 214 items classified with 138 true positives (error rate = 35.5%) #>             Predicted #> Actual       01 02 03 04 05 06 #>   01 Glass 1 53 15  2  0  0  0 #>   02 Glass 2 19 51  0  4  1  1 #>   03 Glass 3 10  6  1  0  0  0 #>   04 Glass 5  0  5  0  6  0  2 #>   05 Glass 6  1  0  0  1  4  3 #>   06 Glass 7  1  2  0  1  2 23  summary(glass_conf) #> 214 items classified with 138 true positives (error = 35.5%) #>  #> Global statistics on reweighted data: #> Error rate: 35.5%, F(micro-average): 0.554, F(macro-average): 0.537 #>  #>            Fscore     Recall Precision Specificity       NPV        FPR #> Glass 7 0.7931034 0.79310345 0.7931034   0.9675676 0.9675676 0.03243243 #> Glass 1 0.6883117 0.75714286 0.6309524   0.7847222 0.8692308 0.21527778 #> Glass 2 0.6580645 0.67105263 0.6455696   0.7971014 0.8148148 0.20289855 #> Glass 6 0.5000000 0.44444444 0.5714286   0.9853659 0.9758454 0.01463415 #> Glass 5 0.4800000 0.46153846 0.5000000   0.9701493 0.9653465 0.02985075 #> Glass 3 0.1000000 0.05882353 0.3333333   0.9898477 0.9241706 0.01015228 #>               FNR       FDR        FOR      LRPT      LRNT      LRPS      LRNS #> Glass 7 0.2068966 0.2068966 0.03243243 24.454023 0.2138316 24.454023 0.2138316 #> Glass 1 0.2428571 0.3690476 0.13076923  3.517051 0.3094817  4.824930 0.4245681 #> Glass 2 0.3289474 0.3544304 0.18518519  3.307331 0.4126794  3.486076 0.4349827 #> Glass 6 0.5555556 0.4285714 0.02415459 30.370370 0.5638064 23.657143 0.4391796 #> Glass 5 0.5384615 0.5000000 0.03465347 15.461538 0.5550296 14.428571 0.5179487 #> Glass 3 0.9411765 0.6666667 0.07582938  5.794118 0.9508296  4.395833 0.7213675 #>            BalAcc       MCC     Chisq        Bray Auto Manu A_M TP FP FN  TN #> Glass 7 0.8803355 0.7606710 123.82476 0.000000000   29   29   0 23  6  6 179 #> Glass 1 0.7709325 0.5206071  58.00080 0.032710280   84   70  14 53 31 17 113 #> Glass 2 0.7340770 0.4642530  46.12360 0.007009346   79   76   3 51 28 25 110 #> Glass 6 0.7149051 0.4849990  50.33793 0.004672897    7    9  -2  4  3  5 202 #> Glass 5 0.7158439 0.4482013  42.98926 0.002336449   12   13  -1  6  6  7 195 #> Glass 3 0.5243356 0.1119511   2.68207 0.032710280    3   17 -14  1  2 16 195 summary(glass_conf, type = \"Fscore\") #>   Glass 7   Glass 1   Glass 2   Glass 6   Glass 5   Glass 3  #> 0.7931034 0.6883117 0.6580645 0.5000000 0.4800000 0.1000000  #> attr(,\"stat.type\") #> [1] \"Fscore\""},{"path":"https://www.sciviews.org/mlearning/reference/mlKnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification using k-nearest neighbor — mlKnn","title":"Supervised classification using k-nearest neighbor — mlKnn","text":"Unified (formula-based) interface version k-nearest neighbor algorithm provided class::knn().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlKnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification using k-nearest neighbor — mlKnn","text":"","code":"mlKnn(train, ...)  ml_knn(train, ...)  # S3 method for formula mlKnn(formula, data, k.nn = 5, ..., subset, na.action)  # S3 method for default mlKnn(train, response, k.nn = 5, ...)  # S3 method for mlKnn summary(object, ...)  # S3 method for summary.mlKnn print(x, ...)  # S3 method for mlKnn predict(   object,   newdata,   type = c(\"class\", \"prob\", \"both\"),   method = c(\"direct\", \"cv\"),   na.action = na.exclude,   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlKnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification using k-nearest neighbor — mlKnn","text":"train matrix data frame predictors. ... arguments passed classification method predict() method (used now). formula formula left term factor variable predict right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. k.nn k used k-NN number neighbor considered. Default 5. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_knn() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor classification. x, object mlKnn object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"prob\" \"probability\" different classes assessed number neighbors classes, \"\" return classes \"probabilities\", method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different data set newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlKnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification using k-nearest neighbor — mlKnn","text":"ml_knn()/mlKnn() creates mlKnn, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlKnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification using k-nearest neighbor — mlKnn","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_knn <- ml_knn(data = iris_train, Species ~ .) summary(iris_knn) #> Train dataset: #> data frame with 0 columns and 0 rows predict(iris_knn) # This object only returns classes #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor virginica  versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica # Self-consistency, do not use for assessing classifier performances! confusion(iris_knn) #> 99 items classified with 96 true positives (error rate = 3%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 32  1    33      3 #>   03 virginica   0  2 31    33      6 #>   (sum)         33 34 32    99      3 # Use an independent test set instead confusion(predict(iris_knn, newdata = iris_test), iris_test$Species) #> 50 items classified with 48 true positives (error rate = 4%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 15  1    17     12 #>   04 virginica   0  0  0 17    17      0 #>   (sum)         16  1 15 18    50      4"},{"path":"https://www.sciviews.org/mlearning/reference/mlLda.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification using linear discriminant analysis — mlLda","title":"Supervised classification using linear discriminant analysis — mlLda","text":"Unified (formula-based) interface version linear discriminant analysis algorithm provided MASS::lda().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlLda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification using linear discriminant analysis — mlLda","text":"","code":"mlLda(train, ...)  ml_lda(train, ...)  # S3 method for formula mlLda(formula, data, ..., subset, na.action)  # S3 method for default mlLda(train, response, ...)  # S3 method for mlLda predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\", \"projection\"),   prior = object$prior,   dimension = NULL,   method = c(\"plug-in\", \"predictive\", \"debiased\", \"cv\"),   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlLda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification using linear discriminant analysis — mlLda","text":"train matrix data frame predictors. ... arguments passed MASS::lda()  predict() method (see corresponding help page). formula formula left term factor variable predict right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_lda() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor classification. object mlLda object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes, \"\" return classes memberships. type = \"projection\" returns projection individuals plane represented dimension=  discriminant components. prior prior probabilities class membership. default, prior obtained object , changed, correspond proportions observed training set. dimension number predictive space use. NULL (default) reasonable value used. less min(p, ng-1), first dimension discriminant components used (except method = \"predictive\"), dimensions returned x. method \"plug-\", \"predictive\", \"debiased\", \"cv\". \"plug-\" (default) usual unbiased parameter estimates used. \"predictive\", parameters integrated using vague prior. \"debiased\", unbiased estimator log posterior probabilities used. \"cv\", cross-validation used instead. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlLda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification using linear discriminant analysis — mlLda","text":"ml_lda()/mlLda() creates mlLda, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlLda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification using linear discriminant analysis — mlLda","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_lda <- ml_lda(data = iris_train, Species ~ .) iris_lda #> A mlearning object of class mlLda (linear discriminant analysis): #> Call: mlLda.formula(formula = Species ~ ., data = iris_train) #> Trained using 99 out of 100 cases: #>     setosa versicolor  virginica  #>         33         33         33  summary(iris_lda) #> A mlearning object of class mlLda (linear discriminant analysis): #> Initial call: mlLda.formula(formula = Species ~ ., data = iris_train) #> Call: #> lda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>     setosa versicolor  virginica  #>  0.3333333  0.3333333  0.3333333  #>  #> Group means: #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa         5.048485    3.478788     1.478788   0.2454545 #> versicolor     6.027273    2.763636     4.284848   1.3303030 #> virginica      6.642424    2.951515     5.642424   2.0090909 #>  #> Coefficients of linear discriminants: #>                    LD1        LD2 #> Sepal.Length  0.731861  0.9595049 #> Sepal.Width   1.522827 -2.8735104 #> Petal.Length -1.992262 -0.1425383 #> Petal.Width  -3.021434 -1.5467486 #>  #> Proportion of trace: #>    LD1    LD2  #> 0.9916 0.0084  plot(iris_lda, col = as.numeric(response(iris_lda)) + 1) # Prediction using a test set predict(iris_lda, newdata = iris_test) # class (default type) #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     virginica  versicolor #> [19] versicolor versicolor versicolor versicolor versicolor versicolor #> [25] <NA>       versicolor versicolor versicolor versicolor versicolor #> [31] versicolor versicolor versicolor versicolor virginica  virginica  #> [37] virginica  virginica  virginica  virginica  virginica  virginica  #> [43] virginica  virginica  virginica  virginica  virginica  virginica  #> [49] virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_lda, type = \"membership\") # posterior probability #>              setosa   versicolor    virginica #>   [1,] 1.000000e+00 9.524232e-17 5.937500e-36 #>   [2,] 1.000000e+00 1.758123e-18 6.393714e-38 #>   [3,] 1.000000e+00 6.513799e-16 2.394500e-34 #>   [4,] 1.000000e+00 2.346163e-21 1.932405e-41 #>   [5,] 1.000000e+00 7.169015e-20 9.796544e-39 #>   [6,] 1.000000e+00 9.874841e-18 2.894202e-36 #>   [7,] 1.000000e+00 4.123017e-19 1.318116e-38 #>   [8,] 1.000000e+00 9.594213e-15 6.039947e-33 #>   [9,] 1.000000e+00 7.873691e-18 2.097109e-37 #>  [10,] 1.000000e+00 2.609732e-22 7.255914e-43 #>  [11,] 1.000000e+00 8.320901e-18 1.364519e-36 #>  [12,] 1.000000e+00 1.211508e-17 2.805560e-37 #>  [13,] 1.000000e+00 9.536296e-19 1.465443e-38 #>  [14,] 1.000000e+00 6.860245e-28 1.426815e-50 #>  [15,] 1.000000e+00 7.475537e-26 1.520329e-46 #>  [16,] 1.000000e+00 4.785676e-23 2.483069e-43 #>  [17,] 1.000000e+00 9.802813e-20 2.845112e-39 #>  [18,] 1.000000e+00 4.605666e-21 5.968926e-41 #>  [19,] 1.000000e+00 4.062215e-21 1.190882e-40 #>  [20,] 1.000000e+00 1.516425e-18 4.852725e-38 #>  [21,] 1.000000e+00 3.056987e-19 4.751958e-38 #>  [22,] 1.000000e+00 1.648141e-23 2.642554e-44 #>  [23,] 1.000000e+00 1.336747e-13 1.268148e-30 #>  [24,] 1.000000e+00 2.003584e-15 3.819817e-33 #>  [25,] 1.000000e+00 2.046680e-15 4.351542e-34 #>  [26,] 1.000000e+00 5.142538e-16 6.111497e-34 #>  [27,] 1.000000e+00 2.391388e-20 2.579476e-40 #>  [28,] 1.000000e+00 2.042960e-20 1.273291e-40 #>  [29,] 1.000000e+00 4.233371e-16 1.789849e-34 #>  [30,] 1.000000e+00 1.249215e-15 4.594423e-34 #>  [31,] 1.000000e+00 7.854999e-18 8.037421e-37 #>  [32,] 1.000000e+00 7.493326e-26 3.966815e-47 #>  [33,] 1.000000e+00 5.492045e-27 1.165689e-48 #>  [34,] 6.728356e-18 9.999301e-01 6.988092e-05 #>  [35,] 5.384547e-19 9.993600e-01 6.399678e-04 #>  [36,] 1.279003e-21 9.971466e-01 2.853444e-03 #>  [37,] 1.491199e-21 9.997082e-01 2.917717e-04 #>  [38,] 1.949409e-22 9.971777e-01 2.822274e-03 #>  [39,] 2.197454e-21 9.980027e-01 1.997293e-03 #>  [40,] 2.857474e-21 9.849268e-01 1.507319e-02 #>  [41,] 2.377941e-13 9.999999e-01 1.271743e-07 #>  [42,] 3.751481e-19 9.999124e-01 8.756011e-05 #>  [43,] 8.944958e-20 9.993947e-01 6.053269e-04 #>  [44,] 1.386835e-17 9.999987e-01 1.273302e-06 #>  [45,] 2.421474e-19 9.992724e-01 7.275660e-04 #>  [46,] 1.510041e-17 9.999993e-01 7.449744e-07 #>  [47,] 2.233292e-22 9.938387e-01 6.161252e-03 #>  [48,] 9.070886e-14 9.999985e-01 1.464089e-06 #>  [49,] 5.217489e-17 9.999732e-01 2.680851e-05 #>  [50,] 1.674158e-22 9.719677e-01 2.803234e-02 #>  [51,] 3.173058e-15 9.999990e-01 9.944224e-07 #>  [52,] 9.041922e-27 9.790373e-01 2.096266e-02 #>  [53,] 9.465411e-17 9.999970e-01 3.045202e-06 #>  [54,] 8.632202e-27 2.075616e-01 7.924384e-01 #>  [55,] 2.159399e-16 9.999935e-01 6.542439e-06 #>  [56,] 1.428380e-27 8.568276e-01 1.431724e-01 #>  [57,] 8.472457e-21 9.995057e-01 4.942844e-04 #>  [58,] 2.784802e-17 9.999829e-01 1.705453e-05 #>  [59,] 5.450336e-18 9.999472e-01 5.280245e-05 #>  [60,] 4.174183e-22 9.989487e-01 1.051274e-03 #>  [61,] 4.534883e-26 7.602933e-01 2.397067e-01 #>  [62,] 3.385492e-22 9.926904e-01 7.309575e-03 #>  [63,] 1.921877e-11 1.000000e+00 1.449145e-08 #>  [64,] 6.151650e-17 9.999974e-01 2.647688e-06 #>  [65,] 5.418928e-15 9.999997e-01 2.885234e-07 #>  [66,] 6.125645e-16 9.999968e-01 3.179623e-06 #>  [67,] 3.260056e-50 5.375879e-09 1.000000e+00 #>  [68,] 1.534619e-36 9.609211e-04 9.990391e-01 #>  [69,] 4.298766e-41 3.843125e-05 9.999616e-01 #>  [70,] 1.120396e-36 8.624695e-04 9.991375e-01 #>  [71,] 2.665504e-44 1.851447e-06 9.999981e-01 #>  [72,] 5.691479e-47 9.683420e-07 9.999990e-01 #>  [73,] 1.005543e-31 3.051764e-02 9.694824e-01 #>  [74,] 2.170455e-40 1.677627e-04 9.998322e-01 #>  [75,] 1.299923e-40 2.936646e-04 9.997063e-01 #>  [76,] 6.144375e-45 2.105094e-07 9.999998e-01 #>  [77,] 4.613890e-31 1.568862e-02 9.843114e-01 #>  [78,] 3.058067e-36 2.171019e-03 9.978290e-01 #>  [79,] 8.517422e-38 2.969936e-04 9.997030e-01 #>  [80,] 2.876903e-39 2.085888e-04 9.997914e-01 #>  [81,] 1.721481e-44 1.154274e-06 9.999988e-01 #>  [82,] 4.614421e-39 3.104958e-05 9.999690e-01 #>  [83,] 8.029333e-34 5.762455e-03 9.942375e-01 #>  [84,] 1.047025e-42 1.368459e-06 9.999986e-01 #>  [85,] 7.150283e-57 2.601747e-09 1.000000e+00 #>  [86,] 7.847879e-32 2.568989e-01 7.431011e-01 #>  [87,] 1.710232e-41 9.095346e-06 9.999909e-01 #>  [88,] 5.034775e-36 7.039438e-04 9.992961e-01 #>  [89,] 1.297055e-47 1.520685e-06 9.999985e-01 #>  [90,] 2.216946e-30 1.336448e-01 8.663552e-01 #>  [91,] 5.328521e-38 8.627299e-05 9.999137e-01 #>  [92,] 7.527919e-35 2.899037e-03 9.971010e-01 #>  [93,] 7.099373e-29 2.331240e-01 7.668760e-01 #>  [94,] 1.017544e-28 1.325438e-01 8.674562e-01 #>  [95,] 2.315077e-42 1.492841e-05 9.999851e-01 #>  [96,] 8.918840e-31 1.250327e-01 8.749673e-01 #>  [97,] 2.927915e-40 2.443079e-04 9.997557e-01 #>  [98,] 7.078517e-35 5.907724e-04 9.994092e-01 #>  [99,] 4.036304e-44 3.685307e-06 9.999963e-01 predict(iris_lda, type = \"both\") # both class and membership in a list #> $class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>              setosa   versicolor    virginica #>   [1,] 1.000000e+00 9.524232e-17 5.937500e-36 #>   [2,] 1.000000e+00 1.758123e-18 6.393714e-38 #>   [3,] 1.000000e+00 6.513799e-16 2.394500e-34 #>   [4,] 1.000000e+00 2.346163e-21 1.932405e-41 #>   [5,] 1.000000e+00 7.169015e-20 9.796544e-39 #>   [6,] 1.000000e+00 9.874841e-18 2.894202e-36 #>   [7,] 1.000000e+00 4.123017e-19 1.318116e-38 #>   [8,] 1.000000e+00 9.594213e-15 6.039947e-33 #>   [9,] 1.000000e+00 7.873691e-18 2.097109e-37 #>  [10,] 1.000000e+00 2.609732e-22 7.255914e-43 #>  [11,] 1.000000e+00 8.320901e-18 1.364519e-36 #>  [12,] 1.000000e+00 1.211508e-17 2.805560e-37 #>  [13,] 1.000000e+00 9.536296e-19 1.465443e-38 #>  [14,] 1.000000e+00 6.860245e-28 1.426815e-50 #>  [15,] 1.000000e+00 7.475537e-26 1.520329e-46 #>  [16,] 1.000000e+00 4.785676e-23 2.483069e-43 #>  [17,] 1.000000e+00 9.802813e-20 2.845112e-39 #>  [18,] 1.000000e+00 4.605666e-21 5.968926e-41 #>  [19,] 1.000000e+00 4.062215e-21 1.190882e-40 #>  [20,] 1.000000e+00 1.516425e-18 4.852725e-38 #>  [21,] 1.000000e+00 3.056987e-19 4.751958e-38 #>  [22,] 1.000000e+00 1.648141e-23 2.642554e-44 #>  [23,] 1.000000e+00 1.336747e-13 1.268148e-30 #>  [24,] 1.000000e+00 2.003584e-15 3.819817e-33 #>  [25,] 1.000000e+00 2.046680e-15 4.351542e-34 #>  [26,] 1.000000e+00 5.142538e-16 6.111497e-34 #>  [27,] 1.000000e+00 2.391388e-20 2.579476e-40 #>  [28,] 1.000000e+00 2.042960e-20 1.273291e-40 #>  [29,] 1.000000e+00 4.233371e-16 1.789849e-34 #>  [30,] 1.000000e+00 1.249215e-15 4.594423e-34 #>  [31,] 1.000000e+00 7.854999e-18 8.037421e-37 #>  [32,] 1.000000e+00 7.493326e-26 3.966815e-47 #>  [33,] 1.000000e+00 5.492045e-27 1.165689e-48 #>  [34,] 6.728356e-18 9.999301e-01 6.988092e-05 #>  [35,] 5.384547e-19 9.993600e-01 6.399678e-04 #>  [36,] 1.279003e-21 9.971466e-01 2.853444e-03 #>  [37,] 1.491199e-21 9.997082e-01 2.917717e-04 #>  [38,] 1.949409e-22 9.971777e-01 2.822274e-03 #>  [39,] 2.197454e-21 9.980027e-01 1.997293e-03 #>  [40,] 2.857474e-21 9.849268e-01 1.507319e-02 #>  [41,] 2.377941e-13 9.999999e-01 1.271743e-07 #>  [42,] 3.751481e-19 9.999124e-01 8.756011e-05 #>  [43,] 8.944958e-20 9.993947e-01 6.053269e-04 #>  [44,] 1.386835e-17 9.999987e-01 1.273302e-06 #>  [45,] 2.421474e-19 9.992724e-01 7.275660e-04 #>  [46,] 1.510041e-17 9.999993e-01 7.449744e-07 #>  [47,] 2.233292e-22 9.938387e-01 6.161252e-03 #>  [48,] 9.070886e-14 9.999985e-01 1.464089e-06 #>  [49,] 5.217489e-17 9.999732e-01 2.680851e-05 #>  [50,] 1.674158e-22 9.719677e-01 2.803234e-02 #>  [51,] 3.173058e-15 9.999990e-01 9.944224e-07 #>  [52,] 9.041922e-27 9.790373e-01 2.096266e-02 #>  [53,] 9.465411e-17 9.999970e-01 3.045202e-06 #>  [54,] 8.632202e-27 2.075616e-01 7.924384e-01 #>  [55,] 2.159399e-16 9.999935e-01 6.542439e-06 #>  [56,] 1.428380e-27 8.568276e-01 1.431724e-01 #>  [57,] 8.472457e-21 9.995057e-01 4.942844e-04 #>  [58,] 2.784802e-17 9.999829e-01 1.705453e-05 #>  [59,] 5.450336e-18 9.999472e-01 5.280245e-05 #>  [60,] 4.174183e-22 9.989487e-01 1.051274e-03 #>  [61,] 4.534883e-26 7.602933e-01 2.397067e-01 #>  [62,] 3.385492e-22 9.926904e-01 7.309575e-03 #>  [63,] 1.921877e-11 1.000000e+00 1.449145e-08 #>  [64,] 6.151650e-17 9.999974e-01 2.647688e-06 #>  [65,] 5.418928e-15 9.999997e-01 2.885234e-07 #>  [66,] 6.125645e-16 9.999968e-01 3.179623e-06 #>  [67,] 3.260056e-50 5.375879e-09 1.000000e+00 #>  [68,] 1.534619e-36 9.609211e-04 9.990391e-01 #>  [69,] 4.298766e-41 3.843125e-05 9.999616e-01 #>  [70,] 1.120396e-36 8.624695e-04 9.991375e-01 #>  [71,] 2.665504e-44 1.851447e-06 9.999981e-01 #>  [72,] 5.691479e-47 9.683420e-07 9.999990e-01 #>  [73,] 1.005543e-31 3.051764e-02 9.694824e-01 #>  [74,] 2.170455e-40 1.677627e-04 9.998322e-01 #>  [75,] 1.299923e-40 2.936646e-04 9.997063e-01 #>  [76,] 6.144375e-45 2.105094e-07 9.999998e-01 #>  [77,] 4.613890e-31 1.568862e-02 9.843114e-01 #>  [78,] 3.058067e-36 2.171019e-03 9.978290e-01 #>  [79,] 8.517422e-38 2.969936e-04 9.997030e-01 #>  [80,] 2.876903e-39 2.085888e-04 9.997914e-01 #>  [81,] 1.721481e-44 1.154274e-06 9.999988e-01 #>  [82,] 4.614421e-39 3.104958e-05 9.999690e-01 #>  [83,] 8.029333e-34 5.762455e-03 9.942375e-01 #>  [84,] 1.047025e-42 1.368459e-06 9.999986e-01 #>  [85,] 7.150283e-57 2.601747e-09 1.000000e+00 #>  [86,] 7.847879e-32 2.568989e-01 7.431011e-01 #>  [87,] 1.710232e-41 9.095346e-06 9.999909e-01 #>  [88,] 5.034775e-36 7.039438e-04 9.992961e-01 #>  [89,] 1.297055e-47 1.520685e-06 9.999985e-01 #>  [90,] 2.216946e-30 1.336448e-01 8.663552e-01 #>  [91,] 5.328521e-38 8.627299e-05 9.999137e-01 #>  [92,] 7.527919e-35 2.899037e-03 9.971010e-01 #>  [93,] 7.099373e-29 2.331240e-01 7.668760e-01 #>  [94,] 1.017544e-28 1.325438e-01 8.674562e-01 #>  [95,] 2.315077e-42 1.492841e-05 9.999851e-01 #>  [96,] 8.918840e-31 1.250327e-01 8.749673e-01 #>  [97,] 2.927915e-40 2.443079e-04 9.997557e-01 #>  [98,] 7.078517e-35 5.907724e-04 9.994092e-01 #>  [99,] 4.036304e-44 3.685307e-06 9.999963e-01 #>  # Type projection predict(iris_lda, type = \"projection\") # Projection on the LD axes #>               LD1          LD2 #>   [1,]  6.9568849  1.101758773 #>   [2,]  7.3143043  0.349409550 #>   [3,]  6.6903832  0.512302451 #>   [4,]  7.9437673 -0.526396971 #>   [5,]  7.4913946 -1.356759342 #>   [6,]  7.0443141 -0.490171701 #>   [7,]  7.4399757  0.034051280 #>   [8,]  6.4386717  0.909357381 #>   [9,]  7.2120848  0.954828771 #>  [10,]  8.1895682 -0.444199891 #>  [11,]  7.0943773 -0.172103518 #>  [12,]  7.1858422  1.160483149 #>  [13,]  7.4175902  0.723492195 #>  [14,]  9.5368393 -0.879689584 #>  [15,]  8.8708188 -2.477155425 #>  [16,]  8.2882993 -1.299744040 #>  [17,]  7.5625273 -0.297770308 #>  [18,]  7.8608135 -0.626881984 #>  [19,]  7.8201493 -1.174077249 #>  [20,]  7.3342677  0.389345573 #>  [21,]  7.3657232 -1.041401073 #>  [22,]  8.4479276 -0.853183613 #>  [23,]  6.0559967 -0.075179432 #>  [24,]  6.4966988 -0.214864995 #>  [25,]  6.6316187  1.169201608 #>  [26,]  6.6364628 -0.289552269 #>  [27,]  7.7386306 -0.061398786 #>  [28,]  7.7855741  0.240206078 #>  [29,]  6.7166258  0.306648073 #>  [30,]  6.6375292  0.689949598 #>  [31,]  7.1284334  0.108503501 #>  [32,]  8.9544702 -1.630830155 #>  [33,]  9.2233941 -1.770750771 #>  [34,] -1.4018257  0.215542317 #>  [35,] -1.7446333 -0.486327810 #>  [36,] -2.3278902  0.223760356 #>  [37,] -2.1734350  1.616896012 #>  [38,] -2.4798042  0.744773004 #>  [39,] -2.2617801  0.300772664 #>  [40,] -2.3661324 -1.052811848 #>  [41,] -0.1592556  1.317643422 #>  [42,] -1.6500487  0.862722175 #>  [43,] -1.8867796  0.039219364 #>  [44,] -1.0936527  2.534490411 #>  [45,] -1.8174507 -0.348616687 #>  [46,] -1.0533572  2.848024067 #>  [47,] -2.5173487  0.214041057 #>  [48,] -0.3896479  0.045755570 #>  [49,] -1.1759882  0.257803374 #>  [50,] -2.6346875 -0.679229623 #>  [51,] -0.6375420  1.205114077 #>  [52,] -3.4138326  2.195281602 #>  [53,] -0.9921706  1.461747971 #>  [54,] -3.6146724 -1.472866304 #>  [55,] -0.9729048  0.755843737 #>  [56,] -3.6807031  1.372163670 #>  [57,] -2.0653447  0.810741819 #>  [58,] -1.1987424  0.713582680 #>  [59,] -1.4014570  0.449203926 #>  [60,] -2.3565549  1.158791673 #>  [61,] -3.4300580 -0.004393127 #>  [62,] -2.4942258 -0.008076640 #>  [63,]  0.3323463  1.482037583 #>  [64,] -1.0184132  1.667402349 #>  [65,] -0.5170437  1.836331036 #>  [66,] -0.8433763  0.924272004 #>  [67,] -7.6753629 -2.630185338 #>  [68,] -5.3490939 -0.329497936 #>  [69,] -6.1389227 -0.367575061 #>  [70,] -5.3725855 -0.341041849 #>  [71,] -6.6809564 -1.083699014 #>  [72,] -7.1675754  0.012400591 #>  [73,] -4.5126905 -0.223477557 #>  [74,] -6.0353078  0.518686233 #>  [75,] -6.0874244  1.163656598 #>  [76,] -6.7590661 -2.642937904 #>  [77,] -4.3775210 -1.249274587 #>  [78,] -5.3084297  0.217697329 #>  [79,] -5.5615762 -0.598411216 #>  [80,] -5.8297626  0.008832618 #>  [81,] -6.7075280 -1.390223284 #>  [82,] -5.7555896 -1.837757310 #>  [83,] -4.8747045 -0.422238089 #>  [84,] -6.3774971 -2.359385918 #>  [85,] -8.9054854  0.905644030 #>  [86,] -4.5563357  1.932111501 #>  [87,] -6.1865638 -1.415020182 #>  [88,] -5.2468744 -0.934917158 #>  [89,] -7.2960376  0.823474190 #>  [90,] -4.2825677  0.333437008 #>  [91,] -5.5763665 -1.584922469 #>  [92,] -5.0539672 -0.396555891 #>  [93,] -4.0042449 -0.035610691 #>  [94,] -3.9720918 -0.720517080 #>  [95,] -6.3581122 -0.421764910 #>  [96,] -4.3557936  0.516003560 #>  [97,] -6.0180954  0.775820547 #>  [98,] -5.0291597 -1.815373745 #>  [99,] -6.6602556 -0.576439772 # Add test set items to the previous plot points(predict(iris_lda, newdata = iris_test, type = \"projection\"),   col = as.numeric(predict(iris_lda, newdata = iris_test)) + 1, pch = 19)  # predict() and confusion() should be used on a separate test set # for unbiased estimation (or using cross-validation, bootstrap, ...) # Wrong, cf. biased estimation (so-called, self-consistency) confusion(iris_lda) #> 99 items classified with 98 true positives (error rate = 1%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 32  1    33      3 #>   03 virginica   0  0 33    33      0 #>   (sum)         33 32 34    99      1 # Estimation using a separate test set confusion(predict(iris_lda, newdata = iris_test), iris_test$Species) #> 50 items classified with 47 true positives (error rate = 6%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 15  1    17     12 #>   04 virginica   0  0  1 16    17      6 #>   (sum)         16  1 16 17    50      6  # Another dataset (binary predictor... not optimal for lda, just for test) data(\"HouseVotes84\", package = \"mlbench\") house_lda <- ml_lda(data = HouseVotes84, na.action = na.omit, Class ~ .) #> Warning: force conversion from factor to numeric; may be not optimal or suitable summary(house_lda) #> A mlearning object of class mlLda (linear discriminant analysis): #> Initial call: mlLda.formula(formula = Class ~ ., data = HouseVotes84, na.action = na.omit) #> Call: #> lda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>   democrat republican  #>  0.5344828  0.4655172  #>  #> Group means: #>                  V1       V2       V3       V4       V5       V6       V7 #> democrat   1.588710 1.451613 1.854839 1.048387 1.201613 1.443548 1.766129 #> republican 1.212963 1.472222 1.157407 1.990741 1.953704 1.870370 1.268519 #>                  V8       V9      V10      V11      V12      V13      V14 #> democrat   1.830645 1.790323 1.532258 1.508065 1.129032 1.290323 1.346774 #> republican 1.148148 1.138889 1.574074 1.157407 1.851852 1.842593 1.981481 #>                 V15      V16 #> democrat   1.596774 1.943548 #> republican 1.111111 1.666667 #>  #> Coefficients of linear discriminants: #>             LD1 #> V1   0.05874608 #> V2  -0.13982178 #> V3  -0.78702772 #> V4   5.64762176 #> V5   0.12150873 #> V6  -0.08307307 #> V7   0.24825927 #> V8  -0.06528145 #> V9  -0.21114235 #> V10  0.25213648 #> V11 -0.70823602 #> V12  0.02863686 #> V13  0.23819274 #> V14 -0.07092076 #> V15  0.18474183 #> V16  0.37102658 confusion(house_lda) # Self-consistency (biased metrics) #> 232 items classified with 225 true positives (error rate = 3%) #>                Predicted #> Actual           01  02 (sum) (FNR%) #>   01 democrat   118   6   124      5 #>   02 republican   1 107   108      1 #>   (sum)         119 113   232      3 print(confusion(house_lda), error.col = FALSE) # Without error column #> 232 items classified with 225 true positives (error rate = 3%) #>                Predicted #> Actual           01  02 (sum) #>   01 democrat   118   6   124 #>   02 republican   1 107   108 #>   (sum)         119 113   232  # More complex formulas # Exclude one or more variables iris_lda2 <- ml_lda(data = iris, Species ~ . - Sepal.Width) summary(iris_lda2) #> A mlearning object of class mlLda (linear discriminant analysis): #> Initial call: mlLda.formula(formula = Species ~ . - Sepal.Width, data = iris) #> Call: #> lda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>     setosa versicolor  virginica  #>  0.3333333  0.3333333  0.3333333  #>  #> Group means: #>            Sepal.Length Petal.Length Petal.Width #> setosa            5.006        1.462       0.246 #> versicolor        5.936        4.260       1.326 #> virginica         6.588        5.552       2.026 #>  #> Coefficients of linear discriminants: #>                    LD1       LD2 #> Sepal.Length -1.539022  1.591246 #> Petal.Length  2.719004 -2.619277 #> Petal.Width   2.035445  4.719647 #>  #> Proportion of trace: #>    LD1    LD2  #> 0.9936 0.0064  # With calculation iris_lda3 <- ml_lda(data = iris, Species ~ log(Petal.Length) +   log(Petal.Width) + I(Petal.Length/Sepal.Length)) summary(iris_lda3) #> A mlearning object of class mlLda (linear discriminant analysis): #> Initial call: mlLda.formula(formula = Species ~ log(Petal.Length) + log(Petal.Width) +     I(Petal.Length/Sepal.Length), data = iris) #> Call: #> lda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>     setosa versicolor  virginica  #>  0.3333333  0.3333333  0.3333333  #>  #> Group means: #>            log(Petal.Length) log(Petal.Width) I(Petal.Length/Sepal.Length) #> setosa             0.3727587       -1.4846488                    0.2927557 #> versicolor         1.4429301        0.2709331                    0.7177285 #> virginica          1.7094260        0.6967478                    0.8437495 #>  #> Coefficients of linear discriminants: #>                                    LD1        LD2 #> log(Petal.Length)             3.487170 -8.4773418 #> log(Petal.Width)              1.213501 -0.8427381 #> I(Petal.Length/Sepal.Length) 12.699248 24.1497766 #>  #> Proportion of trace: #>    LD1    LD2  #> 0.9992 0.0008   # Factor levels with missing items are allowed ir2 <- iris[-(51:100), ] # No Iris versicolor in the training set iris_lda4 <- ml_lda(data = ir2, Species ~ .) summary(iris_lda4) # missing class #> A mlearning object of class mlLda (linear discriminant analysis): #> Initial call: mlLda.formula(formula = Species ~ ., data = ir2) #> Call: #> lda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>    setosa virginica  #>       0.5       0.5  #>  #> Group means: #>           Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa           5.006       3.428        1.462       0.246 #> virginica        6.588       2.974        5.552       2.026 #>  #> Coefficients of linear discriminants: #>                     LD1 #> Sepal.Length -1.1338828 #> Sepal.Width  -0.8603685 #> Petal.Length  2.6138926 #> Petal.Width   2.6310427 # Missing levels are reinjected in class or membership by predict() predict(iris_lda4, type = \"both\") #> $class #>   [1] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>   [8] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>  [15] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>  [22] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>  [29] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>  [36] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>  [43] setosa    setosa    setosa    setosa    setosa    setosa    setosa    #>  [50] setosa    virginica virginica virginica virginica virginica virginica #>  [57] virginica virginica virginica virginica virginica virginica virginica #>  [64] virginica virginica virginica virginica virginica virginica virginica #>  [71] virginica virginica virginica virginica virginica virginica virginica #>  [78] virginica virginica virginica virginica virginica virginica virginica #>  [85] virginica virginica virginica virginica virginica virginica virginica #>  [92] virginica virginica virginica virginica virginica virginica virginica #>  [99] virginica virginica #> Levels: setosa versicolor virginica #>  #> $membership #>              setosa versicolor    virginica #>   [1,] 1.000000e+00          0 7.512831e-46 #>   [2,] 1.000000e+00          0 7.276088e-42 #>   [3,] 1.000000e+00          0 4.053527e-43 #>   [4,] 1.000000e+00          0 9.767576e-39 #>   [5,] 1.000000e+00          0 1.100925e-45 #>   [6,] 1.000000e+00          0 4.725337e-42 #>   [7,] 1.000000e+00          0 2.717145e-40 #>   [8,] 1.000000e+00          0 4.696606e-43 #>   [9,] 1.000000e+00          0 6.665430e-38 #>  [10,] 1.000000e+00          0 2.135333e-42 #>  [11,] 1.000000e+00          0 2.258299e-47 #>  [12,] 1.000000e+00          0 4.302480e-40 #>  [13,] 1.000000e+00          0 8.984619e-43 #>  [14,] 1.000000e+00          0 4.320000e-44 #>  [15,] 1.000000e+00          0 1.896091e-56 #>  [16,] 1.000000e+00          0 6.736143e-50 #>  [17,] 1.000000e+00          0 2.140622e-48 #>  [18,] 1.000000e+00          0 2.966078e-44 #>  [19,] 1.000000e+00          0 3.436672e-45 #>  [20,] 1.000000e+00          0 3.105104e-44 #>  [21,] 1.000000e+00          0 1.235400e-42 #>  [22,] 1.000000e+00          0 4.078324e-42 #>  [23,] 1.000000e+00          0 2.817007e-49 #>  [24,] 1.000000e+00          0 2.930308e-35 #>  [25,] 1.000000e+00          0 2.463987e-35 #>  [26,] 1.000000e+00          0 2.217500e-39 #>  [27,] 1.000000e+00          0 2.821729e-38 #>  [28,] 1.000000e+00          0 5.940132e-45 #>  [29,] 1.000000e+00          0 5.126836e-46 #>  [30,] 1.000000e+00          0 2.321415e-38 #>  [31,] 1.000000e+00          0 1.584158e-38 #>  [32,] 1.000000e+00          0 1.296042e-42 #>  [33,] 1.000000e+00          0 1.109833e-49 #>  [34,] 1.000000e+00          0 2.949136e-52 #>  [35,] 1.000000e+00          0 8.430330e-41 #>  [36,] 1.000000e+00          0 9.076484e-47 #>  [37,] 1.000000e+00          0 3.450702e-50 #>  [38,] 1.000000e+00          0 1.359439e-46 #>  [39,] 1.000000e+00          0 5.197906e-40 #>  [40,] 1.000000e+00          0 9.633928e-44 #>  [41,] 1.000000e+00          0 3.751372e-45 #>  [42,] 1.000000e+00          0 1.898509e-35 #>  [43,] 1.000000e+00          0 4.696510e-41 #>  [44,] 1.000000e+00          0 1.322046e-35 #>  [45,] 1.000000e+00          0 2.706126e-36 #>  [46,] 1.000000e+00          0 1.400418e-39 #>  [47,] 1.000000e+00          0 3.031589e-44 #>  [48,] 1.000000e+00          0 7.617054e-41 #>  [49,] 1.000000e+00          0 1.100936e-46 #>  [50,] 1.000000e+00          0 4.053568e-44 #>  [51,] 4.617725e-58          0 1.000000e+00 #>  [52,] 8.798280e-41          0 1.000000e+00 #>  [53,] 3.746998e-46          0 1.000000e+00 #>  [54,] 1.244135e-42          0 1.000000e+00 #>  [55,] 2.725168e-50          0 1.000000e+00 #>  [56,] 8.161589e-54          0 1.000000e+00 #>  [57,] 2.612854e-35          0 1.000000e+00 #>  [58,] 7.462125e-47          0 1.000000e+00 #>  [59,] 3.861338e-45          0 1.000000e+00 #>  [60,] 6.860587e-52          0 1.000000e+00 #>  [61,] 5.943151e-35          0 1.000000e+00 #>  [62,] 7.949426e-40          0 1.000000e+00 #>  [63,] 7.138946e-42          0 1.000000e+00 #>  [64,] 1.592055e-42          0 1.000000e+00 #>  [65,] 3.051613e-48          0 1.000000e+00 #>  [66,] 1.333376e-43          0 1.000000e+00 #>  [67,] 3.791655e-39          0 1.000000e+00 #>  [68,] 3.922977e-52          0 1.000000e+00 #>  [69,] 3.638913e-63          0 1.000000e+00 #>  [70,] 4.805245e-34          0 1.000000e+00 #>  [71,] 1.663273e-46          0 1.000000e+00 #>  [72,] 4.634792e-40          0 1.000000e+00 #>  [73,] 3.682206e-54          0 1.000000e+00 #>  [74,] 1.421107e-32          0 1.000000e+00 #>  [75,] 3.628997e-44          0 1.000000e+00 #>  [76,] 3.227610e-41          0 1.000000e+00 #>  [77,] 3.738057e-31          0 1.000000e+00 #>  [78,] 2.201634e-32          0 1.000000e+00 #>  [79,] 2.962680e-47          0 1.000000e+00 #>  [80,] 6.753553e-36          0 1.000000e+00 #>  [81,] 4.115135e-45          0 1.000000e+00 #>  [82,] 8.322520e-43          0 1.000000e+00 #>  [83,] 7.504224e-49          0 1.000000e+00 #>  [84,] 1.958141e-30          0 1.000000e+00 #>  [85,] 3.454159e-39          0 1.000000e+00 #>  [86,] 2.172019e-48          0 1.000000e+00 #>  [87,] 1.338833e-49          0 1.000000e+00 #>  [88,] 2.587466e-39          0 1.000000e+00 #>  [89,] 1.740755e-31          0 1.000000e+00 #>  [90,] 4.462874e-39          0 1.000000e+00 #>  [91,] 2.053855e-48          0 1.000000e+00 #>  [92,] 1.639745e-37          0 1.000000e+00 #>  [93,] 8.798280e-41          0 1.000000e+00 #>  [94,] 2.296341e-50          0 1.000000e+00 #>  [95,] 1.493725e-50          0 1.000000e+00 #>  [96,] 5.380405e-41          0 1.000000e+00 #>  [97,] 8.437651e-37          0 1.000000e+00 #>  [98,] 1.393126e-37          0 1.000000e+00 #>  [99,] 1.610909e-45          0 1.000000e+00 #> [100,] 6.235010e-37          0 1.000000e+00 #>  # ... but, of course, the classifier is wrong for Iris versicolor confusion(predict(iris_lda4, newdata = iris), iris$Species) #> 150 items classified with 100 true positives (error rate = 33.3%) #>                Predicted #> Actual           01  02  03 (sum) (FNR%) #>   01 setosa      50   0   0    50      0 #>   02 versicolor   1   0  49    50    100 #>   03 virginica    0   0  50    50      0 #>   (sum)          51   0  99   150     33  # Simpler interface, but more memory-effective iris_lda5 <- ml_lda(train = iris[, -5], response = iris$Species) summary(iris_lda5) #> A mlearning object of class mlLda (linear discriminant analysis): #> Initial call: mlLda.default(train = iris[, -5], response = iris$Species) #> Call: #> lda(sapply(train, as.numeric), grouping = response) #>  #> Prior probabilities of groups: #>     setosa versicolor  virginica  #>  0.3333333  0.3333333  0.3333333  #>  #> Group means: #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa            5.006       3.428        1.462       0.246 #> versicolor        5.936       2.770        4.260       1.326 #> virginica         6.588       2.974        5.552       2.026 #>  #> Coefficients of linear discriminants: #>                     LD1         LD2 #> Sepal.Length  0.8293776 -0.02410215 #> Sepal.Width   1.5344731 -2.16452123 #> Petal.Length -2.2012117  0.93192121 #> Petal.Width  -2.8104603 -2.83918785 #>  #> Proportion of trace: #>    LD1    LD2  #> 0.9912 0.0088"},{"path":"https://www.sciviews.org/mlearning/reference/mlLvq.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification using learning vector quantization — mlLvq","title":"Supervised classification using learning vector quantization — mlLvq","text":"Unified (formula-based) interface version learning vector quantization algorithms provided class::olvq1(), class::lvq1(), class::lvq2(), class::lvq3().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlLvq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification using learning vector quantization — mlLvq","text":"","code":"mlLvq(train, ...)  ml_lvq(train, ...)  # S3 method for formula mlLvq(   formula,   data,   k.nn = 5,   size,   prior,   algorithm = \"olvq1\",   ...,   subset,   na.action )  # S3 method for default mlLvq(train, response, k.nn = 5, size, prior, algorithm = \"olvq1\", ...)  # S3 method for mlLvq summary(object, ...)  # S3 method for summary.mlLvq print(x, ...)  # S3 method for mlLvq predict(   object,   newdata,   type = \"class\",   method = c(\"direct\", \"cv\"),   na.action = na.exclude,   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlLvq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification using learning vector quantization — mlLvq","text":"train matrix data frame predictors. ... arguments passed classification method predict() method (used now). formula formula left term factor variable predict right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. k.nn k used k-NN number neighbor considered. Default 5. size size codebook. Defaults min(round(0.4 \\* nc \\* (nc - 1 + p/2),0), n) nc number classes. prior probabilities represent classes codebook (default values proportions training set). algorithm \"olvq1\" (default, optimized 'lvq1' version), \"lvq1\", \"lvq2\", \"lvq3\". subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. [ml_lvq)] na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). [ml_lvq)]: R:ml_lvq) response vector factor classes. x, object mlLvq object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. method, \"class\" accepted, default. returns predicted classes. method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different dataset newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlLvq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification using learning vector quantization — mlLvq","text":"ml_lvq()/mlLvq() creates mlLvq, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlLvq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification using learning vector quantization — mlLvq","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_lvq <- ml_lvq(data = iris_train, Species ~ .) summary(iris_lvq) #> Codebook: #>          Class Sepal.Length Sepal.Width Petal.Length Petal.Width #> 25      setosa     4.828662    3.258599     1.460510   0.2152866 #> 15      setosa     5.462810    3.863636     1.472727   0.2495868 #> 64  versicolor     5.833372    2.610241     4.027718   1.2175021 #> 57  versicolor     6.553846    3.018462     4.596154   1.4223077 #> 111  virginica     6.195060    2.786866     5.227170   1.9315662 #> 129  virginica     7.212621    3.277670     6.098058   2.1000000 predict(iris_lvq) # This object only returns classes #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor virginica  versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #' # Self-consistency, do not use for assessing classifier performances! confusion(iris_lvq) #> 99 items classified with 96 true positives (error rate = 3%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 31  2    33      6 #>   03 virginica   0  1 32    33      3 #>   (sum)         33 32 34    99      3 # Use an independent test set instead confusion(predict(iris_lvq, newdata = iris_test), iris_test$Species) #> 50 items classified with 48 true positives (error rate = 4%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 15  1    17     12 #>   04 virginica   0  0  0 17    17      0 #>   (sum)         16  1 15 18    50      4"},{"path":"https://www.sciviews.org/mlearning/reference/mlNaiveBayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification using naive Bayes — mlNaiveBayes","title":"Supervised classification using naive Bayes — mlNaiveBayes","text":"Unified (formula-based) interface version naive Bayes algorithm provided e1071::naiveBayes().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlNaiveBayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification using naive Bayes — mlNaiveBayes","text":"","code":"mlNaiveBayes(train, ...)  ml_naive_bayes(train, ...)  # S3 method for formula mlNaiveBayes(formula, data, laplace = 0, ..., subset, na.action)  # S3 method for default mlNaiveBayes(train, response, laplace = 0, ...)  # S3 method for mlNaiveBayes predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\"),   method = c(\"direct\", \"cv\"),   na.action = na.exclude,   threshold = 0.001,   eps = 0,   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlNaiveBayes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification using naive Bayes — mlNaiveBayes","text":"train matrix data frame predictors. ... arguments passed classification method predict() method (used now). formula formula left term factor variable predict right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. laplace positive number controlling Laplace smoothing naive Bayes classifier. default (0) disables Laplace smoothing. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_naive_bayes() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor classes. object mlNaiveBayes object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\", posterior probability \"\" return classes memberships, method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different dataset newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case. threshold value replacing cells probabilities within 'eps' range. eps number specifying epsilon-range apply Laplace smoothing (replace zero close-zero probabilities 'threshold').","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlNaiveBayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification using naive Bayes — mlNaiveBayes","text":"ml_naive_bayes()/mlNaiveBayes() creates mlNaiveBayes, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlNaiveBayes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification using naive Bayes — mlNaiveBayes","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_nb <- ml_naive_bayes(data = iris_train, Species ~ .) summary(iris_nb) #> A mlearning object of class mlNaiveBayes (naive Bayes classifier): #> Initial call: mlNaiveBayes.formula(formula = Species ~ ., data = iris_train) #>  #> Naive Bayes Classifier for Discrete Predictors #>  #> Call: #> naiveBayes.default(x = train, y = response, laplace = laplace,  #>     .args. = ..1) #>  #> A-priori probabilities: #> response #>     setosa versicolor  virginica  #>  0.3333333  0.3333333  0.3333333  #>  #> Conditional probabilities: #>             Sepal.Length #> response         [,1]      [,2] #>   setosa     5.048485 0.3725933 #>   versicolor 6.027273 0.5392545 #>   virginica  6.642424 0.7088857 #>  #>             Sepal.Width #> response         [,1]      [,2] #>   setosa     3.478788 0.3805897 #>   versicolor 2.763636 0.3267436 #>   virginica  2.951515 0.3545430 #>  #>             Petal.Length #> response         [,1]      [,2] #>   setosa     1.478788 0.1781109 #>   versicolor 4.284848 0.4651083 #>   virginica  5.642424 0.6179757 #>  #>             Petal.Width #> response          [,1]      [,2] #>   setosa     0.2454545 0.1033529 #>   versicolor 1.3303030 0.2157615 #>   virginica  2.0090909 0.2466825 #>  predict(iris_nb) # Default type is class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_nb, type = \"membership\") #>               setosa   versicolor    virginica #>   [1,]  1.000000e+00 1.803036e-16 1.167697e-24 #>   [2,]  1.000000e+00 1.352715e-17 1.831338e-25 #>   [3,]  1.000000e+00 1.772241e-16 1.353873e-24 #>   [4,]  1.000000e+00 5.692804e-18 1.376409e-25 #>   [5,]  1.000000e+00 6.093062e-14 6.531376e-21 #>   [6,]  1.000000e+00 8.826228e-17 2.837739e-24 #>   [7,]  1.000000e+00 7.437194e-17 8.757620e-25 #>   [8,]  1.000000e+00 1.216747e-16 9.720500e-25 #>   [9,]  1.000000e+00 6.724341e-17 2.531533e-25 #>  [10,]  1.000000e+00 4.566759e-17 1.186716e-24 #>  [11,]  1.000000e+00 1.879913e-16 1.981529e-24 #>  [12,]  1.000000e+00 2.686953e-17 1.085926e-25 #>  [13,]  1.000000e+00 1.650638e-18 2.251328e-26 #>  [14,]  1.000000e+00 2.368402e-18 3.908159e-25 #>  [15,]  1.000000e+00 1.761543e-16 1.792568e-22 #>  [16,]  1.000000e+00 2.706070e-16 6.524103e-23 #>  [17,]  1.000000e+00 1.684941e-16 5.447395e-24 #>  [18,]  1.000000e+00 2.886607e-14 1.441825e-21 #>  [19,]  1.000000e+00 6.739186e-17 4.013941e-24 #>  [20,]  1.000000e+00 9.227945e-15 7.619346e-23 #>  [21,]  1.000000e+00 3.076912e-15 2.554200e-22 #>  [22,]  1.000000e+00 2.412671e-19 2.082673e-26 #>  [23,]  1.000000e+00 5.291974e-11 2.164940e-18 #>  [24,]  1.000000e+00 8.216570e-14 5.480528e-22 #>  [25,]  1.000000e+00 3.650319e-15 1.524151e-23 #>  [26,]  1.000000e+00 7.652882e-14 2.445210e-21 #>  [27,]  1.000000e+00 7.812216e-17 1.149116e-24 #>  [28,]  1.000000e+00 4.476095e-17 6.365960e-25 #>  [29,]  1.000000e+00 5.252216e-16 3.756987e-24 #>  [30,]  1.000000e+00 1.184327e-15 6.488869e-24 #>  [31,]  1.000000e+00 8.338509e-14 3.263362e-21 #>  [32,]  1.000000e+00 1.690882e-19 8.721554e-27 #>  [33,]  1.000000e+00 4.316419e-19 7.033715e-26 #>  [34,] 7.689655e-103 9.159085e-01 8.409150e-02 #>  [35,]  1.096673e-96 9.667123e-01 3.328772e-02 #>  [36,] 1.930514e-116 6.802159e-01 3.197841e-01 #>  [37,]  1.102081e-67 9.999412e-01 5.879818e-05 #>  [38,] 3.602267e-102 9.709066e-01 2.909345e-02 #>  [39,]  3.335613e-86 9.993121e-01 6.878536e-04 #>  [40,] 1.129911e-109 7.783585e-01 2.216415e-01 #>  [41,]  2.982269e-33 9.999994e-01 5.938056e-07 #>  [42,]  4.546204e-93 9.957187e-01 4.281285e-03 #>  [43,]  2.245740e-67 9.998022e-01 1.977854e-04 #>  [44,]  1.295311e-39 9.999994e-01 6.198654e-07 #>  [45,]  8.639091e-84 9.962009e-01 3.799095e-03 #>  [46,]  1.306052e-57 9.999962e-01 3.800063e-06 #>  [47,] 5.129110e-100 9.913456e-01 8.654381e-03 #>  [48,]  1.247004e-53 9.999529e-01 4.710380e-05 #>  [49,]  2.751640e-89 9.894678e-01 1.053225e-02 #>  [50,]  9.204587e-95 9.910781e-01 8.921933e-03 #>  [51,]  1.056753e-59 9.999929e-01 7.109959e-06 #>  [52,]  4.759450e-98 9.939246e-01 6.075414e-03 #>  [53,]  4.172627e-56 9.999931e-01 6.930611e-06 #>  [54,] 1.147499e-124 2.598343e-01 7.401657e-01 #>  [55,]  2.430851e-68 9.998174e-01 1.826315e-04 #>  [56,] 3.305367e-115 9.485689e-01 5.143106e-02 #>  [57,]  1.322349e-91 9.991780e-01 8.219604e-04 #>  [58,]  3.383621e-80 9.990860e-01 9.139928e-04 #>  [59,]  4.018503e-89 9.929331e-01 7.066888e-03 #>  [60,] 2.885304e-107 9.596243e-01 4.037565e-02 #>  [61,] 1.500417e-131 1.709929e-01 8.290071e-01 #>  [62,]  4.494759e-96 9.893143e-01 1.068568e-02 #>  [63,]  5.408043e-40 9.999988e-01 1.216458e-06 #>  [64,]  1.002758e-52 9.999955e-01 4.522882e-06 #>  [65,]  5.628947e-46 9.999987e-01 1.332260e-06 #>  [66,]  4.602589e-60 9.999711e-01 2.887912e-05 #>  [67,] 4.079871e-244 3.576071e-09 1.000000e+00 #>  [68,] 3.689196e-146 5.139596e-02 9.486040e-01 #>  [69,] 4.834773e-210 1.388701e-06 9.999986e-01 #>  [70,] 1.096501e-167 4.868244e-03 9.951318e-01 #>  [71,] 1.614505e-208 2.397766e-06 9.999976e-01 #>  [72,] 1.416791e-258 1.522784e-09 1.000000e+00 #>  [73,] 4.215784e-105 9.531740e-01 4.682597e-02 #>  [74,] 1.247937e-215 3.770575e-06 9.999962e-01 #>  [75,] 1.109378e-181 1.112506e-03 9.988875e-01 #>  [76,] 1.716419e-254 1.088139e-10 1.000000e+00 #>  [77,] 4.973153e-155 2.017456e-03 9.979825e-01 #>  [78,] 1.101493e-158 8.236383e-03 9.917636e-01 #>  [79,] 3.008552e-185 3.859199e-05 9.999614e-01 #>  [80,] 3.744500e-148 2.931546e-02 9.706845e-01 #>  [81,] 3.347013e-184 2.234062e-05 9.999777e-01 #>  [82,] 8.606627e-188 8.413290e-06 9.999916e-01 #>  [83,] 7.840619e-163 5.273864e-03 9.947261e-01 #>  [84,] 1.759992e-272 1.484798e-11 1.000000e+00 #>  [85,] 8.472771e-297 6.900804e-12 1.000000e+00 #>  [86,] 2.360925e-119 9.577011e-01 4.229886e-02 #>  [87,] 2.229069e-212 2.646252e-07 9.999997e-01 #>  [88,] 1.285230e-142 3.410444e-02 9.658956e-01 #>  [89,] 2.856722e-259 2.856759e-09 1.000000e+00 #>  [90,] 1.186200e-131 2.371171e-01 7.628829e-01 #>  [91,] 1.619220e-195 7.614926e-06 9.999924e-01 #>  [92,] 1.704706e-195 2.566160e-05 9.999743e-01 #>  [93,] 1.903352e-126 3.334420e-01 6.665580e-01 #>  [94,] 2.236975e-130 2.044628e-01 7.955372e-01 #>  [95,] 3.459428e-189 6.688505e-05 9.999331e-01 #>  [96,] 1.646362e-171 1.997026e-03 9.980030e-01 #>  [97,] 2.796148e-210 3.580998e-06 9.999964e-01 #>  [98,] 1.557094e-238 1.596656e-09 1.000000e+00 #>  [99,] 7.874332e-197 1.449503e-05 9.999855e-01 predict(iris_nb, type = \"both\") #> $class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>               setosa   versicolor    virginica #>   [1,]  1.000000e+00 1.803036e-16 1.167697e-24 #>   [2,]  1.000000e+00 1.352715e-17 1.831338e-25 #>   [3,]  1.000000e+00 1.772241e-16 1.353873e-24 #>   [4,]  1.000000e+00 5.692804e-18 1.376409e-25 #>   [5,]  1.000000e+00 6.093062e-14 6.531376e-21 #>   [6,]  1.000000e+00 8.826228e-17 2.837739e-24 #>   [7,]  1.000000e+00 7.437194e-17 8.757620e-25 #>   [8,]  1.000000e+00 1.216747e-16 9.720500e-25 #>   [9,]  1.000000e+00 6.724341e-17 2.531533e-25 #>  [10,]  1.000000e+00 4.566759e-17 1.186716e-24 #>  [11,]  1.000000e+00 1.879913e-16 1.981529e-24 #>  [12,]  1.000000e+00 2.686953e-17 1.085926e-25 #>  [13,]  1.000000e+00 1.650638e-18 2.251328e-26 #>  [14,]  1.000000e+00 2.368402e-18 3.908159e-25 #>  [15,]  1.000000e+00 1.761543e-16 1.792568e-22 #>  [16,]  1.000000e+00 2.706070e-16 6.524103e-23 #>  [17,]  1.000000e+00 1.684941e-16 5.447395e-24 #>  [18,]  1.000000e+00 2.886607e-14 1.441825e-21 #>  [19,]  1.000000e+00 6.739186e-17 4.013941e-24 #>  [20,]  1.000000e+00 9.227945e-15 7.619346e-23 #>  [21,]  1.000000e+00 3.076912e-15 2.554200e-22 #>  [22,]  1.000000e+00 2.412671e-19 2.082673e-26 #>  [23,]  1.000000e+00 5.291974e-11 2.164940e-18 #>  [24,]  1.000000e+00 8.216570e-14 5.480528e-22 #>  [25,]  1.000000e+00 3.650319e-15 1.524151e-23 #>  [26,]  1.000000e+00 7.652882e-14 2.445210e-21 #>  [27,]  1.000000e+00 7.812216e-17 1.149116e-24 #>  [28,]  1.000000e+00 4.476095e-17 6.365960e-25 #>  [29,]  1.000000e+00 5.252216e-16 3.756987e-24 #>  [30,]  1.000000e+00 1.184327e-15 6.488869e-24 #>  [31,]  1.000000e+00 8.338509e-14 3.263362e-21 #>  [32,]  1.000000e+00 1.690882e-19 8.721554e-27 #>  [33,]  1.000000e+00 4.316419e-19 7.033715e-26 #>  [34,] 7.689655e-103 9.159085e-01 8.409150e-02 #>  [35,]  1.096673e-96 9.667123e-01 3.328772e-02 #>  [36,] 1.930514e-116 6.802159e-01 3.197841e-01 #>  [37,]  1.102081e-67 9.999412e-01 5.879818e-05 #>  [38,] 3.602267e-102 9.709066e-01 2.909345e-02 #>  [39,]  3.335613e-86 9.993121e-01 6.878536e-04 #>  [40,] 1.129911e-109 7.783585e-01 2.216415e-01 #>  [41,]  2.982269e-33 9.999994e-01 5.938056e-07 #>  [42,]  4.546204e-93 9.957187e-01 4.281285e-03 #>  [43,]  2.245740e-67 9.998022e-01 1.977854e-04 #>  [44,]  1.295311e-39 9.999994e-01 6.198654e-07 #>  [45,]  8.639091e-84 9.962009e-01 3.799095e-03 #>  [46,]  1.306052e-57 9.999962e-01 3.800063e-06 #>  [47,] 5.129110e-100 9.913456e-01 8.654381e-03 #>  [48,]  1.247004e-53 9.999529e-01 4.710380e-05 #>  [49,]  2.751640e-89 9.894678e-01 1.053225e-02 #>  [50,]  9.204587e-95 9.910781e-01 8.921933e-03 #>  [51,]  1.056753e-59 9.999929e-01 7.109959e-06 #>  [52,]  4.759450e-98 9.939246e-01 6.075414e-03 #>  [53,]  4.172627e-56 9.999931e-01 6.930611e-06 #>  [54,] 1.147499e-124 2.598343e-01 7.401657e-01 #>  [55,]  2.430851e-68 9.998174e-01 1.826315e-04 #>  [56,] 3.305367e-115 9.485689e-01 5.143106e-02 #>  [57,]  1.322349e-91 9.991780e-01 8.219604e-04 #>  [58,]  3.383621e-80 9.990860e-01 9.139928e-04 #>  [59,]  4.018503e-89 9.929331e-01 7.066888e-03 #>  [60,] 2.885304e-107 9.596243e-01 4.037565e-02 #>  [61,] 1.500417e-131 1.709929e-01 8.290071e-01 #>  [62,]  4.494759e-96 9.893143e-01 1.068568e-02 #>  [63,]  5.408043e-40 9.999988e-01 1.216458e-06 #>  [64,]  1.002758e-52 9.999955e-01 4.522882e-06 #>  [65,]  5.628947e-46 9.999987e-01 1.332260e-06 #>  [66,]  4.602589e-60 9.999711e-01 2.887912e-05 #>  [67,] 4.079871e-244 3.576071e-09 1.000000e+00 #>  [68,] 3.689196e-146 5.139596e-02 9.486040e-01 #>  [69,] 4.834773e-210 1.388701e-06 9.999986e-01 #>  [70,] 1.096501e-167 4.868244e-03 9.951318e-01 #>  [71,] 1.614505e-208 2.397766e-06 9.999976e-01 #>  [72,] 1.416791e-258 1.522784e-09 1.000000e+00 #>  [73,] 4.215784e-105 9.531740e-01 4.682597e-02 #>  [74,] 1.247937e-215 3.770575e-06 9.999962e-01 #>  [75,] 1.109378e-181 1.112506e-03 9.988875e-01 #>  [76,] 1.716419e-254 1.088139e-10 1.000000e+00 #>  [77,] 4.973153e-155 2.017456e-03 9.979825e-01 #>  [78,] 1.101493e-158 8.236383e-03 9.917636e-01 #>  [79,] 3.008552e-185 3.859199e-05 9.999614e-01 #>  [80,] 3.744500e-148 2.931546e-02 9.706845e-01 #>  [81,] 3.347013e-184 2.234062e-05 9.999777e-01 #>  [82,] 8.606627e-188 8.413290e-06 9.999916e-01 #>  [83,] 7.840619e-163 5.273864e-03 9.947261e-01 #>  [84,] 1.759992e-272 1.484798e-11 1.000000e+00 #>  [85,] 8.472771e-297 6.900804e-12 1.000000e+00 #>  [86,] 2.360925e-119 9.577011e-01 4.229886e-02 #>  [87,] 2.229069e-212 2.646252e-07 9.999997e-01 #>  [88,] 1.285230e-142 3.410444e-02 9.658956e-01 #>  [89,] 2.856722e-259 2.856759e-09 1.000000e+00 #>  [90,] 1.186200e-131 2.371171e-01 7.628829e-01 #>  [91,] 1.619220e-195 7.614926e-06 9.999924e-01 #>  [92,] 1.704706e-195 2.566160e-05 9.999743e-01 #>  [93,] 1.903352e-126 3.334420e-01 6.665580e-01 #>  [94,] 2.236975e-130 2.044628e-01 7.955372e-01 #>  [95,] 3.459428e-189 6.688505e-05 9.999331e-01 #>  [96,] 1.646362e-171 1.997026e-03 9.980030e-01 #>  [97,] 2.796148e-210 3.580998e-06 9.999964e-01 #>  [98,] 1.557094e-238 1.596656e-09 1.000000e+00 #>  [99,] 7.874332e-197 1.449503e-05 9.999855e-01 #>  # Self-consistency, do not use for assessing classifier performances! confusion(iris_nb) #> 99 items classified with 95 true positives (error rate = 4%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 31  2    33      6 #>   03 virginica   0  2 31    33      6 #>   (sum)         33 33 33    99      4 # Use an independent test set instead confusion(predict(iris_nb, newdata = iris_test), iris_test$Species) #> 50 items classified with 47 true positives (error rate = 6%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 16  0    17      6 #>   04 virginica   0  0  2 15    17     12 #>   (sum)         16  1 18 15    50      6  # Another dataset data(\"HouseVotes84\", package = \"mlbench\") house_nb <- ml_naive_bayes(data = HouseVotes84, Class ~ .,   na.action = na.omit) summary(house_nb) #> A mlearning object of class mlNaiveBayes (naive Bayes classifier): #> Initial call: mlNaiveBayes.formula(formula = Class ~ ., data = HouseVotes84,     na.action = na.omit) #>  #> Naive Bayes Classifier for Discrete Predictors #>  #> Call: #> naiveBayes.default(x = train, y = response, laplace = laplace,  #>     .args. = ..1) #>  #> A-priori probabilities: #> response #>   democrat republican  #>  0.5344828  0.4655172  #>  #> Conditional probabilities: #>             V1 #> response             n         y #>   democrat   0.4112903 0.5887097 #>   republican 0.7870370 0.2129630 #>  #>             V2 #> response             n         y #>   democrat   0.5483871 0.4516129 #>   republican 0.5277778 0.4722222 #>  #>             V3 #> response             n         y #>   democrat   0.1451613 0.8548387 #>   republican 0.8425926 0.1574074 #>  #>             V4 #> response               n           y #>   democrat   0.951612903 0.048387097 #>   republican 0.009259259 0.990740741 #>  #>             V5 #> response             n         y #>   democrat   0.7983871 0.2016129 #>   republican 0.0462963 0.9537037 #>  #>             V6 #> response             n         y #>   democrat   0.5564516 0.4435484 #>   republican 0.1296296 0.8703704 #>  #>             V7 #> response             n         y #>   democrat   0.2338710 0.7661290 #>   republican 0.7314815 0.2685185 #>  #>             V8 #> response             n         y #>   democrat   0.1693548 0.8306452 #>   republican 0.8518519 0.1481481 #>  #>             V9 #> response             n         y #>   democrat   0.2096774 0.7903226 #>   republican 0.8611111 0.1388889 #>  #>             V10 #> response             n         y #>   democrat   0.4677419 0.5322581 #>   republican 0.4259259 0.5740741 #>  #>             V11 #> response             n         y #>   democrat   0.4919355 0.5080645 #>   republican 0.8425926 0.1574074 #>  #>             V12 #> response             n         y #>   democrat   0.8709677 0.1290323 #>   republican 0.1481481 0.8518519 #>  #>             V13 #> response             n         y #>   democrat   0.7096774 0.2903226 #>   republican 0.1574074 0.8425926 #>  #>             V14 #> response              n          y #>   democrat   0.65322581 0.34677419 #>   republican 0.01851852 0.98148148 #>  #>             V15 #> response             n         y #>   democrat   0.4032258 0.5967742 #>   republican 0.8888889 0.1111111 #>  #>             V16 #> response              n          y #>   democrat   0.05645161 0.94354839 #>   republican 0.33333333 0.66666667 #>  confusion(house_nb) # Self-consistency #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> 232 items classified with 218 true positives (error rate = 6%) #>                Predicted #> Actual           01  02 (sum) (FNR%) #>   01 democrat   111  13   124     10 #>   02 republican   1 107   108      1 #>   (sum)         112 120   232      6 confusion(cvpredict(house_nb), na.omit(HouseVotes84)$Class) #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V1'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V2'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V3'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V4'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V5'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V6'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V7'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V8'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V9'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V10'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V11'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V12'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V13'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V14'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V15'. Did you use factors with numeric labels for training, and numeric values for new data? #> Warning: Type mismatch between training and new data for variable 'V16'. Did you use factors with numeric labels for training, and numeric values for new data? #> 232 items classified with 218 true positives (error rate = 6%) #>                Predicted #> Actual           01  02 (sum) (FNR%) #>   01 democrat   111  13   124     10 #>   02 republican   1 107   108      1 #>   (sum)         112 120   232      6"},{"path":"https://www.sciviews.org/mlearning/reference/mlNnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification and regression using neural network — mlNnet","title":"Supervised classification and regression using neural network — mlNnet","text":"Unified (formula-based) interface version single-hidden-layer neural network algorithm, possibly skip-layer connections provided nnet::nnet().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlNnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification and regression using neural network — mlNnet","text":"","code":"mlNnet(train, ...)  ml_nnet(train, ...)  # S3 method for formula mlNnet(   formula,   data,   size = NULL,   rang = NULL,   decay = 0,   maxit = 1000,   ...,   subset,   na.action )  # S3 method for default mlNnet(train, response, size = NULL, rang = NULL, decay = 0, maxit = 1000, ...)  # S3 method for mlNnet predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\", \"raw\"),   method = c(\"direct\", \"cv\"),   na.action = na.exclude,   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlNnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification and regression using neural network — mlNnet","text":"train matrix data frame predictors. ... arguments passed nnet::nnet() many parameters (see help page). formula formula left term factor variable predict (supervised classification), vector numbers (regression) right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. size number units hidden layer. Can zero skip-layer units. NULL (default), reasonable value computed. rang initial random weights [-rang, rang]. Value 0.5 unless inputs large, case chosen rang * max(|x|) 1. NULL, reasonable default computed. decay parameter weight decay. Default 0. maxit maximum number iterations. Default 1000 (100 nnet::nnet()). subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_nnet() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor (classification) numeric (regression). object mlNnet object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes, \"\" return classes memberships. Also type \"raw\" non normalized result returned nnet::nnet() (useful regression, see examples). method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different data set newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlNnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification and regression using neural network — mlNnet","text":"ml_nnet()/mlNnet() creates mlNnet, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlNnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification and regression using neural network — mlNnet","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  set.seed(689) # Useful for reproductibility, use a different value each time! iris_nnet <- ml_nnet(data = iris_train, Species ~ .) #> # weights:  19 #> initial  value 109.026448  #> iter  10 value 39.094573 #> iter  20 value 5.901073 #> iter  30 value 0.126987 #> iter  40 value 0.000192 #> final  value 0.000054  #> converged summary(iris_nnet) #> A mlearning object of class mlNnet (single-hidden-layer neural network): #> Initial call: mlNnet.formula(formula = Species ~ ., data = iris_train) #> a 4-2-3 network with 19 weights #> options were - softmax modelling  #>   b->h1  i1->h1  i2->h1  i3->h1  i4->h1  #>  -42.30 -213.56 -147.78  -62.49  -10.54  #>   b->h2  i1->h2  i2->h2  i3->h2  i4->h2  #>   -3.14   -0.13   -1.10    0.78    2.82  #>   b->o1  h1->o1  h2->o1  #>  188.59   46.07 -343.82  #>   b->o2  h1->o2  h2->o2  #>  146.83  -16.17 -121.13  #>   b->o3  h1->o3  h2->o3  #> -335.40  -29.85  464.76  predict(iris_nnet) # Default type is class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_nnet, type = \"membership\") #>               setosa   versicolor     virginica #>   [1,]  1.000000e+00 1.981261e-18 9.954308e-227 #>   [2,]  1.000000e+00 1.563429e-18 4.212192e-227 #>   [3,]  1.000000e+00 1.991958e-18 1.015086e-226 #>   [4,]  1.000000e+00 1.218574e-18 1.704251e-227 #>   [5,]  1.000000e+00 1.581490e-18 4.391585e-227 #>   [6,]  1.000000e+00 1.768573e-18 6.590714e-227 #>   [7,]  1.000000e+00 1.452927e-18 3.227868e-227 #>   [8,]  1.000000e+00 2.388205e-18 1.961545e-226 #>   [9,]  1.000000e+00 1.517479e-18 3.779780e-227 #>  [10,]  1.000000e+00 1.170688e-18 1.473378e-227 #>  [11,]  1.000000e+00 1.564473e-18 4.222421e-227 #>  [12,]  1.000000e+00 1.568725e-18 4.264236e-227 #>  [13,]  1.000000e+00 1.392460e-18 2.766197e-227 #>  [14,]  1.000000e+00 9.449426e-19 6.768557e-228 #>  [15,]  1.000000e+00 1.057297e-18 1.017797e-227 #>  [16,]  1.000000e+00 1.287978e-18 2.083934e-227 #>  [17,]  1.000000e+00 1.538456e-18 3.972978e-227 #>  [18,]  1.000000e+00 1.368858e-18 2.599712e-227 #>  [19,]  1.000000e+00 1.304507e-18 2.182693e-227 #>  [20,]  1.000000e+00 1.567544e-18 4.252587e-227 #>  [21,]  1.000000e+00 1.717278e-18 5.922687e-227 #>  [22,]  1.000000e+00 1.084511e-18 1.116184e-227 #>  [23,]  1.000000e+00 5.626048e-18 4.403810e-225 #>  [24,]  1.000000e+00 1.908550e-18 8.690599e-227 #>  [25,]  1.000000e+00 2.307574e-18 1.731561e-226 #>  [26,]  1.000000e+00 2.675016e-18 2.961044e-226 #>  [27,]  1.000000e+00 1.332828e-18 2.359724e-227 #>  [28,]  1.000000e+00 1.359101e-18 2.533053e-227 #>  [29,]  1.000000e+00 1.906942e-18 8.664044e-227 #>  [30,]  1.000000e+00 2.102434e-18 1.234872e-226 #>  [31,]  1.000000e+00 2.290961e-18 1.686721e-226 #>  [32,]  1.000000e+00 9.262816e-19 6.295678e-228 #>  [33,]  1.000000e+00 9.386502e-19 6.606327e-228 #>  [34,]  1.696393e-32 1.000000e+00  7.388244e-79 #>  [35,]  2.536259e-37 1.000000e+00  3.666456e-66 #>  [36,]  1.385692e-45 1.000000e+00  2.010898e-44 #>  [37,]  6.596643e-41 1.000000e+00  9.913570e-57 #>  [38,]  9.642900e-49 1.000000e+00  4.081612e-36 #>  [39,]  1.633207e-36 1.000000e+00  2.729769e-68 #>  [40,]  3.755319e-45 1.000000e+00  1.459475e-45 #>  [41,]  1.959318e-08 1.000000e+00 3.619331e-142 #>  [42,]  5.103285e-33 1.000000e+00  1.742208e-77 #>  [43,]  3.411322e-36 1.000000e+00  3.930977e-69 #>  [44,]  7.470698e-21 1.000000e+00 1.710007e-109 #>  [45,]  1.713477e-38 1.000000e+00  4.399548e-63 #>  [46,]  7.171178e-22 1.000000e+00 8.143408e-107 #>  [47,]  5.543993e-43 1.000000e+00  2.864052e-51 #>  [48,]  1.519393e-17 1.000000e+00 3.378024e-118 #>  [49,]  2.045539e-30 1.000000e+00  2.469336e-84 #>  [50,]  9.761421e-45 1.000000e+00  1.182121e-46 #>  [51,]  5.488221e-12 1.000000e+00 8.051914e-133 #>  [52,]  4.410224e-60 9.999972e-01  2.794471e-06 #>  [53,]  4.371794e-20 1.000000e+00 1.637552e-111 #>  [54,]  2.738495e-60 9.999902e-01  9.789977e-06 #>  [55,]  6.677921e-26 1.000000e+00  3.284325e-96 #>  [56,]  1.358014e-59 9.999999e-01  1.449382e-07 #>  [57,]  2.769750e-32 1.000000e+00  2.034009e-79 #>  [58,]  5.720934e-28 1.000000e+00  9.021752e-91 #>  [59,]  2.188126e-33 1.000000e+00  1.617118e-76 #>  [60,]  3.139089e-45 1.000000e+00  2.338872e-45 #>  [61,]  3.324344e-60 9.999941e-01  5.878531e-06 #>  [62,]  4.647318e-46 1.000000e+00  3.562305e-43 #>  [63,]  1.989700e-05 9.999801e-01 4.444841e-150 #>  [64,]  4.028926e-21 1.000000e+00 8.681052e-109 #>  [65,]  3.708411e-13 1.000000e+00 9.657659e-130 #>  [66,]  6.520096e-21 1.000000e+00 2.446313e-109 #>  [67,] 6.639578e-119 8.071938e-42  1.000000e+00 #>  [68,]  2.180618e-97 3.145877e-26  1.000000e+00 #>  [69,] 2.539473e-110 1.336328e-35  1.000000e+00 #>  [70,]  1.064604e-92 7.854112e-23  1.000000e+00 #>  [71,] 1.254516e-113 5.372777e-38  1.000000e+00 #>  [72,] 1.481366e-115 2.154316e-39  1.000000e+00 #>  [73,]  4.403234e-74 2.424556e-09  1.000000e+00 #>  [74,] 6.760041e-103 3.207750e-30  1.000000e+00 #>  [75,] 2.629593e-105 5.752754e-32  1.000000e+00 #>  [76,] 1.463118e-116 4.025418e-40  1.000000e+00 #>  [77,]  5.092016e-87 1.024699e-18  1.000000e+00 #>  [78,]  2.510455e-99 1.238431e-27  1.000000e+00 #>  [79,] 3.968675e-106 1.461521e-32  1.000000e+00 #>  [80,] 1.943255e-106 8.711557e-33  1.000000e+00 #>  [81,] 1.552111e-116 4.201383e-40  1.000000e+00 #>  [82,] 8.298563e-110 3.151694e-35  1.000000e+00 #>  [83,]  5.440640e-86 5.702084e-18  1.000000e+00 #>  [84,] 1.326191e-109 4.426643e-35  1.000000e+00 #>  [85,] 1.052184e-121 7.552117e-44  1.000000e+00 #>  [86,]  6.402383e-75 5.995593e-10  1.000000e+00 #>  [87,] 6.599996e-113 1.789299e-37  1.000000e+00 #>  [88,]  2.243228e-98 6.054134e-27  1.000000e+00 #>  [89,] 1.356615e-115 2.021275e-39  1.000000e+00 #>  [90,]  5.449674e-82 4.518006e-15  1.000000e+00 #>  [91,] 4.102581e-103 2.233783e-30  1.000000e+00 #>  [92,]  1.092023e-88 6.331265e-20  1.000000e+00 #>  [93,]  5.403221e-75 5.301971e-10  1.000000e+00 #>  [94,]  3.242733e-69 8.153604e-06  9.999918e-01 #>  [95,] 2.728115e-111 2.653774e-36  1.000000e+00 #>  [96,]  2.992629e-69 7.692923e-06  9.999923e-01 #>  [97,] 5.651859e-107 3.560132e-33  1.000000e+00 #>  [98,]  1.643945e-92 1.076039e-22  1.000000e+00 #>  [99,] 2.121284e-114 1.482171e-38  1.000000e+00 predict(iris_nnet, type = \"both\") #> $class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>               setosa   versicolor     virginica #>   [1,]  1.000000e+00 1.981261e-18 9.954308e-227 #>   [2,]  1.000000e+00 1.563429e-18 4.212192e-227 #>   [3,]  1.000000e+00 1.991958e-18 1.015086e-226 #>   [4,]  1.000000e+00 1.218574e-18 1.704251e-227 #>   [5,]  1.000000e+00 1.581490e-18 4.391585e-227 #>   [6,]  1.000000e+00 1.768573e-18 6.590714e-227 #>   [7,]  1.000000e+00 1.452927e-18 3.227868e-227 #>   [8,]  1.000000e+00 2.388205e-18 1.961545e-226 #>   [9,]  1.000000e+00 1.517479e-18 3.779780e-227 #>  [10,]  1.000000e+00 1.170688e-18 1.473378e-227 #>  [11,]  1.000000e+00 1.564473e-18 4.222421e-227 #>  [12,]  1.000000e+00 1.568725e-18 4.264236e-227 #>  [13,]  1.000000e+00 1.392460e-18 2.766197e-227 #>  [14,]  1.000000e+00 9.449426e-19 6.768557e-228 #>  [15,]  1.000000e+00 1.057297e-18 1.017797e-227 #>  [16,]  1.000000e+00 1.287978e-18 2.083934e-227 #>  [17,]  1.000000e+00 1.538456e-18 3.972978e-227 #>  [18,]  1.000000e+00 1.368858e-18 2.599712e-227 #>  [19,]  1.000000e+00 1.304507e-18 2.182693e-227 #>  [20,]  1.000000e+00 1.567544e-18 4.252587e-227 #>  [21,]  1.000000e+00 1.717278e-18 5.922687e-227 #>  [22,]  1.000000e+00 1.084511e-18 1.116184e-227 #>  [23,]  1.000000e+00 5.626048e-18 4.403810e-225 #>  [24,]  1.000000e+00 1.908550e-18 8.690599e-227 #>  [25,]  1.000000e+00 2.307574e-18 1.731561e-226 #>  [26,]  1.000000e+00 2.675016e-18 2.961044e-226 #>  [27,]  1.000000e+00 1.332828e-18 2.359724e-227 #>  [28,]  1.000000e+00 1.359101e-18 2.533053e-227 #>  [29,]  1.000000e+00 1.906942e-18 8.664044e-227 #>  [30,]  1.000000e+00 2.102434e-18 1.234872e-226 #>  [31,]  1.000000e+00 2.290961e-18 1.686721e-226 #>  [32,]  1.000000e+00 9.262816e-19 6.295678e-228 #>  [33,]  1.000000e+00 9.386502e-19 6.606327e-228 #>  [34,]  1.696393e-32 1.000000e+00  7.388244e-79 #>  [35,]  2.536259e-37 1.000000e+00  3.666456e-66 #>  [36,]  1.385692e-45 1.000000e+00  2.010898e-44 #>  [37,]  6.596643e-41 1.000000e+00  9.913570e-57 #>  [38,]  9.642900e-49 1.000000e+00  4.081612e-36 #>  [39,]  1.633207e-36 1.000000e+00  2.729769e-68 #>  [40,]  3.755319e-45 1.000000e+00  1.459475e-45 #>  [41,]  1.959318e-08 1.000000e+00 3.619331e-142 #>  [42,]  5.103285e-33 1.000000e+00  1.742208e-77 #>  [43,]  3.411322e-36 1.000000e+00  3.930977e-69 #>  [44,]  7.470698e-21 1.000000e+00 1.710007e-109 #>  [45,]  1.713477e-38 1.000000e+00  4.399548e-63 #>  [46,]  7.171178e-22 1.000000e+00 8.143408e-107 #>  [47,]  5.543993e-43 1.000000e+00  2.864052e-51 #>  [48,]  1.519393e-17 1.000000e+00 3.378024e-118 #>  [49,]  2.045539e-30 1.000000e+00  2.469336e-84 #>  [50,]  9.761421e-45 1.000000e+00  1.182121e-46 #>  [51,]  5.488221e-12 1.000000e+00 8.051914e-133 #>  [52,]  4.410224e-60 9.999972e-01  2.794471e-06 #>  [53,]  4.371794e-20 1.000000e+00 1.637552e-111 #>  [54,]  2.738495e-60 9.999902e-01  9.789977e-06 #>  [55,]  6.677921e-26 1.000000e+00  3.284325e-96 #>  [56,]  1.358014e-59 9.999999e-01  1.449382e-07 #>  [57,]  2.769750e-32 1.000000e+00  2.034009e-79 #>  [58,]  5.720934e-28 1.000000e+00  9.021752e-91 #>  [59,]  2.188126e-33 1.000000e+00  1.617118e-76 #>  [60,]  3.139089e-45 1.000000e+00  2.338872e-45 #>  [61,]  3.324344e-60 9.999941e-01  5.878531e-06 #>  [62,]  4.647318e-46 1.000000e+00  3.562305e-43 #>  [63,]  1.989700e-05 9.999801e-01 4.444841e-150 #>  [64,]  4.028926e-21 1.000000e+00 8.681052e-109 #>  [65,]  3.708411e-13 1.000000e+00 9.657659e-130 #>  [66,]  6.520096e-21 1.000000e+00 2.446313e-109 #>  [67,] 6.639578e-119 8.071938e-42  1.000000e+00 #>  [68,]  2.180618e-97 3.145877e-26  1.000000e+00 #>  [69,] 2.539473e-110 1.336328e-35  1.000000e+00 #>  [70,]  1.064604e-92 7.854112e-23  1.000000e+00 #>  [71,] 1.254516e-113 5.372777e-38  1.000000e+00 #>  [72,] 1.481366e-115 2.154316e-39  1.000000e+00 #>  [73,]  4.403234e-74 2.424556e-09  1.000000e+00 #>  [74,] 6.760041e-103 3.207750e-30  1.000000e+00 #>  [75,] 2.629593e-105 5.752754e-32  1.000000e+00 #>  [76,] 1.463118e-116 4.025418e-40  1.000000e+00 #>  [77,]  5.092016e-87 1.024699e-18  1.000000e+00 #>  [78,]  2.510455e-99 1.238431e-27  1.000000e+00 #>  [79,] 3.968675e-106 1.461521e-32  1.000000e+00 #>  [80,] 1.943255e-106 8.711557e-33  1.000000e+00 #>  [81,] 1.552111e-116 4.201383e-40  1.000000e+00 #>  [82,] 8.298563e-110 3.151694e-35  1.000000e+00 #>  [83,]  5.440640e-86 5.702084e-18  1.000000e+00 #>  [84,] 1.326191e-109 4.426643e-35  1.000000e+00 #>  [85,] 1.052184e-121 7.552117e-44  1.000000e+00 #>  [86,]  6.402383e-75 5.995593e-10  1.000000e+00 #>  [87,] 6.599996e-113 1.789299e-37  1.000000e+00 #>  [88,]  2.243228e-98 6.054134e-27  1.000000e+00 #>  [89,] 1.356615e-115 2.021275e-39  1.000000e+00 #>  [90,]  5.449674e-82 4.518006e-15  1.000000e+00 #>  [91,] 4.102581e-103 2.233783e-30  1.000000e+00 #>  [92,]  1.092023e-88 6.331265e-20  1.000000e+00 #>  [93,]  5.403221e-75 5.301971e-10  1.000000e+00 #>  [94,]  3.242733e-69 8.153604e-06  9.999918e-01 #>  [95,] 2.728115e-111 2.653774e-36  1.000000e+00 #>  [96,]  2.992629e-69 7.692923e-06  9.999923e-01 #>  [97,] 5.651859e-107 3.560132e-33  1.000000e+00 #>  [98,]  1.643945e-92 1.076039e-22  1.000000e+00 #>  [99,] 2.121284e-114 1.482171e-38  1.000000e+00 #>  # Self-consistency, do not use for assessing classifier performances! confusion(iris_nnet) #> 99 items classified with 99 true positives (error rate = 0%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 virginica  33  0  0    33      0 #>   02 setosa      0 33  0    33      0 #>   03 versicolor  0  0 33    33      0 #>   (sum)         33 33 33    99      0 # Use an independent test set instead confusion(predict(iris_nnet, newdata = iris_test), iris_test$Species) #> 50 items classified with 47 true positives (error rate = 6%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 15  1    17     12 #>   04 virginica   0  0  1 16    17      6 #>   (sum)         16  1 16 17    50      6  # Idem, but two classes prediction data(\"HouseVotes84\", package = \"mlbench\") set.seed(325) house_nnet <- ml_nnet(data = HouseVotes84, Class ~ ., na.action = na.omit) #> # weights:  19 #> initial  value 169.309559  #> iter  10 value 56.372600 #> iter  20 value 25.973675 #> iter  30 value 13.853049 #> iter  40 value 13.799839 #> iter  50 value 13.777997 #> iter  60 value 11.056490 #> iter  70 value 10.065342 #> iter  80 value 10.031942 #> iter  90 value 10.015482 #> iter 100 value 10.013087 #> iter 110 value 10.011137 #> iter 120 value 10.008720 #> iter 130 value 10.005719 #> iter 140 value 10.004402 #> iter 150 value 10.003256 #> iter 160 value 10.002171 #> iter 170 value 10.001886 #> iter 180 value 10.001430 #> iter 190 value 10.001084 #> iter 200 value 10.000223 #> iter 210 value 9.999774 #> iter 220 value 9.999346 #> iter 230 value 9.998816 #> iter 240 value 9.998468 #> iter 250 value 9.998363 #> iter 260 value 9.998319 #> iter 270 value 9.998238 #> iter 280 value 9.998194 #> iter 290 value 9.997054 #> iter 300 value 9.996987 #> iter 310 value 9.996838 #> iter 320 value 9.996751 #> final  value 9.996749  #> converged summary(house_nnet) #> A mlearning object of class mlNnet (single-hidden-layer neural network): #> Initial call: mlNnet.formula(formula = Class ~ ., data = HouseVotes84, na.action = na.omit) #> a 16-1-1 network with 19 weights #> options were - entropy fitting  #>   b->h1  i1->h1  i2->h1  i3->h1  i4->h1  i5->h1  i6->h1  i7->h1  i8->h1  i9->h1  #>  -25.97    8.30    8.57  -26.71   52.83    0.18   -7.20   12.39  -12.27  -11.17  #> i10->h1 i11->h1 i12->h1 i13->h1 i14->h1 i15->h1 i16->h1  #>   14.87  -30.35    5.41  -11.96    5.94   -7.00    9.42  #>   b->o  h1->o  #> -26.46  30.45  # Cross-validated confusion matrix confusion(cvpredict(house_nnet), na.omit(HouseVotes84)$Class) #> # weights:  19 #> initial  value 143.717759  #> iter  10 value 18.848757 #> iter  20 value 9.444928 #> iter  30 value 6.417139 #> iter  40 value 5.774767 #> iter  50 value 5.621749 #> iter  60 value 5.585126 #> iter  70 value 5.583271 #> iter  80 value 5.582734 #> iter  90 value 5.581081 #> iter 100 value 5.580656 #> iter 110 value 5.580538 #> iter 120 value 5.580411 #> iter 130 value 5.580388 #> iter 140 value 5.580370 #> iter 150 value 5.580355 #> iter 160 value 5.580332 #> iter 170 value 5.580127 #> final  value 5.580121  #> converged #> # weights:  19 #> initial  value 143.812486  #> iter  10 value 30.720972 #> iter  20 value 9.782350 #> iter  30 value 5.604171 #> iter  40 value 5.514307 #> iter  50 value 0.093468 #> iter  60 value 0.016101 #> iter  70 value 0.005208 #> iter  80 value 0.002482 #> iter  90 value 0.001895 #> iter 100 value 0.001498 #> iter 110 value 0.001053 #> iter 120 value 0.000671 #> iter 130 value 0.000593 #> iter 140 value 0.000474 #> iter 150 value 0.000410 #> iter 160 value 0.000309 #> iter 170 value 0.000223 #> iter 180 value 0.000210 #> iter 190 value 0.000132 #> iter 200 value 0.000106 #> final  value 0.000099  #> converged #> # weights:  19 #> initial  value 145.016337  #> iter  10 value 10.046476 #> iter  20 value 0.116477 #> iter  30 value 0.000132 #> iter  30 value 0.000063 #> iter  30 value 0.000059 #> final  value 0.000059  #> converged #> # weights:  19 #> initial  value 144.235058  #> iter  10 value 15.299952 #> iter  20 value 13.546904 #> iter  30 value 13.466835 #> iter  40 value 11.189888 #> iter  50 value 9.894254 #> iter  60 value 9.831545 #> iter  70 value 9.808962 #> iter  80 value 9.795122 #> iter  90 value 9.793556 #> iter 100 value 9.791575 #> iter 110 value 9.791049 #> iter 120 value 9.790586 #> iter 130 value 9.790261 #> iter 140 value 9.789627 #> iter 150 value 9.789211 #> iter 160 value 9.788917 #> iter 170 value 9.788296 #> iter 180 value 9.787547 #> iter 190 value 9.787253 #> iter 200 value 9.787207 #> iter 210 value 9.787004 #> iter 220 value 9.786783 #> iter 230 value 9.785834 #> iter 240 value 9.785472 #> iter 250 value 9.784911 #> iter 260 value 9.784756 #> iter 270 value 9.784424 #> final  value 9.784419  #> converged #> # weights:  19 #> initial  value 145.290442  #> iter  10 value 23.029519 #> iter  20 value 13.559441 #> iter  30 value 12.126742 #> iter  40 value 5.965420 #> iter  50 value 1.326633 #> iter  60 value 0.098605 #> iter  70 value 0.014886 #> iter  80 value 0.004322 #> iter  90 value 0.002398 #> iter 100 value 0.001729 #> iter 110 value 0.000896 #> iter 120 value 0.000614 #> iter 130 value 0.000434 #> iter 140 value 0.000375 #> iter 150 value 0.000311 #> iter 160 value 0.000218 #> iter 170 value 0.000191 #> iter 180 value 0.000186 #> iter 190 value 0.000162 #> iter 200 value 0.000134 #> iter 210 value 0.000124 #> iter 220 value 0.000122 #> iter 230 value 0.000109 #> final  value 0.000098  #> converged #> # weights:  19 #> initial  value 136.866761  #> iter  10 value 10.055239 #> iter  20 value 9.042402 #> iter  30 value 0.462250 #> iter  40 value 0.013432 #> iter  50 value 0.002624 #> iter  60 value 0.000764 #> iter  70 value 0.000586 #> iter  80 value 0.000152 #> iter  90 value 0.000143 #> iter 100 value 0.000143 #> iter 110 value 0.000140 #> iter 120 value 0.000140 #> iter 120 value 0.000140 #> final  value 0.000140  #> converged #> # weights:  19 #> initial  value 145.921787  #> iter  10 value 51.158020 #> iter  20 value 20.204569 #> iter  30 value 19.954583 #> iter  40 value 19.953281 #> iter  50 value 19.953130 #> iter  50 value 19.953130 #> iter  50 value 19.953130 #> final  value 19.953130  #> converged #> # weights:  19 #> initial  value 146.064948  #> iter  10 value 18.762028 #> iter  20 value 5.130610 #> iter  30 value 0.027649 #> final  value 0.000051  #> converged #> # weights:  19 #> initial  value 153.194181  #> iter  10 value 37.154579 #> iter  20 value 5.531590 #> iter  30 value 2.236234 #> iter  40 value 0.176689 #> iter  50 value 0.010148 #> iter  60 value 0.000397 #> iter  70 value 0.000179 #> final  value 0.000081  #> converged #> # weights:  19 #> initial  value 148.213782  #> iter  10 value 20.337335 #> iter  20 value 1.795220 #> iter  30 value 0.003358 #> final  value 0.000057  #> converged #> 232 items classified with 219 true positives (error rate = 5.6%) #>                Predicted #> Actual           01  02 (sum) (FNR%) #>   01 democrat   115   9   124      7 #>   02 republican   4 104   108      4 #>   (sum)         119 113   232      6  # Regression data(airquality, package = \"datasets\") set.seed(74) ozone_nnet <- ml_nnet(data = airquality, Ozone ~ ., na.action = na.omit,   skip = TRUE, decay = 1e-3, size = 20, linout = TRUE) #> # weights:  146 #> initial  value 329959.371356  #> iter  10 value 52446.220833 #> iter  20 value 49603.490076 #> iter  30 value 46259.519458 #> iter  40 value 44142.030523 #> iter  50 value 43250.816616 #> iter  60 value 39852.930471 #> iter  70 value 36766.639329 #> iter  80 value 36359.340847 #> iter  90 value 35067.999132 #> iter 100 value 34179.089643 #> iter 110 value 33104.742531 #> iter 120 value 32109.128326 #> iter 130 value 30504.969695 #> iter 140 value 29816.599569 #> iter 150 value 29768.593874 #> iter 160 value 29744.667394 #> iter 170 value 29669.107831 #> iter 180 value 29597.956072 #> iter 190 value 29560.077614 #> iter 200 value 29383.233180 #> iter 210 value 29327.895164 #> iter 220 value 29207.975099 #> iter 230 value 29123.797795 #> iter 240 value 29003.256685 #> iter 250 value 28921.883585 #> iter 260 value 28819.788764 #> iter 270 value 28707.201615 #> iter 280 value 28626.060649 #> iter 290 value 28612.478500 #> iter 300 value 28595.835225 #> iter 310 value 28524.664792 #> iter 320 value 28475.905238 #> iter 330 value 28348.265526 #> iter 340 value 28315.347852 #> iter 350 value 28314.082408 #> iter 350 value 28314.082371 #> iter 350 value 28314.082213 #> final  value 28314.082213  #> converged summary(ozone_nnet) #> A mlearning object of class mlNnet (single-hidden-layer neural network): #> [regression variant] #> Initial call: mlNnet.formula(formula = Ozone ~ ., data = airquality, size = 20,     decay = 0.001, skip = TRUE, linout = TRUE, na.action = na.omit) #> a 5-20-1 network with 146 weights #> options were - skip-layer connections  linear output units  decay=0.001 #>  b->h1 i1->h1 i2->h1 i3->h1 i4->h1 i5->h1  #>  -0.76   5.72 -16.12 -11.64  -5.74 -13.39  #>  b->h2 i1->h2 i2->h2 i3->h2 i4->h2 i5->h2  #>  -0.01   1.39   0.07   0.06   0.02  -0.10  #>  b->h3 i1->h3 i2->h3 i3->h3 i4->h3 i5->h3  #>  -0.29   0.85  -7.25   2.90  -8.27  -3.13  #>  b->h4 i1->h4 i2->h4 i3->h4 i4->h4 i5->h4  #>   0.03   1.59  -0.19   3.13   0.24   0.78  #>  b->h5 i1->h5 i2->h5 i3->h5 i4->h5 i5->h5  #>   7.94   2.46 -23.57   1.39  -4.07 -16.10  #>  b->h6 i1->h6 i2->h6 i3->h6 i4->h6 i5->h6  #>   0.01   1.65   0.32   0.62   0.09   0.02  #>  b->h7 i1->h7 i2->h7 i3->h7 i4->h7 i5->h7  #>   0.01   0.67   0.12   0.58   0.07   0.07  #>  b->h8 i1->h8 i2->h8 i3->h8 i4->h8 i5->h8  #>   0.06   0.38   0.94   2.91   0.39   0.83  #>  b->h9 i1->h9 i2->h9 i3->h9 i4->h9 i5->h9  #>   0.29   2.47   1.59   1.14   4.24  -6.88  #>  b->h10 i1->h10 i2->h10 i3->h10 i4->h10 i5->h10  #>    0.01    3.49    0.10    0.00    0.01   -0.16  #>  b->h11 i1->h11 i2->h11 i3->h11 i4->h11 i5->h11  #>    0.01    1.91    0.22    0.66    0.09    0.07  #>  b->h12 i1->h12 i2->h12 i3->h12 i4->h12 i5->h12  #>    0.04    1.56    0.72    2.32    0.28    0.55  #>  b->h13 i1->h13 i2->h13 i3->h13 i4->h13 i5->h13  #>   -1.44    3.53   -4.33  -17.18   -6.21   20.59  #>  b->h14 i1->h14 i2->h14 i3->h14 i4->h14 i5->h14  #>    0.07    7.32    1.08    3.59    0.45    1.02  #>  b->h15 i1->h15 i2->h15 i3->h15 i4->h15 i5->h15  #>    0.16   -8.91    1.17    8.40    1.56   -0.64  #>  b->h16 i1->h16 i2->h16 i3->h16 i4->h16 i5->h16  #>   -2.17    3.02  -21.18    0.78   -6.75   -2.44  #>  b->h17 i1->h17 i2->h17 i3->h17 i4->h17 i5->h17  #>    0.04    2.61    0.68    3.06    0.34    0.75  #>  b->h18 i1->h18 i2->h18 i3->h18 i4->h18 i5->h18  #>   -0.06    3.87   -1.93   -0.72   -0.72   -1.01  #>  b->h19 i1->h19 i2->h19 i3->h19 i4->h19 i5->h19  #>   -0.04    6.47   -0.99   -2.31   -0.55   -0.90  #>  b->h20 i1->h20 i2->h20 i3->h20 i4->h20 i5->h20  #>    0.01    2.75    0.06   -0.02    0.00   -0.13  #>   b->o  h1->o  h2->o  h3->o  h4->o  h5->o  h6->o  h7->o  h8->o  h9->o h10->o  #>  -9.17 -30.81  -1.46 -36.98   0.41   9.42   2.41  -1.54   3.12  26.63   3.55  #> h11->o h12->o h13->o h14->o h15->o h16->o h17->o h18->o h19->o h20->o  i1->o  #>   0.20   5.03 -19.61   2.28 -16.38 -46.10   2.63 -15.19   1.10   5.83   0.32  #>  i2->o  i3->o  i4->o  i5->o  #>  -4.68   1.29  -1.95   0.06  plot(na.omit(airquality)$Ozone, predict(ozone_nnet, type = \"raw\")) abline(a = 0, b = 1)"},{"path":"https://www.sciviews.org/mlearning/reference/mlQda.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification using quadratic discriminant analysis — mlQda","title":"Supervised classification using quadratic discriminant analysis — mlQda","text":"Unified (formula-based) interface version quadratic discriminant analysis algorithm provided MASS::qda().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlQda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification using quadratic discriminant analysis — mlQda","text":"","code":"mlQda(train, ...)  ml_qda(train, ...)  # S3 method for formula mlQda(formula, data, ..., subset, na.action)  # S3 method for default mlQda(train, response, ...)  # S3 method for mlQda predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\"),   prior = object$prior,   method = c(\"plug-in\", \"predictive\", \"debiased\", \"looCV\", \"cv\"),   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlQda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification using quadratic discriminant analysis — mlQda","text":"train matrix data frame predictors. ... arguments passed MASS::qda()  predict() method (see corresponding help page). formula formula left term factor variable predict right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_qda() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor classification. object mlQda object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes, \"\" return classes memberships. prior prior probabilities class membership. default, prior obtained object , changed, correspond proportions observed training set. method \"plug-\", \"predictive\", \"debiased\", \"looCV\", \"cv\". \"plug-\" (default) usual unbiased parameter estimates used. \"predictive\", parameters integrated using vague prior. \"debiased\", unbiased estimator log posterior probabilities used. \"looCV\", leave-one-cross-validation fits original data set computed returned. \"cv\", cross-validation used instead. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlQda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification using quadratic discriminant analysis — mlQda","text":"ml_qda()/mlQda() creates mlQda, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlQda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification using quadratic discriminant analysis — mlQda","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_qda <- ml_qda(data = iris_train, Species ~ .) summary(iris_qda) #> A mlearning object of class mlQda (quadratic discriminant analysis): #> Initial call: mlQda.formula(formula = Species ~ ., data = iris_train) #> Call: #> qda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>     setosa versicolor  virginica  #>  0.3333333  0.3333333  0.3333333  #>  #> Group means: #>            Sepal.Length Sepal.Width Petal.Length Petal.Width #> setosa         5.048485    3.478788     1.478788   0.2454545 #> versicolor     6.027273    2.763636     4.284848   1.3303030 #> virginica      6.642424    2.951515     5.642424   2.0090909 confusion(iris_qda) #> 99 items classified with 98 true positives (error rate = 1%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 32  1    33      3 #>   03 virginica   0  0 33    33      0 #>   (sum)         33 32 34    99      1 confusion(predict(iris_qda, newdata = iris_test), iris_test$Species) #> 50 items classified with 48 true positives (error rate = 4%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 15  1    17     12 #>   04 virginica   0  0  0 17    17      0 #>   (sum)         16  1 15 18    50      4  # Another dataset (binary predictor... not optimal for qda, just for test) data(\"HouseVotes84\", package = \"mlbench\") house_qda <- ml_qda(data = HouseVotes84, Class ~ ., na.action = na.omit) #> Warning: force conversion from factor to numeric; may be not optimal or suitable summary(house_qda) #> A mlearning object of class mlQda (quadratic discriminant analysis): #> Initial call: mlQda.formula(formula = Class ~ ., data = HouseVotes84, na.action = na.omit) #> Call: #> qda(sapply(train, as.numeric), grouping = response, .args. = ..1) #>  #> Prior probabilities of groups: #>   democrat republican  #>  0.5344828  0.4655172  #>  #> Group means: #>                  V1       V2       V3       V4       V5       V6       V7 #> democrat   1.588710 1.451613 1.854839 1.048387 1.201613 1.443548 1.766129 #> republican 1.212963 1.472222 1.157407 1.990741 1.953704 1.870370 1.268519 #>                  V8       V9      V10      V11      V12      V13      V14 #> democrat   1.830645 1.790323 1.532258 1.508065 1.129032 1.290323 1.346774 #> republican 1.148148 1.138889 1.574074 1.157407 1.851852 1.842593 1.981481 #>                 V15      V16 #> democrat   1.596774 1.943548 #> republican 1.111111 1.666667"},{"path":"https://www.sciviews.org/mlearning/reference/mlRforest.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification and regression using random forest — mlRforest","title":"Supervised classification and regression using random forest — mlRforest","text":"Unified (formula-based) interface version random forest algorithm provided randomForest::randomForest().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlRforest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification and regression using random forest — mlRforest","text":"","code":"mlRforest(train, ...)  ml_rforest(train, ...)  # S3 method for formula mlRforest(   formula,   data,   ntree = 500,   mtry,   replace = TRUE,   classwt = NULL,   ...,   subset,   na.action )  # S3 method for default mlRforest(   train,   response,   ntree = 500,   mtry,   replace = TRUE,   classwt = NULL,   ... )  # S3 method for mlRforest predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\", \"vote\"),   method = c(\"direct\", \"oob\", \"cv\"),   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlRforest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification and regression using random forest — mlRforest","text":"train matrix data frame predictors. ... arguments passed randomForest::randomForest() predict() method. many arguments, see corresponding help page. formula formula left term factor variable predict (supervised classification), vector numbers (regression) nothing (unsupervised classification) right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. ntree number trees generate (use value large enough get least predictions input row). Default 500 trees. mtry number variables randomly sampled candidates split. Note default values different classification (sqrt(p) p number variables x) regression (p/3)? replace sample cases without replacement (TRUE default)? classwt priors classes. Need add one. Ignored regression. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_rforest() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor (classification) numeric (regression), NULL (unsupervised classification). object mlRforest object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes assessed number neighbors classes, \"\" return classes memberships. One can also use \"vote\", returns number trees voted class. method \"direct\" (default), \"oob\" \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances (case Random Forest, metrics certainly falsely indicate perfect classifier). Either use different data set newdata= use alternate approaches: --bag (\"oob\") cross-validation (\"cv\"). --bag approach uses individuals used build trees assess performances. unbiased estimates. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlRforest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification and regression using random forest — mlRforest","text":"ml_rforest()/mlRforest() creates mlRforest, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlRforest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification and regression using random forest — mlRforest","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_rf <- ml_rforest(data = iris_train, Species ~ .) summary(iris_rf) #> A mlearning object of class mlRforest (random forest): #> Initial call: mlRforest.formula(formula = Species ~ ., data = iris_train) #>  #> Call: #>  randomForest(x = train, y = response, ntree = ntree, replace = replace,      classwt = classwt, .args. = ..1)  #>                Type of random forest: classification #>                      Number of trees: 500 #> No. of variables tried at each split: 2 #>  #>         OOB estimate of  error rate: 5.05% #> Confusion matrix: #>            setosa versicolor virginica class.error #> setosa         33          0         0  0.00000000 #> versicolor      0         31         2  0.06060606 #> virginica       0          3        30  0.09090909 plot(iris_rf) # Useful to look at the effect of ntree=  # For such a relatively simple case, 50 trees are enough iris_rf <- ml_rforest(data = iris_train, Species ~ ., ntree = 50) summary(iris_rf) #> A mlearning object of class mlRforest (random forest): #> Initial call: mlRforest.formula(formula = Species ~ ., data = iris_train, ntree = 50) #>  #> Call: #>  randomForest(x = train, y = response, ntree = ntree, replace = replace,      classwt = classwt, .args. = ..1)  #>                Type of random forest: classification #>                      Number of trees: 50 #> No. of variables tried at each split: 2 #>  #>         OOB estimate of  error rate: 6.06% #> Confusion matrix: #>            setosa versicolor virginica class.error #> setosa         33          0         0  0.00000000 #> versicolor      0         31         2  0.06060606 #> virginica       0          4        29  0.12121212 predict(iris_rf) # Default type is class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_rf, type = \"membership\") #>     setosa versicolor virginica #> 2     1.00       0.00      0.00 #> 3     1.00       0.00      0.00 #> 4     1.00       0.00      0.00 #> 5     1.00       0.00      0.00 #> 6     1.00       0.00      0.00 #> 7     1.00       0.00      0.00 #> 8     1.00       0.00      0.00 #> 9     1.00       0.00      0.00 #> 10    1.00       0.00      0.00 #> 11    1.00       0.00      0.00 #> 12    1.00       0.00      0.00 #> 13    1.00       0.00      0.00 #> 14    1.00       0.00      0.00 #> 15    0.96       0.04      0.00 #> 16    0.98       0.02      0.00 #> 17    1.00       0.00      0.00 #> 18    1.00       0.00      0.00 #> 19    0.98       0.02      0.00 #> 20    1.00       0.00      0.00 #> 21    1.00       0.00      0.00 #> 22    1.00       0.00      0.00 #> 23    1.00       0.00      0.00 #> 24    1.00       0.00      0.00 #> 25    1.00       0.00      0.00 #> 26    1.00       0.00      0.00 #> 27    1.00       0.00      0.00 #> 28    1.00       0.00      0.00 #> 29    1.00       0.00      0.00 #> 30    1.00       0.00      0.00 #> 31    1.00       0.00      0.00 #> 32    1.00       0.00      0.00 #> 33    1.00       0.00      0.00 #> 34    1.00       0.00      0.00 #> 51    0.00       0.90      0.10 #> 52    0.00       1.00      0.00 #> 53    0.00       0.92      0.08 #> 54    0.00       0.96      0.04 #> 55    0.00       1.00      0.00 #> 56    0.00       1.00      0.00 #> 57    0.00       0.96      0.04 #> 58    0.00       0.94      0.06 #> 59    0.00       1.00      0.00 #> 60    0.00       0.98      0.02 #> 61    0.00       0.94      0.06 #> 62    0.00       1.00      0.00 #> 63    0.00       0.98      0.02 #> 64    0.00       1.00      0.00 #> 65    0.00       1.00      0.00 #> 66    0.00       1.00      0.00 #> 67    0.00       1.00      0.00 #> 68    0.00       1.00      0.00 #> 69    0.00       0.94      0.06 #> 70    0.00       0.98      0.02 #> 71    0.00       0.72      0.28 #> 72    0.00       1.00      0.00 #> 73    0.00       0.90      0.10 #> 74    0.00       1.00      0.00 #> 75    0.00       1.00      0.00 #> 76    0.00       1.00      0.00 #> 77    0.00       1.00      0.00 #> 78    0.00       0.70      0.30 #> 79    0.00       1.00      0.00 #> 80    0.00       1.00      0.00 #> 81    0.00       1.00      0.00 #> 82    0.00       1.00      0.00 #> 83    0.00       1.00      0.00 #> 101   0.00       0.00      1.00 #> 102   0.00       0.02      0.98 #> 103   0.00       0.00      1.00 #> 104   0.00       0.00      1.00 #> 105   0.00       0.00      1.00 #> 106   0.00       0.00      1.00 #> 107   0.00       0.36      0.64 #> 108   0.00       0.02      0.98 #> 109   0.00       0.02      0.98 #> 110   0.00       0.00      1.00 #> 111   0.00       0.04      0.96 #> 112   0.00       0.00      1.00 #> 113   0.00       0.00      1.00 #> 114   0.00       0.00      1.00 #> 115   0.00       0.00      1.00 #> 116   0.00       0.02      0.98 #> 117   0.00       0.00      1.00 #> 118   0.00       0.00      1.00 #> 119   0.00       0.00      1.00 #> 120   0.00       0.32      0.68 #> 121   0.00       0.02      0.98 #> 122   0.00       0.02      0.98 #> 123   0.00       0.00      1.00 #> 124   0.00       0.02      0.98 #> 125   0.00       0.02      0.98 #> 126   0.00       0.04      0.96 #> 127   0.00       0.26      0.74 #> 128   0.00       0.08      0.92 #> 129   0.00       0.00      1.00 #> 130   0.00       0.24      0.76 #> 131   0.00       0.00      1.00 #> 132   0.00       0.00      1.00 #> 133   0.00       0.00      1.00 predict(iris_rf, type = \"both\") #> $class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] versicolor versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  virginica  virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>     setosa versicolor virginica #> 2     1.00       0.00      0.00 #> 3     1.00       0.00      0.00 #> 4     1.00       0.00      0.00 #> 5     1.00       0.00      0.00 #> 6     1.00       0.00      0.00 #> 7     1.00       0.00      0.00 #> 8     1.00       0.00      0.00 #> 9     1.00       0.00      0.00 #> 10    1.00       0.00      0.00 #> 11    1.00       0.00      0.00 #> 12    1.00       0.00      0.00 #> 13    1.00       0.00      0.00 #> 14    1.00       0.00      0.00 #> 15    0.96       0.04      0.00 #> 16    0.98       0.02      0.00 #> 17    1.00       0.00      0.00 #> 18    1.00       0.00      0.00 #> 19    0.98       0.02      0.00 #> 20    1.00       0.00      0.00 #> 21    1.00       0.00      0.00 #> 22    1.00       0.00      0.00 #> 23    1.00       0.00      0.00 #> 24    1.00       0.00      0.00 #> 25    1.00       0.00      0.00 #> 26    1.00       0.00      0.00 #> 27    1.00       0.00      0.00 #> 28    1.00       0.00      0.00 #> 29    1.00       0.00      0.00 #> 30    1.00       0.00      0.00 #> 31    1.00       0.00      0.00 #> 32    1.00       0.00      0.00 #> 33    1.00       0.00      0.00 #> 34    1.00       0.00      0.00 #> 51    0.00       0.90      0.10 #> 52    0.00       1.00      0.00 #> 53    0.00       0.92      0.08 #> 54    0.00       0.96      0.04 #> 55    0.00       1.00      0.00 #> 56    0.00       1.00      0.00 #> 57    0.00       0.96      0.04 #> 58    0.00       0.94      0.06 #> 59    0.00       1.00      0.00 #> 60    0.00       0.98      0.02 #> 61    0.00       0.94      0.06 #> 62    0.00       1.00      0.00 #> 63    0.00       0.98      0.02 #> 64    0.00       1.00      0.00 #> 65    0.00       1.00      0.00 #> 66    0.00       1.00      0.00 #> 67    0.00       1.00      0.00 #> 68    0.00       1.00      0.00 #> 69    0.00       0.94      0.06 #> 70    0.00       0.98      0.02 #> 71    0.00       0.72      0.28 #> 72    0.00       1.00      0.00 #> 73    0.00       0.90      0.10 #> 74    0.00       1.00      0.00 #> 75    0.00       1.00      0.00 #> 76    0.00       1.00      0.00 #> 77    0.00       1.00      0.00 #> 78    0.00       0.70      0.30 #> 79    0.00       1.00      0.00 #> 80    0.00       1.00      0.00 #> 81    0.00       1.00      0.00 #> 82    0.00       1.00      0.00 #> 83    0.00       1.00      0.00 #> 101   0.00       0.00      1.00 #> 102   0.00       0.02      0.98 #> 103   0.00       0.00      1.00 #> 104   0.00       0.00      1.00 #> 105   0.00       0.00      1.00 #> 106   0.00       0.00      1.00 #> 107   0.00       0.36      0.64 #> 108   0.00       0.02      0.98 #> 109   0.00       0.02      0.98 #> 110   0.00       0.00      1.00 #> 111   0.00       0.04      0.96 #> 112   0.00       0.00      1.00 #> 113   0.00       0.00      1.00 #> 114   0.00       0.00      1.00 #> 115   0.00       0.00      1.00 #> 116   0.00       0.02      0.98 #> 117   0.00       0.00      1.00 #> 118   0.00       0.00      1.00 #> 119   0.00       0.00      1.00 #> 120   0.00       0.32      0.68 #> 121   0.00       0.02      0.98 #> 122   0.00       0.02      0.98 #> 123   0.00       0.00      1.00 #> 124   0.00       0.02      0.98 #> 125   0.00       0.02      0.98 #> 126   0.00       0.04      0.96 #> 127   0.00       0.26      0.74 #> 128   0.00       0.08      0.92 #> 129   0.00       0.00      1.00 #> 130   0.00       0.24      0.76 #> 131   0.00       0.00      1.00 #> 132   0.00       0.00      1.00 #> 133   0.00       0.00      1.00 #>  predict(iris_rf, type = \"vote\") #>     setosa versicolor virginica #> 2       50          0         0 #> 3       50          0         0 #> 4       50          0         0 #> 5       50          0         0 #> 6       50          0         0 #> 7       50          0         0 #> 8       50          0         0 #> 9       50          0         0 #> 10      50          0         0 #> 11      50          0         0 #> 12      50          0         0 #> 13      50          0         0 #> 14      50          0         0 #> 15      48          2         0 #> 16      49          1         0 #> 17      50          0         0 #> 18      50          0         0 #> 19      49          1         0 #> 20      50          0         0 #> 21      50          0         0 #> 22      50          0         0 #> 23      50          0         0 #> 24      50          0         0 #> 25      50          0         0 #> 26      50          0         0 #> 27      50          0         0 #> 28      50          0         0 #> 29      50          0         0 #> 30      50          0         0 #> 31      50          0         0 #> 32      50          0         0 #> 33      50          0         0 #> 34      50          0         0 #> 51       0         45         5 #> 52       0         50         0 #> 53       0         46         4 #> 54       0         48         2 #> 55       0         50         0 #> 56       0         50         0 #> 57       0         48         2 #> 58       0         47         3 #> 59       0         50         0 #> 60       0         49         1 #> 61       0         47         3 #> 62       0         50         0 #> 63       0         49         1 #> 64       0         50         0 #> 65       0         50         0 #> 66       0         50         0 #> 67       0         50         0 #> 68       0         50         0 #> 69       0         47         3 #> 70       0         49         1 #> 71       0         36        14 #> 72       0         50         0 #> 73       0         45         5 #> 74       0         50         0 #> 75       0         50         0 #> 76       0         50         0 #> 77       0         50         0 #> 78       0         35        15 #> 79       0         50         0 #> 80       0         50         0 #> 81       0         50         0 #> 82       0         50         0 #> 83       0         50         0 #> 101      0          0        50 #> 102      0          1        49 #> 103      0          0        50 #> 104      0          0        50 #> 105      0          0        50 #> 106      0          0        50 #> 107      0         18        32 #> 108      0          1        49 #> 109      0          1        49 #> 110      0          0        50 #> 111      0          2        48 #> 112      0          0        50 #> 113      0          0        50 #> 114      0          0        50 #> 115      0          0        50 #> 116      0          1        49 #> 117      0          0        50 #> 118      0          0        50 #> 119      0          0        50 #> 120      0         16        34 #> 121      0          1        49 #> 122      0          1        49 #> 123      0          0        50 #> 124      0          1        49 #> 125      0          1        49 #> 126      0          2        48 #> 127      0         13        37 #> 128      0          4        46 #> 129      0          0        50 #> 130      0         12        38 #> 131      0          0        50 #> 132      0          0        50 #> 133      0          0        50 #> attr(,\"class\") #> [1] \"matrix\" \"array\"  \"votes\"  # Out-of-bag prediction (unbiased) predict(iris_rf, method = \"oob\") #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  versicolor virginica  virginica  versicolor #> [97] virginica  virginica  virginica  #> attr(,\"method\") #> attr(,\"method\")$name #> [1] out-of-bag #>  #> Levels: setosa versicolor virginica # Self-consistency (always very high for random forest, biased, do not use!) confusion(iris_rf) #> 99 items classified with 99 true positives (error rate = 0%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 virginica  33  0  0    33      0 #>   02 setosa      0 33  0    33      0 #>   03 versicolor  0  0 33    33      0 #>   (sum)         33 33 33    99      0 # This one is better confusion(iris_rf, method = \"oob\") # Out-of-bag performances #> 99 items classified with 93 true positives (error rate = 6.1%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 31  2    33      6 #>   03 virginica   0  4 29    33     12 #>   (sum)         33 35 31    99      6 # Cross-validation prediction is also a good choice when there is no test set predict(iris_rf, method = \"cv\")  # Idem: cvpredict(res) #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  versicolor virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> attr(,\"method\") #>  #> Call: #> cvpredict.mlearning(object = object, type = type) #>  #> \t 10-fold cross-validation estimator of misclassification error  #>  #> Misclassification error:  0.0505  #>  #> Levels: setosa versicolor virginica # Cross-validation for performances estimation confusion(iris_rf, method = \"cv\") #> 99 items classified with 92 true positives (error rate = 7.1%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 30  3    33      9 #>   03 virginica   0  4 29    33     12 #>   (sum)         33 34 32    99      7 # Evaluation of performances using a separate test set confusion(predict(iris_rf, newdata = iris_test), iris_test$Species) #> 50 items classified with 46 true positives (error rate = 8%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 versicolor 15  1  0  1    17     12 #>   02 virginica   2 15  0  0    17     12 #>   03 setosa      0  0 16  0    16      0 #>   04 NA          0  0  0  0     0        #>   (sum)         17 16 16  1    50      8  # Regression using random forest (from ?randomForest) set.seed(131) # Useful for reproducibility (use a different number each time) ozone_rf <- ml_rforest(data = airquality, Ozone ~ ., mtry = 3,   importance = TRUE, na.action = na.omit) summary(ozone_rf) #> A mlearning object of class mlRforest (random forest): #> [regression variant] #> Initial call: mlRforest.formula(formula = Ozone ~ ., data = airquality, mtry = 3,     importance = TRUE, na.action = na.omit) #>  #> Call: #>  randomForest(x = train, y = response, ntree = ntree, mtry = mtry,      replace = replace, classwt = classwt, importance = TRUE,      .args. = ..1)  #>                Type of random forest: regression #>                      Number of trees: 500 #> No. of variables tried at each split: 3 #>  #>           Mean of squared residuals: 302.2117 #>                     % Var explained: 72.46 # Show \"importance\" of variables: higher value mean more important variables round(randomForest::importance(ozone_rf), 2) #>         %IncMSE IncNodePurity #> Solar.R    9.76      10741.33 #> Wind      22.15      44234.96 #> Temp      43.85      53787.56 #> Month      2.59       1692.48 #> Day        0.93       6606.39 plot(na.omit(airquality)$Ozone, predict(ozone_rf)) abline(a = 0, b = 1)   # Unsupervised classification using random forest (from ?randomForest) set.seed(17) iris_urf <- ml_rforest(train = iris[, -5]) # Use only quantitative data summary(iris_urf) #> A mlearning object of class mlRforest (random forest): #> [unsupervised classification variant] #> Initial call: mlRforest.default(train = iris[, -5]) #>  #> Call: #>  randomForest(x = train, y = response, ntree = ntree, replace = replace,      classwt = classwt)  #>                Type of random forest: unsupervised #>                      Number of trees: 500 #> No. of variables tried at each split: 2 #>  randomForest::MDSplot(iris_urf, iris$Species)  plot(stats::hclust(stats::as.dist(1 - iris_urf$proximity),   method = \"average\"), labels = iris$Species)"},{"path":"https://www.sciviews.org/mlearning/reference/mlRpart.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification and regression using recursive partitioning — mlRpart","title":"Supervised classification and regression using recursive partitioning — mlRpart","text":"Unified (formula-based) interface version recursive partitioning algorithm implemented rpart::rpart().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlRpart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification and regression using recursive partitioning — mlRpart","text":"","code":"mlRpart(train, ...)  ml_rpart(train, ...)  # S3 method for formula mlRpart(formula, data, ..., subset, na.action)  # S3 method for default mlRpart(train, response, ..., .args. = NULL)  # S3 method for mlRpart predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\"),   method = c(\"direct\", \"cv\"),   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlRpart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification and regression using recursive partitioning — mlRpart","text":"train matrix data frame predictors. ... arguments passed rpart::rpart() predict() method (see corresponding help page. formula formula left term factor variable predict (supervised classification), vector numbers (regression) right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_rpart() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor (classification) numeric (regression). .args. used internally, provide anything . object mlRpart object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes, \"\" return classes memberships, method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different data set newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlRpart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification and regression using recursive partitioning — mlRpart","text":"ml_rpart()/mlRpart() creates mlRpart, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlRpart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification and regression using recursive partitioning — mlRpart","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_rpart <- ml_rpart(data = iris_train, Species ~ .) summary(iris_rpart) #> A mlearning object of class mlRpart (recursive partitioning tree): #> Initial call: mlRpart.formula(formula = Species ~ ., data = iris_train) #> n= 99  #>  #> node), split, n, loss, yval, (yprob) #>       * denotes terminal node #>  #> 1) root 99 66 setosa (0.33333333 0.33333333 0.33333333)   #>   2) Petal.Length< 2.6 33  0 setosa (1.00000000 0.00000000 0.00000000) * #>   3) Petal.Length>=2.6 66 33 versicolor (0.00000000 0.50000000 0.50000000)   #>     6) Petal.Width< 1.55 31  1 versicolor (0.00000000 0.96774194 0.03225806) * #>     7) Petal.Width>=1.55 35  3 virginica (0.00000000 0.08571429 0.91428571) * # Plot the decision tree for this classifier plot(iris_rpart, margin = 0.03, uniform = TRUE) text(iris_rpart, use.n = FALSE)  # Predictions predict(iris_rpart) # Default type is class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor virginica  versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_rpart, type = \"membership\") #>     setosa versicolor  virginica #> 2        1 0.00000000 0.00000000 #> 3        1 0.00000000 0.00000000 #> 4        1 0.00000000 0.00000000 #> 5        1 0.00000000 0.00000000 #> 6        1 0.00000000 0.00000000 #> 7        1 0.00000000 0.00000000 #> 8        1 0.00000000 0.00000000 #> 9        1 0.00000000 0.00000000 #> 10       1 0.00000000 0.00000000 #> 11       1 0.00000000 0.00000000 #> 12       1 0.00000000 0.00000000 #> 13       1 0.00000000 0.00000000 #> 14       1 0.00000000 0.00000000 #> 15       1 0.00000000 0.00000000 #> 16       1 0.00000000 0.00000000 #> 17       1 0.00000000 0.00000000 #> 18       1 0.00000000 0.00000000 #> 19       1 0.00000000 0.00000000 #> 20       1 0.00000000 0.00000000 #> 21       1 0.00000000 0.00000000 #> 22       1 0.00000000 0.00000000 #> 23       1 0.00000000 0.00000000 #> 24       1 0.00000000 0.00000000 #> 25       1 0.00000000 0.00000000 #> 26       1 0.00000000 0.00000000 #> 27       1 0.00000000 0.00000000 #> 28       1 0.00000000 0.00000000 #> 29       1 0.00000000 0.00000000 #> 30       1 0.00000000 0.00000000 #> 31       1 0.00000000 0.00000000 #> 32       1 0.00000000 0.00000000 #> 33       1 0.00000000 0.00000000 #> 34       1 0.00000000 0.00000000 #> 51       0 0.96774194 0.03225806 #> 52       0 0.96774194 0.03225806 #> 53       0 0.96774194 0.03225806 #> 54       0 0.96774194 0.03225806 #> 55       0 0.96774194 0.03225806 #> 56       0 0.96774194 0.03225806 #> 57       0 0.08571429 0.91428571 #> 58       0 0.96774194 0.03225806 #> 59       0 0.96774194 0.03225806 #> 60       0 0.96774194 0.03225806 #> 61       0 0.96774194 0.03225806 #> 62       0 0.96774194 0.03225806 #> 63       0 0.96774194 0.03225806 #> 64       0 0.96774194 0.03225806 #> 65       0 0.96774194 0.03225806 #> 66       0 0.96774194 0.03225806 #> 67       0 0.96774194 0.03225806 #> 68       0 0.96774194 0.03225806 #> 69       0 0.96774194 0.03225806 #> 70       0 0.96774194 0.03225806 #> 71       0 0.08571429 0.91428571 #> 72       0 0.96774194 0.03225806 #> 73       0 0.96774194 0.03225806 #> 74       0 0.96774194 0.03225806 #> 75       0 0.96774194 0.03225806 #> 76       0 0.96774194 0.03225806 #> 77       0 0.96774194 0.03225806 #> 78       0 0.08571429 0.91428571 #> 79       0 0.96774194 0.03225806 #> 80       0 0.96774194 0.03225806 #> 81       0 0.96774194 0.03225806 #> 82       0 0.96774194 0.03225806 #> 83       0 0.96774194 0.03225806 #> 101      0 0.08571429 0.91428571 #> 102      0 0.08571429 0.91428571 #> 103      0 0.08571429 0.91428571 #> 104      0 0.08571429 0.91428571 #> 105      0 0.08571429 0.91428571 #> 106      0 0.08571429 0.91428571 #> 107      0 0.08571429 0.91428571 #> 108      0 0.08571429 0.91428571 #> 109      0 0.08571429 0.91428571 #> 110      0 0.08571429 0.91428571 #> 111      0 0.08571429 0.91428571 #> 112      0 0.08571429 0.91428571 #> 113      0 0.08571429 0.91428571 #> 114      0 0.08571429 0.91428571 #> 115      0 0.08571429 0.91428571 #> 116      0 0.08571429 0.91428571 #> 117      0 0.08571429 0.91428571 #> 118      0 0.08571429 0.91428571 #> 119      0 0.08571429 0.91428571 #> 120      0 0.96774194 0.03225806 #> 121      0 0.08571429 0.91428571 #> 122      0 0.08571429 0.91428571 #> 123      0 0.08571429 0.91428571 #> 124      0 0.08571429 0.91428571 #> 125      0 0.08571429 0.91428571 #> 126      0 0.08571429 0.91428571 #> 127      0 0.08571429 0.91428571 #> 128      0 0.08571429 0.91428571 #> 129      0 0.08571429 0.91428571 #> 130      0 0.08571429 0.91428571 #> 131      0 0.08571429 0.91428571 #> 132      0 0.08571429 0.91428571 #> 133      0 0.08571429 0.91428571 predict(iris_rpart, type = \"both\") #> $class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor virginica  versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>     setosa versicolor  virginica #> 2        1 0.00000000 0.00000000 #> 3        1 0.00000000 0.00000000 #> 4        1 0.00000000 0.00000000 #> 5        1 0.00000000 0.00000000 #> 6        1 0.00000000 0.00000000 #> 7        1 0.00000000 0.00000000 #> 8        1 0.00000000 0.00000000 #> 9        1 0.00000000 0.00000000 #> 10       1 0.00000000 0.00000000 #> 11       1 0.00000000 0.00000000 #> 12       1 0.00000000 0.00000000 #> 13       1 0.00000000 0.00000000 #> 14       1 0.00000000 0.00000000 #> 15       1 0.00000000 0.00000000 #> 16       1 0.00000000 0.00000000 #> 17       1 0.00000000 0.00000000 #> 18       1 0.00000000 0.00000000 #> 19       1 0.00000000 0.00000000 #> 20       1 0.00000000 0.00000000 #> 21       1 0.00000000 0.00000000 #> 22       1 0.00000000 0.00000000 #> 23       1 0.00000000 0.00000000 #> 24       1 0.00000000 0.00000000 #> 25       1 0.00000000 0.00000000 #> 26       1 0.00000000 0.00000000 #> 27       1 0.00000000 0.00000000 #> 28       1 0.00000000 0.00000000 #> 29       1 0.00000000 0.00000000 #> 30       1 0.00000000 0.00000000 #> 31       1 0.00000000 0.00000000 #> 32       1 0.00000000 0.00000000 #> 33       1 0.00000000 0.00000000 #> 34       1 0.00000000 0.00000000 #> 51       0 0.96774194 0.03225806 #> 52       0 0.96774194 0.03225806 #> 53       0 0.96774194 0.03225806 #> 54       0 0.96774194 0.03225806 #> 55       0 0.96774194 0.03225806 #> 56       0 0.96774194 0.03225806 #> 57       0 0.08571429 0.91428571 #> 58       0 0.96774194 0.03225806 #> 59       0 0.96774194 0.03225806 #> 60       0 0.96774194 0.03225806 #> 61       0 0.96774194 0.03225806 #> 62       0 0.96774194 0.03225806 #> 63       0 0.96774194 0.03225806 #> 64       0 0.96774194 0.03225806 #> 65       0 0.96774194 0.03225806 #> 66       0 0.96774194 0.03225806 #> 67       0 0.96774194 0.03225806 #> 68       0 0.96774194 0.03225806 #> 69       0 0.96774194 0.03225806 #> 70       0 0.96774194 0.03225806 #> 71       0 0.08571429 0.91428571 #> 72       0 0.96774194 0.03225806 #> 73       0 0.96774194 0.03225806 #> 74       0 0.96774194 0.03225806 #> 75       0 0.96774194 0.03225806 #> 76       0 0.96774194 0.03225806 #> 77       0 0.96774194 0.03225806 #> 78       0 0.08571429 0.91428571 #> 79       0 0.96774194 0.03225806 #> 80       0 0.96774194 0.03225806 #> 81       0 0.96774194 0.03225806 #> 82       0 0.96774194 0.03225806 #> 83       0 0.96774194 0.03225806 #> 101      0 0.08571429 0.91428571 #> 102      0 0.08571429 0.91428571 #> 103      0 0.08571429 0.91428571 #> 104      0 0.08571429 0.91428571 #> 105      0 0.08571429 0.91428571 #> 106      0 0.08571429 0.91428571 #> 107      0 0.08571429 0.91428571 #> 108      0 0.08571429 0.91428571 #> 109      0 0.08571429 0.91428571 #> 110      0 0.08571429 0.91428571 #> 111      0 0.08571429 0.91428571 #> 112      0 0.08571429 0.91428571 #> 113      0 0.08571429 0.91428571 #> 114      0 0.08571429 0.91428571 #> 115      0 0.08571429 0.91428571 #> 116      0 0.08571429 0.91428571 #> 117      0 0.08571429 0.91428571 #> 118      0 0.08571429 0.91428571 #> 119      0 0.08571429 0.91428571 #> 120      0 0.96774194 0.03225806 #> 121      0 0.08571429 0.91428571 #> 122      0 0.08571429 0.91428571 #> 123      0 0.08571429 0.91428571 #> 124      0 0.08571429 0.91428571 #> 125      0 0.08571429 0.91428571 #> 126      0 0.08571429 0.91428571 #> 127      0 0.08571429 0.91428571 #> 128      0 0.08571429 0.91428571 #> 129      0 0.08571429 0.91428571 #> 130      0 0.08571429 0.91428571 #> 131      0 0.08571429 0.91428571 #> 132      0 0.08571429 0.91428571 #> 133      0 0.08571429 0.91428571 #>  # Self-consistency, do not use for assessing classifier performances! confusion(iris_rpart) #> 99 items classified with 95 true positives (error rate = 4%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 30  3    33      9 #>   03 virginica   0  1 32    33      3 #>   (sum)         33 31 35    99      4 # Cross-validation prediction is a good choice when there is no test set predict(iris_rpart, method = \"cv\")  # Idem: cvpredict(res) #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor virginica  versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor virginica  #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] versicolor virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  versicolor #> [97] virginica  virginica  virginica  #> attr(,\"method\") #>  #> Call: #> cvpredict.mlearning(object = object, type = type) #>  #> \t 10-fold cross-validation estimator of misclassification error  #>  #> Misclassification error:  0.0606  #>  #> Levels: setosa versicolor virginica confusion(iris_rpart, method = \"cv\") #> 99 items classified with 90 true positives (error rate = 9.1%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 28  5    33     15 #>   03 virginica   0  4 29    33     12 #>   (sum)         33 32 34    99      9 # Evaluation of performances using a separate test set confusion(predict(iris_rpart, newdata = iris_test), iris_test$Species) #> 50 items classified with 45 true positives (error rate = 10%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 versicolor 14  2  0  1    17     18 #>   02 virginica   2 15  0  0    17     12 #>   03 setosa      0  0 16  0    16      0 #>   04 NA          0  0  0  0     0        #>   (sum)         16 17 16  1    50     10"},{"path":"https://www.sciviews.org/mlearning/reference/mlSvm.html","id":null,"dir":"Reference","previous_headings":"","what":"Supervised classification and regression using support vector machine — mlSvm","title":"Supervised classification and regression using support vector machine — mlSvm","text":"Unified (formula-based) interface version support vector machine algorithm provided e1071::svm().","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlSvm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Supervised classification and regression using support vector machine — mlSvm","text":"","code":"mlSvm(train, ...)  ml_svm(train, ...)  # S3 method for formula mlSvm(   formula,   data,   scale = TRUE,   type = NULL,   kernel = \"radial\",   classwt = NULL,   ...,   subset,   na.action )  # S3 method for default mlSvm(   train,   response,   scale = TRUE,   type = NULL,   kernel = \"radial\",   classwt = NULL,   ... )  # S3 method for mlSvm predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\"),   method = c(\"direct\", \"cv\"),   na.action = na.exclude,   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlSvm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Supervised classification and regression using support vector machine — mlSvm","text":"train matrix data frame predictors. ... arguments passed classification regression method. See e1071::svm(). formula formula left term factor variable predict (supervised classification), vector numbers (regression) nothing (unsupervised classification) right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). data data.frame use training set. scale variables scaled (mean = 0 standard deviation = 1)? TRUE default. vector provided, applied variables recycling. type ml_svm()/mlSvm(), type classification regression machine use. default value NULL uses \"C-classification\" response variable factor  eps-regression numeric. can also \"nu-classification\" \"nu-regression\". \"C\" \"nu\" versions basically different parameterisation. range C zero infinity, range nu zero one. fifth option \"one_classification\" specific novelty detection (find items different rest). predict(), type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes,  \"\" return classes memberships. kernel kernel used svm, see e1071::svm() explanations. Can \"radial\", \"linear\", \"polynomial\" \"sigmoid\". classwt priors classes. Need add one. subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_svm() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). response vector factor (classification) numeric (regression). object mlSvm object newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different data set newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlSvm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Supervised classification and regression using support vector machine — mlSvm","text":"ml_svm()/mlSvm() creates mlSvm, mlearning object containing classifier lot additional metadata used functions methods can apply like predict() cvpredict(). case want program new functions extract specific components, inspect \"unclassed\" object using unclass().","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlSvm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Supervised classification and regression using support vector machine — mlSvm","text":"","code":"# Prepare data: split into training set (2/3) and test set (1/3) data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) iris_train <- iris[train, ] iris_test <- iris[-train, ] # One case with missing data in train set, and another case in test set iris_train[1, 1] <- NA iris_test[25, 2] <- NA  iris_svm <- ml_svm(data = iris_train, Species ~ .) summary(iris_svm) #> A mlearning object of class mlSvm (support vector machine): #> Initial call: mlSvm.formula(formula = Species ~ ., data = iris_train) #>  #> Call: #> svm.default(x = sapply(train, as.numeric), y = response, scale = scale,  #>     type = type, kernel = kernel, class.weights = classwt, probability = TRUE,  #>     .args. = ..1) #>  #>  #> Parameters: #>    SVM-Type:  C-classification  #>  SVM-Kernel:  radial  #>        cost:  1  #>  #> Number of Support Vectors:  42 #>  #>  ( 8 17 17 ) #>  #>  #> Number of Classes:  3  #>  #> Levels:  #>  setosa versicolor virginica #>  #>  #>  predict(iris_svm) # Default type is class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_svm, type = \"membership\") #>         setosa  versicolor   virginica #> 1  0.952774913 0.030871412 0.016353675 #> 2  0.968246698 0.017814836 0.013938466 #> 3  0.962422695 0.022536988 0.015040318 #> 4  0.964604420 0.019090493 0.016305087 #> 5  0.952920314 0.028915268 0.018164418 #> 6  0.964235804 0.019066326 0.016697870 #> 7  0.967382674 0.018700065 0.013917261 #> 8  0.940953600 0.037583456 0.021462944 #> 9  0.962462310 0.022665107 0.014872582 #> 10 0.957392104 0.025273315 0.017334581 #> 11 0.967071625 0.018274876 0.014653499 #> 12 0.957854393 0.025941969 0.016203638 #> 13 0.951230899 0.027373121 0.021395980 #> 14 0.938694244 0.036062239 0.025243518 #> 15 0.930932398 0.037660337 0.031407266 #> 16 0.953219533 0.028252123 0.018528344 #> 17 0.965037235 0.020515079 0.014447686 #> 18 0.942181745 0.037607045 0.020211210 #> 19 0.956476189 0.025217590 0.018306221 #> 20 0.951143415 0.031490947 0.017365638 #> 21 0.959398293 0.023903159 0.016698548 #> 22 0.951029511 0.024514896 0.024455593 #> 23 0.944831578 0.038361661 0.016806761 #> 24 0.962832146 0.021532656 0.015635198 #> 25 0.943963470 0.038356821 0.017679709 #> 26 0.961153875 0.024412795 0.014433330 #> 27 0.963690404 0.021316634 0.014992962 #> 28 0.964058935 0.021234779 0.014706286 #> 29 0.965595320 0.020291904 0.014112777 #> 30 0.960520933 0.024951684 0.014527383 #> 31 0.946807644 0.035864733 0.017327624 #> 32 0.940206660 0.032229594 0.027563746 #> 33 0.943120577 0.031535798 0.025343625 #> 34 0.020963596 0.915961853 0.063074551 #> 35 0.016993047 0.952505456 0.030501498 #> 36 0.017671232 0.819575341 0.162753427 #> 37 0.009421227 0.950487262 0.040091511 #> 38 0.010969701 0.896696242 0.092334057 #> 39 0.010661393 0.968091338 0.021247269 #> 40 0.021361907 0.899152303 0.079485790 #> 41 0.029940743 0.947635850 0.022423407 #> 42 0.012796178 0.969508994 0.017694828 #> 43 0.014664266 0.944933707 0.040402026 #> 44 0.024259722 0.934219500 0.041520778 #> 45 0.013588403 0.964252771 0.022158826 #> 46 0.018468182 0.969363152 0.012168667 #> 47 0.010214462 0.949759268 0.040026270 #> 48 0.023092401 0.969530046 0.007377553 #> 49 0.015699093 0.965414193 0.018886714 #> 50 0.016055557 0.929710698 0.054233745 #> 51 0.015149669 0.980865568 0.003984763 #> 52 0.019918527 0.787181492 0.192899980 #> 53 0.011021435 0.981596035 0.007382529 #> 54 0.022867705 0.619562645 0.357569650 #> 55 0.010899323 0.983325760 0.005774916 #> 56 0.013576732 0.678881884 0.307541384 #> 57 0.010787973 0.976565011 0.012647017 #> 58 0.011807199 0.980322599 0.007870202 #> 59 0.013159217 0.968493106 0.018347677 #> 60 0.014841249 0.898495677 0.086663074 #> 61 0.015575825 0.423357433 0.561066741 #> 62 0.010068025 0.934621023 0.055310951 #> 63 0.019267710 0.975995279 0.004737011 #> 64 0.011135553 0.978660908 0.010203539 #> 65 0.013676848 0.978899513 0.007423639 #> 66 0.011502970 0.983392125 0.005104905 #> 67 0.019235585 0.006796298 0.973968117 #> 68 0.010494832 0.046342605 0.943162563 #> 69 0.010480653 0.005983146 0.983536201 #> 70 0.011887374 0.044459946 0.943652680 #> 71 0.010915737 0.003342065 0.985742198 #> 72 0.013081741 0.007100951 0.979817308 #> 73 0.015892993 0.392434228 0.591672779 #> 74 0.013193374 0.015848064 0.970958562 #> 75 0.013684208 0.030398101 0.955917690 #> 76 0.019045733 0.015013759 0.965940508 #> 77 0.016725909 0.096380607 0.886893484 #> 78 0.010731795 0.029645356 0.959622849 #> 79 0.011009667 0.009681532 0.979308801 #> 80 0.010060303 0.027605123 0.962334574 #> 81 0.014098795 0.004140644 0.981760561 #> 82 0.015411488 0.012837844 0.971750668 #> 83 0.013296118 0.073259072 0.913444810 #> 84 0.024332649 0.023191460 0.952475891 #> 85 0.023234439 0.016001691 0.960763869 #> 86 0.018080835 0.572952116 0.408967049 #> 87 0.011863861 0.005932962 0.982203177 #> 88 0.012470468 0.050151824 0.937377708 #> 89 0.017177365 0.013410635 0.969412000 #> 90 0.012209514 0.170174585 0.817615901 #> 91 0.013586337 0.019251949 0.967161714 #> 92 0.012729685 0.030355806 0.956914509 #> 93 0.012665785 0.251274790 0.736059425 #> 94 0.015233431 0.338331853 0.646434716 #> 95 0.010240692 0.004450885 0.985308423 #> 96 0.017185457 0.118323789 0.864490754 #> 97 0.013799572 0.016625171 0.969575258 #> 98 0.025818577 0.031130094 0.943051329 #> 99 0.010528869 0.002922124 0.986549007 predict(iris_svm, type = \"both\") #> $class #>  [1] setosa     setosa     setosa     setosa     setosa     setosa     #>  [7] setosa     setosa     setosa     setosa     setosa     setosa     #> [13] setosa     setosa     setosa     setosa     setosa     setosa     #> [19] setosa     setosa     setosa     setosa     setosa     setosa     #> [25] setosa     setosa     setosa     setosa     setosa     setosa     #> [31] setosa     setosa     setosa     versicolor versicolor versicolor #> [37] versicolor versicolor versicolor versicolor versicolor versicolor #> [43] versicolor versicolor versicolor versicolor versicolor versicolor #> [49] versicolor versicolor versicolor versicolor versicolor versicolor #> [55] versicolor versicolor versicolor versicolor versicolor versicolor #> [61] virginica  versicolor versicolor versicolor versicolor versicolor #> [67] virginica  virginica  virginica  virginica  virginica  virginica  #> [73] virginica  virginica  virginica  virginica  virginica  virginica  #> [79] virginica  virginica  virginica  virginica  virginica  virginica  #> [85] virginica  versicolor virginica  virginica  virginica  virginica  #> [91] virginica  virginica  virginica  virginica  virginica  virginica  #> [97] virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>         setosa  versicolor   virginica #> 1  0.952774913 0.030871412 0.016353675 #> 2  0.968246698 0.017814836 0.013938466 #> 3  0.962422695 0.022536988 0.015040318 #> 4  0.964604420 0.019090493 0.016305087 #> 5  0.952920314 0.028915268 0.018164418 #> 6  0.964235804 0.019066326 0.016697870 #> 7  0.967382674 0.018700065 0.013917261 #> 8  0.940953600 0.037583456 0.021462944 #> 9  0.962462310 0.022665107 0.014872582 #> 10 0.957392104 0.025273315 0.017334581 #> 11 0.967071625 0.018274876 0.014653499 #> 12 0.957854393 0.025941969 0.016203638 #> 13 0.951230899 0.027373121 0.021395980 #> 14 0.938694244 0.036062239 0.025243518 #> 15 0.930932398 0.037660337 0.031407266 #> 16 0.953219533 0.028252123 0.018528344 #> 17 0.965037235 0.020515079 0.014447686 #> 18 0.942181745 0.037607045 0.020211210 #> 19 0.956476189 0.025217590 0.018306221 #> 20 0.951143415 0.031490947 0.017365638 #> 21 0.959398293 0.023903159 0.016698548 #> 22 0.951029511 0.024514896 0.024455593 #> 23 0.944831578 0.038361661 0.016806761 #> 24 0.962832146 0.021532656 0.015635198 #> 25 0.943963470 0.038356821 0.017679709 #> 26 0.961153875 0.024412795 0.014433330 #> 27 0.963690404 0.021316634 0.014992962 #> 28 0.964058935 0.021234779 0.014706286 #> 29 0.965595320 0.020291904 0.014112777 #> 30 0.960520933 0.024951684 0.014527383 #> 31 0.946807644 0.035864733 0.017327624 #> 32 0.940206660 0.032229594 0.027563746 #> 33 0.943120577 0.031535798 0.025343625 #> 34 0.020963596 0.915961853 0.063074551 #> 35 0.016993047 0.952505456 0.030501498 #> 36 0.017671232 0.819575341 0.162753427 #> 37 0.009421227 0.950487262 0.040091511 #> 38 0.010969701 0.896696242 0.092334057 #> 39 0.010661393 0.968091338 0.021247269 #> 40 0.021361907 0.899152303 0.079485790 #> 41 0.029940743 0.947635850 0.022423407 #> 42 0.012796178 0.969508994 0.017694828 #> 43 0.014664266 0.944933707 0.040402026 #> 44 0.024259722 0.934219500 0.041520778 #> 45 0.013588403 0.964252771 0.022158826 #> 46 0.018468182 0.969363152 0.012168667 #> 47 0.010214462 0.949759268 0.040026270 #> 48 0.023092401 0.969530046 0.007377553 #> 49 0.015699093 0.965414193 0.018886714 #> 50 0.016055557 0.929710698 0.054233745 #> 51 0.015149669 0.980865568 0.003984763 #> 52 0.019918527 0.787181492 0.192899980 #> 53 0.011021435 0.981596035 0.007382529 #> 54 0.022867705 0.619562645 0.357569650 #> 55 0.010899323 0.983325760 0.005774916 #> 56 0.013576732 0.678881884 0.307541384 #> 57 0.010787973 0.976565011 0.012647017 #> 58 0.011807199 0.980322599 0.007870202 #> 59 0.013159217 0.968493106 0.018347677 #> 60 0.014841249 0.898495677 0.086663074 #> 61 0.015575825 0.423357433 0.561066741 #> 62 0.010068025 0.934621023 0.055310951 #> 63 0.019267710 0.975995279 0.004737011 #> 64 0.011135553 0.978660908 0.010203539 #> 65 0.013676848 0.978899513 0.007423639 #> 66 0.011502970 0.983392125 0.005104905 #> 67 0.019235585 0.006796298 0.973968117 #> 68 0.010494832 0.046342605 0.943162563 #> 69 0.010480653 0.005983146 0.983536201 #> 70 0.011887374 0.044459946 0.943652680 #> 71 0.010915737 0.003342065 0.985742198 #> 72 0.013081741 0.007100951 0.979817308 #> 73 0.015892993 0.392434228 0.591672779 #> 74 0.013193374 0.015848064 0.970958562 #> 75 0.013684208 0.030398101 0.955917690 #> 76 0.019045733 0.015013759 0.965940508 #> 77 0.016725909 0.096380607 0.886893484 #> 78 0.010731795 0.029645356 0.959622849 #> 79 0.011009667 0.009681532 0.979308801 #> 80 0.010060303 0.027605123 0.962334574 #> 81 0.014098795 0.004140644 0.981760561 #> 82 0.015411488 0.012837844 0.971750668 #> 83 0.013296118 0.073259072 0.913444810 #> 84 0.024332649 0.023191460 0.952475891 #> 85 0.023234439 0.016001691 0.960763869 #> 86 0.018080835 0.572952116 0.408967049 #> 87 0.011863861 0.005932962 0.982203177 #> 88 0.012470468 0.050151824 0.937377708 #> 89 0.017177365 0.013410635 0.969412000 #> 90 0.012209514 0.170174585 0.817615901 #> 91 0.013586337 0.019251949 0.967161714 #> 92 0.012729685 0.030355806 0.956914509 #> 93 0.012665785 0.251274790 0.736059425 #> 94 0.015233431 0.338331853 0.646434716 #> 95 0.010240692 0.004450885 0.985308423 #> 96 0.017185457 0.118323789 0.864490754 #> 97 0.013799572 0.016625171 0.969575258 #> 98 0.025818577 0.031130094 0.943051329 #> 99 0.010528869 0.002922124 0.986549007 #>  # Self-consistency, do not use for assessing classifier performances! confusion(iris_svm) #> 99 items classified with 97 true positives (error rate = 2%) #>                Predicted #> Actual          01 02 03 (sum) (FNR%) #>   01 setosa     33  0  0    33      0 #>   02 versicolor  0 32  1    33      3 #>   03 virginica   0  1 32    33      3 #>   (sum)         33 33 33    99      2 # Use an independent test set instead confusion(predict(iris_svm, newdata = iris_test), iris_test$Species) #> 50 items classified with 47 true positives (error rate = 6%) #>                Predicted #> Actual          01 02 03 04 (sum) (FNR%) #>   01 setosa     16  0  0  0    16      0 #>   02 NA          0  0  0  0     0        #>   03 versicolor  0  1 15  1    17     12 #>   04 virginica   0  0  1 16    17      6 #>   (sum)         16  1 16 17    50      6  # Another dataset data(\"HouseVotes84\", package = \"mlbench\") house_svm <- ml_svm(data = HouseVotes84, Class ~ ., na.action = na.omit) summary(house_svm) #> A mlearning object of class mlSvm (support vector machine): #> Initial call: mlSvm.formula(formula = Class ~ ., data = HouseVotes84, na.action = na.omit) #>  #> Call: #> svm.default(x = sapply(train, as.numeric), y = response, scale = scale,  #>     type = type, kernel = kernel, class.weights = classwt, probability = TRUE,  #>     .args. = ..1) #>  #>  #> Parameters: #>    SVM-Type:  C-classification  #>  SVM-Kernel:  radial  #>        cost:  1  #>  #> Number of Support Vectors:  78 #>  #>  ( 43 35 ) #>  #>  #> Number of Classes:  2  #>  #> Levels:  #>  democrat republican #>  #>  #>  # Cross-validated confusion matrix confusion(cvpredict(house_svm), na.omit(HouseVotes84)$Class) #> 232 items classified with 224 true positives (error rate = 3.4%) #>                Predicted #> Actual           01  02 (sum) (FNR%) #>   01 democrat   118   6   124      5 #>   02 republican   2 106   108      2 #>   (sum)         120 112   232      3  # Regression using support vector machine data(airquality, package = \"datasets\") ozone_svm <- ml_svm(data = airquality, Ozone ~ ., na.action = na.omit) summary(ozone_svm) #> A mlearning object of class mlSvm (support vector machine): #> [regression variant] #> Initial call: mlSvm.formula(formula = Ozone ~ ., data = airquality, na.action = na.omit) #>  #> Call: #> svm.default(x = sapply(train, as.numeric), y = response, scale = scale,  #>     type = type, kernel = kernel, class.weights = classwt, probability = TRUE,  #>     .args. = ..1) #>  #>  #> Parameters: #>    SVM-Type:  eps-regression  #>  SVM-Kernel:  radial  #>        cost:  1  #>       gamma:  0.2  #>     epsilon:  0.1  #>  #> Sigma:  0.3644775  #>  #>  #> Number of Support Vectors:  90 #>  #>  #>  #>  #>  plot(na.omit(airquality)$Ozone, predict(ozone_svm)) abline(a = 0, b = 1)"},{"path":"https://www.sciviews.org/mlearning/reference/mlearning-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Machine Learning Algorithms with Unified Interface and Confusion Matrices — mlearning-package","title":"Machine Learning Algorithms with Unified Interface and Confusion Matrices — mlearning-package","text":"package provides wrappers around several existing machine learning algorithms R, unified user interface. Confusion matrices can also calculated viewed tables plots. Key features : Unified, formula-based interface algorithms, similar stats::lm(). Optimized code simplified formula y ~ . used, meaning variables data used (one (y ) class predicted (classification problem, factor variable), dependent variable model (regression problem, numeric variable). Similar way dealing missing data, training set predictions. Underlying algorithms deal differently missing data. accept , . Unified way dealing factor levels cases training set. training succeeds, classifier , course, unable classify items missing class. predict() methods similar arguments. return class, membership classes, , something else (probabilities, raw predictions, ...) depending algorithm problem (classification regression). cvpredict() method available algorithms performs easily cross-validation, even leave_one_out validation (cv.k = number cases). operates transparently end-user. confusion() method creates confusion matrix object can printed, summarized, plotted. Various metrics easily derived confusion matrix. Also, allows adjust prior probabilities classes classification problem, order obtain representative estimates metrics priors adjusted values closes real proportions classes data. See mlearning() explanations example analysis. See mlLda() examples different forms formula can used. See plot.confusion() different ways explore confusion matrix.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlearning-package.html","id":"important-functions","dir":"Reference","previous_headings":"","what":"Important functions","title":"Machine Learning Algorithms with Unified Interface and Confusion Matrices — mlearning-package","text":"ml_lda(), ml_qda(), ml_naive_bayes(), ml_knn(), ml_lvq(), ml_nnet(), ml_rpart(), ml_rforest() ml_svm() train classifiers regressors different algorithms supported package, predict() cvpredict() predictions, including using cross-validation, confusion() calculate confusion matrix (various methods analyze calculate derived metrics like recall, precision, F-score, ...) prior() adjust prior probabilities, response() train() extract response training variables mlearning object.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlearning.html","id":null,"dir":"Reference","previous_headings":"","what":"Machine learning model for (un)supervised classification or regression — mlearning","title":"Machine learning model for (un)supervised classification or regression — mlearning","text":"mlearning object provides unified (formula-based) interface several machine learning algorithms. share interface similar arguments. conform formula-based approach, say, stats::lm() base R, coherent handling missing data missing class levels. optimized version exists simplified y ~ . formula. Finally, cross-validation also built-.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlearning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Machine learning model for (un)supervised classification or regression — mlearning","text":"","code":"mlearning(   formula,   data,   method,   model.args,   call = match.call(),   ...,   subset,   na.action = na.fail )  # S3 method for mlearning print(x, ...)  # S3 method for mlearning summary(object, ...)  # S3 method for summary.mlearning print(x, ...)  # S3 method for mlearning plot(x, y, ...)  # S3 method for mlearning predict(   object,   newdata,   type = c(\"class\", \"membership\", \"both\"),   method = c(\"direct\", \"cv\"),   na.action = na.exclude,   ... )  cvpredict(object, ...)  # S3 method for mlearning cvpredict(   object,   type = c(\"class\", \"membership\", \"both\"),   cv.k = 10,   cv.strat = TRUE,   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/mlearning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Machine learning model for (un)supervised classification or regression — mlearning","text":"formula formula left term factor variable predict (supervised classification), vector numbers (regression) nothing (unsupervised classification) right term list independent, predictive variables, separated plus sign. data frame provided contains dependent independent variables, one can use class ~ . short version (one strongly encouraged). Variables minus sign eliminated. Calculations variables possible according usual formula convention (possibly protected using ()). Supervised classification, regression unsupervised classification available algorithms. Check respective help pages. data data.frame use training set. method \"direct\" (default) \"cv\". \"direct\" predicts new cases newdata= argument provided, cases training set . Take care providing newdata= means just calculate self-consistency classifier use metrics derived results assessment performances. Either use different dataset newdata= use alternate cross-validation (\"cv\") technique. specify method = \"cv\" cvpredict() used provide newdata= case. methods may provided various algorithms (check help pages) model.args arguments formula modeling substituted data subset... used end-user. call function call. used end-user. ... arguments (depends method). subset index vector cases define training set use (argument must named, provided). na.action function specify action taken NAs found. ml_qda() na.fail used default. calculation stopped NA data. Another option na.omit, cases missing values required variable dropped (argument must named, provided). predict() method, default, suitable option, na.exclude. case, rows NAs newdata= excluded prediction, reinjected final results number items still (order newdata=). x, object mlearning object y second mlearning object nothing (used several plots) newdata new dataset conformation training set (variables, except may class classification dependent variable regression). Usually test set, new dataset predicted. type type prediction return. \"class\" default, predicted classes. options \"membership\" membership (number 0 1) different classes, \"\" return classes memberships. types may provided algorithms (read respective help pages). cv.k k k-fold cross-validation, cf ipred::errorest(). default, 10. cv.strat subsampling stratified cross-validation, cf ipred::errorest(). TRUE default.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/mlearning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Machine learning model for (un)supervised classification or regression — mlearning","text":"mlearning object mlearning(). Methods return results can mlearning, data.frame, vector, etc.","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/mlearning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Machine learning model for (un)supervised classification or regression — mlearning","text":"","code":"# mlearning() should not be calle directly. Use the mlXXX() functions instead # for instance, for Random Forest, use ml_rforest()/mlRforest() # A typical classification involves several steps: # # 1) Prepare data: split into training set (2/3) and test set (1/3) #    Data cleaning (elimination of unwanted variables), transformation of #    others (scaling, log, ratios, numeric to factor, ...) may be necessary #    here. Apply the same treatments on the training and test sets data(\"iris\", package = \"datasets\") train <- c(1:34, 51:83, 101:133) # Also random or stratified sampling iris_train <- iris[train, ] iris_test <- iris[-train, ]  # 2) Train the classifier, use of the simplified formula class ~ . encouraged #    so, you may have to prepare the train/test sets to keep only relevant #    variables and to possibly transform them before use iris_rf <- ml_rforest(data = iris_train, Species ~ .) iris_rf #> A mlearning object of class mlRforest (random forest): #> Call: mlRforest.formula(formula = Species ~ ., data = iris_train) #> Trained using 100 cases: #>     setosa versicolor  virginica  #>         34         33         33  summary(iris_rf) #> A mlearning object of class mlRforest (random forest): #> Initial call: mlRforest.formula(formula = Species ~ ., data = iris_train) #>  #> Call: #>  randomForest(x = train, y = response, ntree = ntree, replace = replace,      classwt = classwt, .args. = ..1)  #>                Type of random forest: classification #>                      Number of trees: 500 #> No. of variables tried at each split: 2 #>  #>         OOB estimate of  error rate: 6% #> Confusion matrix: #>            setosa versicolor virginica class.error #> setosa         34          0         0  0.00000000 #> versicolor      0         31         2  0.06060606 #> virginica       0          4        29  0.12121212 train(iris_rf) #>     Sepal.Length Sepal.Width Petal.Length Petal.Width #> 1            5.1         3.5          1.4         0.2 #> 2            4.9         3.0          1.4         0.2 #> 3            4.7         3.2          1.3         0.2 #> 4            4.6         3.1          1.5         0.2 #> 5            5.0         3.6          1.4         0.2 #> 6            5.4         3.9          1.7         0.4 #> 7            4.6         3.4          1.4         0.3 #> 8            5.0         3.4          1.5         0.2 #> 9            4.4         2.9          1.4         0.2 #> 10           4.9         3.1          1.5         0.1 #> 11           5.4         3.7          1.5         0.2 #> 12           4.8         3.4          1.6         0.2 #> 13           4.8         3.0          1.4         0.1 #> 14           4.3         3.0          1.1         0.1 #> 15           5.8         4.0          1.2         0.2 #> 16           5.7         4.4          1.5         0.4 #> 17           5.4         3.9          1.3         0.4 #> 18           5.1         3.5          1.4         0.3 #> 19           5.7         3.8          1.7         0.3 #> 20           5.1         3.8          1.5         0.3 #> 21           5.4         3.4          1.7         0.2 #> 22           5.1         3.7          1.5         0.4 #> 23           4.6         3.6          1.0         0.2 #> 24           5.1         3.3          1.7         0.5 #> 25           4.8         3.4          1.9         0.2 #> 26           5.0         3.0          1.6         0.2 #> 27           5.0         3.4          1.6         0.4 #> 28           5.2         3.5          1.5         0.2 #> 29           5.2         3.4          1.4         0.2 #> 30           4.7         3.2          1.6         0.2 #> 31           4.8         3.1          1.6         0.2 #> 32           5.4         3.4          1.5         0.4 #> 33           5.2         4.1          1.5         0.1 #> 34           5.5         4.2          1.4         0.2 #> 51           7.0         3.2          4.7         1.4 #> 52           6.4         3.2          4.5         1.5 #> 53           6.9         3.1          4.9         1.5 #> 54           5.5         2.3          4.0         1.3 #> 55           6.5         2.8          4.6         1.5 #> 56           5.7         2.8          4.5         1.3 #> 57           6.3         3.3          4.7         1.6 #> 58           4.9         2.4          3.3         1.0 #> 59           6.6         2.9          4.6         1.3 #> 60           5.2         2.7          3.9         1.4 #> 61           5.0         2.0          3.5         1.0 #> 62           5.9         3.0          4.2         1.5 #> 63           6.0         2.2          4.0         1.0 #> 64           6.1         2.9          4.7         1.4 #> 65           5.6         2.9          3.6         1.3 #> 66           6.7         3.1          4.4         1.4 #> 67           5.6         3.0          4.5         1.5 #> 68           5.8         2.7          4.1         1.0 #> 69           6.2         2.2          4.5         1.5 #> 70           5.6         2.5          3.9         1.1 #> 71           5.9         3.2          4.8         1.8 #> 72           6.1         2.8          4.0         1.3 #> 73           6.3         2.5          4.9         1.5 #> 74           6.1         2.8          4.7         1.2 #> 75           6.4         2.9          4.3         1.3 #> 76           6.6         3.0          4.4         1.4 #> 77           6.8         2.8          4.8         1.4 #> 78           6.7         3.0          5.0         1.7 #> 79           6.0         2.9          4.5         1.5 #> 80           5.7         2.6          3.5         1.0 #> 81           5.5         2.4          3.8         1.1 #> 82           5.5         2.4          3.7         1.0 #> 83           5.8         2.7          3.9         1.2 #> 101          6.3         3.3          6.0         2.5 #> 102          5.8         2.7          5.1         1.9 #> 103          7.1         3.0          5.9         2.1 #> 104          6.3         2.9          5.6         1.8 #> 105          6.5         3.0          5.8         2.2 #> 106          7.6         3.0          6.6         2.1 #> 107          4.9         2.5          4.5         1.7 #> 108          7.3         2.9          6.3         1.8 #> 109          6.7         2.5          5.8         1.8 #> 110          7.2         3.6          6.1         2.5 #> 111          6.5         3.2          5.1         2.0 #> 112          6.4         2.7          5.3         1.9 #> 113          6.8         3.0          5.5         2.1 #> 114          5.7         2.5          5.0         2.0 #> 115          5.8         2.8          5.1         2.4 #> 116          6.4         3.2          5.3         2.3 #> 117          6.5         3.0          5.5         1.8 #> 118          7.7         3.8          6.7         2.2 #> 119          7.7         2.6          6.9         2.3 #> 120          6.0         2.2          5.0         1.5 #> 121          6.9         3.2          5.7         2.3 #> 122          5.6         2.8          4.9         2.0 #> 123          7.7         2.8          6.7         2.0 #> 124          6.3         2.7          4.9         1.8 #> 125          6.7         3.3          5.7         2.1 #> 126          7.2         3.2          6.0         1.8 #> 127          6.2         2.8          4.8         1.8 #> 128          6.1         3.0          4.9         1.8 #> 129          6.4         2.8          5.6         2.1 #> 130          7.2         3.0          5.8         1.6 #> 131          7.4         2.8          6.1         1.9 #> 132          7.9         3.8          6.4         2.0 #> 133          6.4         2.8          5.6         2.2 response(iris_rf) #>   [1] setosa     setosa     setosa     setosa     setosa     setosa     #>   [7] setosa     setosa     setosa     setosa     setosa     setosa     #>  [13] setosa     setosa     setosa     setosa     setosa     setosa     #>  [19] setosa     setosa     setosa     setosa     setosa     setosa     #>  [25] setosa     setosa     setosa     setosa     setosa     setosa     #>  [31] setosa     setosa     setosa     setosa     versicolor versicolor #>  [37] versicolor versicolor versicolor versicolor versicolor versicolor #>  [43] versicolor versicolor versicolor versicolor versicolor versicolor #>  [49] versicolor versicolor versicolor versicolor versicolor versicolor #>  [55] versicolor versicolor versicolor versicolor versicolor versicolor #>  [61] versicolor versicolor versicolor versicolor versicolor versicolor #>  [67] versicolor virginica  virginica  virginica  virginica  virginica  #>  [73] virginica  virginica  virginica  virginica  virginica  virginica  #>  [79] virginica  virginica  virginica  virginica  virginica  virginica  #>  [85] virginica  virginica  virginica  virginica  virginica  virginica  #>  [91] virginica  virginica  virginica  virginica  virginica  virginica  #>  [97] virginica  virginica  virginica  virginica  #> Levels: setosa versicolor virginica  # 3) Find optimal values for the parameters of the model #    This is usally done iteratively. Just an example with ntree where a plot #    exists to help finding optimal value plot(iris_rf)  # For such a relatively simple case, 50 trees are enough, retrain with it iris_rf <- ml_rforest(data = iris_train, Species ~ ., ntree = 50) summary(iris_rf) #> A mlearning object of class mlRforest (random forest): #> Initial call: mlRforest.formula(formula = Species ~ ., data = iris_train, ntree = 50) #>  #> Call: #>  randomForest(x = train, y = response, ntree = ntree, replace = replace,      classwt = classwt, .args. = ..1)  #>                Type of random forest: classification #>                      Number of trees: 50 #> No. of variables tried at each split: 2 #>  #>         OOB estimate of  error rate: 6% #> Confusion matrix: #>            setosa versicolor virginica class.error #> setosa         34          0         0  0.00000000 #> versicolor      0         31         2  0.06060606 #> virginica       0          4        29  0.12121212  # 4) Study the classifier performances. Several metrics and tools exists #    like ROC curves, AUC, etc. Tools provided here are the confusion matrix #    and the metrics that are calculated on it. predict(iris_rf) # Default type is class #>   [1] setosa     setosa     setosa     setosa     setosa     setosa     #>   [7] setosa     setosa     setosa     setosa     setosa     setosa     #>  [13] setosa     setosa     setosa     setosa     setosa     setosa     #>  [19] setosa     setosa     setosa     setosa     setosa     setosa     #>  [25] setosa     setosa     setosa     setosa     setosa     setosa     #>  [31] setosa     setosa     setosa     setosa     versicolor versicolor #>  [37] versicolor versicolor versicolor versicolor versicolor versicolor #>  [43] versicolor versicolor versicolor versicolor versicolor versicolor #>  [49] versicolor versicolor versicolor versicolor versicolor versicolor #>  [55] versicolor versicolor versicolor versicolor versicolor versicolor #>  [61] versicolor versicolor versicolor versicolor versicolor versicolor #>  [67] versicolor virginica  virginica  virginica  virginica  virginica  #>  [73] virginica  virginica  virginica  virginica  virginica  virginica  #>  [79] virginica  virginica  virginica  virginica  virginica  virginica  #>  [85] virginica  virginica  virginica  virginica  virginica  virginica  #>  [91] virginica  virginica  virginica  virginica  virginica  virginica  #>  [97] virginica  virginica  virginica  virginica  #> Levels: setosa versicolor virginica predict(iris_rf, type = \"membership\") #>     setosa versicolor virginica #> 1     1.00       0.00      0.00 #> 2     1.00       0.00      0.00 #> 3     1.00       0.00      0.00 #> 4     1.00       0.00      0.00 #> 5     1.00       0.00      0.00 #> 6     1.00       0.00      0.00 #> 7     1.00       0.00      0.00 #> 8     1.00       0.00      0.00 #> 9     1.00       0.00      0.00 #> 10    1.00       0.00      0.00 #> 11    1.00       0.00      0.00 #> 12    1.00       0.00      0.00 #> 13    1.00       0.00      0.00 #> 14    1.00       0.00      0.00 #> 15    0.94       0.06      0.00 #> 16    0.94       0.06      0.00 #> 17    1.00       0.00      0.00 #> 18    1.00       0.00      0.00 #> 19    0.94       0.06      0.00 #> 20    1.00       0.00      0.00 #> 21    1.00       0.00      0.00 #> 22    1.00       0.00      0.00 #> 23    1.00       0.00      0.00 #> 24    1.00       0.00      0.00 #> 25    1.00       0.00      0.00 #> 26    1.00       0.00      0.00 #> 27    1.00       0.00      0.00 #> 28    1.00       0.00      0.00 #> 29    1.00       0.00      0.00 #> 30    1.00       0.00      0.00 #> 31    1.00       0.00      0.00 #> 32    1.00       0.00      0.00 #> 33    1.00       0.00      0.00 #> 34    0.94       0.06      0.00 #> 51    0.00       0.98      0.02 #> 52    0.00       1.00      0.00 #> 53    0.00       0.94      0.06 #> 54    0.00       0.98      0.02 #> 55    0.00       1.00      0.00 #> 56    0.00       1.00      0.00 #> 57    0.00       0.94      0.06 #> 58    0.00       0.94      0.06 #> 59    0.00       1.00      0.00 #> 60    0.00       0.96      0.04 #> 61    0.00       0.96      0.04 #> 62    0.00       1.00      0.00 #> 63    0.00       0.96      0.04 #> 64    0.00       1.00      0.00 #> 65    0.00       1.00      0.00 #> 66    0.00       1.00      0.00 #> 67    0.00       1.00      0.00 #> 68    0.00       1.00      0.00 #> 69    0.00       0.96      0.04 #> 70    0.00       1.00      0.00 #> 71    0.00       0.72      0.28 #> 72    0.00       1.00      0.00 #> 73    0.00       0.82      0.18 #> 74    0.00       1.00      0.00 #> 75    0.00       1.00      0.00 #> 76    0.00       1.00      0.00 #> 77    0.00       0.96      0.04 #> 78    0.00       0.62      0.38 #> 79    0.00       1.00      0.00 #> 80    0.00       1.00      0.00 #> 81    0.00       1.00      0.00 #> 82    0.00       1.00      0.00 #> 83    0.00       1.00      0.00 #> 101   0.00       0.00      1.00 #> 102   0.00       0.02      0.98 #> 103   0.00       0.00      1.00 #> 104   0.00       0.00      1.00 #> 105   0.00       0.00      1.00 #> 106   0.00       0.00      1.00 #> 107   0.00       0.42      0.58 #> 108   0.00       0.00      1.00 #> 109   0.00       0.00      1.00 #> 110   0.00       0.00      1.00 #> 111   0.00       0.00      1.00 #> 112   0.00       0.00      1.00 #> 113   0.00       0.00      1.00 #> 114   0.00       0.02      0.98 #> 115   0.00       0.02      0.98 #> 116   0.00       0.00      1.00 #> 117   0.00       0.00      1.00 #> 118   0.00       0.00      1.00 #> 119   0.00       0.00      1.00 #> 120   0.00       0.36      0.64 #> 121   0.00       0.02      0.98 #> 122   0.00       0.06      0.94 #> 123   0.00       0.00      1.00 #> 124   0.00       0.04      0.96 #> 125   0.00       0.00      1.00 #> 126   0.00       0.00      1.00 #> 127   0.00       0.20      0.80 #> 128   0.00       0.10      0.90 #> 129   0.00       0.00      1.00 #> 130   0.00       0.20      0.80 #> 131   0.00       0.00      1.00 #> 132   0.00       0.00      1.00 #> 133   0.00       0.00      1.00 predict(iris_rf, type = \"both\") #> $class #>   [1] setosa     setosa     setosa     setosa     setosa     setosa     #>   [7] setosa     setosa     setosa     setosa     setosa     setosa     #>  [13] setosa     setosa     setosa     setosa     setosa     setosa     #>  [19] setosa     setosa     setosa     setosa     setosa     setosa     #>  [25] setosa     setosa     setosa     setosa     setosa     setosa     #>  [31] setosa     setosa     setosa     setosa     versicolor versicolor #>  [37] versicolor versicolor versicolor versicolor versicolor versicolor #>  [43] versicolor versicolor versicolor versicolor versicolor versicolor #>  [49] versicolor versicolor versicolor versicolor versicolor versicolor #>  [55] versicolor versicolor versicolor versicolor versicolor versicolor #>  [61] versicolor versicolor versicolor versicolor versicolor versicolor #>  [67] versicolor virginica  virginica  virginica  virginica  virginica  #>  [73] virginica  virginica  virginica  virginica  virginica  virginica  #>  [79] virginica  virginica  virginica  virginica  virginica  virginica  #>  [85] virginica  virginica  virginica  virginica  virginica  virginica  #>  [91] virginica  virginica  virginica  virginica  virginica  virginica  #>  [97] virginica  virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>  #> $membership #>     setosa versicolor virginica #> 1     1.00       0.00      0.00 #> 2     1.00       0.00      0.00 #> 3     1.00       0.00      0.00 #> 4     1.00       0.00      0.00 #> 5     1.00       0.00      0.00 #> 6     1.00       0.00      0.00 #> 7     1.00       0.00      0.00 #> 8     1.00       0.00      0.00 #> 9     1.00       0.00      0.00 #> 10    1.00       0.00      0.00 #> 11    1.00       0.00      0.00 #> 12    1.00       0.00      0.00 #> 13    1.00       0.00      0.00 #> 14    1.00       0.00      0.00 #> 15    0.94       0.06      0.00 #> 16    0.94       0.06      0.00 #> 17    1.00       0.00      0.00 #> 18    1.00       0.00      0.00 #> 19    0.94       0.06      0.00 #> 20    1.00       0.00      0.00 #> 21    1.00       0.00      0.00 #> 22    1.00       0.00      0.00 #> 23    1.00       0.00      0.00 #> 24    1.00       0.00      0.00 #> 25    1.00       0.00      0.00 #> 26    1.00       0.00      0.00 #> 27    1.00       0.00      0.00 #> 28    1.00       0.00      0.00 #> 29    1.00       0.00      0.00 #> 30    1.00       0.00      0.00 #> 31    1.00       0.00      0.00 #> 32    1.00       0.00      0.00 #> 33    1.00       0.00      0.00 #> 34    0.94       0.06      0.00 #> 51    0.00       0.98      0.02 #> 52    0.00       1.00      0.00 #> 53    0.00       0.94      0.06 #> 54    0.00       0.98      0.02 #> 55    0.00       1.00      0.00 #> 56    0.00       1.00      0.00 #> 57    0.00       0.94      0.06 #> 58    0.00       0.94      0.06 #> 59    0.00       1.00      0.00 #> 60    0.00       0.96      0.04 #> 61    0.00       0.96      0.04 #> 62    0.00       1.00      0.00 #> 63    0.00       0.96      0.04 #> 64    0.00       1.00      0.00 #> 65    0.00       1.00      0.00 #> 66    0.00       1.00      0.00 #> 67    0.00       1.00      0.00 #> 68    0.00       1.00      0.00 #> 69    0.00       0.96      0.04 #> 70    0.00       1.00      0.00 #> 71    0.00       0.72      0.28 #> 72    0.00       1.00      0.00 #> 73    0.00       0.82      0.18 #> 74    0.00       1.00      0.00 #> 75    0.00       1.00      0.00 #> 76    0.00       1.00      0.00 #> 77    0.00       0.96      0.04 #> 78    0.00       0.62      0.38 #> 79    0.00       1.00      0.00 #> 80    0.00       1.00      0.00 #> 81    0.00       1.00      0.00 #> 82    0.00       1.00      0.00 #> 83    0.00       1.00      0.00 #> 101   0.00       0.00      1.00 #> 102   0.00       0.02      0.98 #> 103   0.00       0.00      1.00 #> 104   0.00       0.00      1.00 #> 105   0.00       0.00      1.00 #> 106   0.00       0.00      1.00 #> 107   0.00       0.42      0.58 #> 108   0.00       0.00      1.00 #> 109   0.00       0.00      1.00 #> 110   0.00       0.00      1.00 #> 111   0.00       0.00      1.00 #> 112   0.00       0.00      1.00 #> 113   0.00       0.00      1.00 #> 114   0.00       0.02      0.98 #> 115   0.00       0.02      0.98 #> 116   0.00       0.00      1.00 #> 117   0.00       0.00      1.00 #> 118   0.00       0.00      1.00 #> 119   0.00       0.00      1.00 #> 120   0.00       0.36      0.64 #> 121   0.00       0.02      0.98 #> 122   0.00       0.06      0.94 #> 123   0.00       0.00      1.00 #> 124   0.00       0.04      0.96 #> 125   0.00       0.00      1.00 #> 126   0.00       0.00      1.00 #> 127   0.00       0.20      0.80 #> 128   0.00       0.10      0.90 #> 129   0.00       0.00      1.00 #> 130   0.00       0.20      0.80 #> 131   0.00       0.00      1.00 #> 132   0.00       0.00      1.00 #> 133   0.00       0.00      1.00 #>  # Confusion matrice and metrics using 10-fols cross-validation iris_rf_conf <- confusion(iris_rf, method = \"cv\") iris_rf_conf #> 100 items classified with 95 true positives (error rate = 5%) #>                Predicted #> Actual           01  02  03 (sum) (FNR%) #>   01 setosa      34   0   0    34      0 #>   02 versicolor   0  31   2    33      6 #>   03 virginica    0   3  30    33      9 #>   (sum)          34  34  32   100      5 summary(iris_rf_conf) #> 100 items classified with 95 true positives (error = 5%) #>  #> Global statistics on reweighted data: #> Error rate: 5%, F(micro-average): 0.95, F(macro-average): 0.949 #>  #>               Fscore    Recall Precision Specificity       NPV        FPR #> setosa     1.0000000 1.0000000 1.0000000   1.0000000 1.0000000 0.00000000 #> versicolor 0.9253731 0.9393939 0.9117647   0.9552239 0.9696970 0.04477612 #> virginica  0.9230769 0.9090909 0.9375000   0.9701493 0.9558824 0.02985075 #>                   FNR        FDR        FOR     LRPT       LRNT     LRPS #> setosa     0.00000000 0.00000000 0.00000000      Inf 0.00000000      Inf #> versicolor 0.06060606 0.08823529 0.03030303 20.97980 0.06344697 30.08824 #> virginica  0.09090909 0.06250000 0.04411765 30.45455 0.09370629 21.25000 #>                  LRNS    BalAcc       MCC     Chisq  Bray Auto Manu A_M TP FP #> setosa     0.00000000 1.0000000 1.0000000 100.00000 0.000   34   34   0 34  0 #> versicolor 0.09099265 0.9473089 0.8880154  78.85713 0.005   34   33   1 31  3 #> virginica  0.06538462 0.9396201 0.8862831  78.54976 0.005   32   33  -1 30  2 #>            FN TN #> setosa      0 66 #> versicolor  2 64 #> virginica   3 65 # Note you may want to manipulate priors too, see ?prior  # 5) Go back to step #1 and refine the process until you are happy with the #    results. Then, you can use the classifier to predict unknown items."},{"path":"https://www.sciviews.org/mlearning/reference/plot.confusion.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a confusion matrix — plot.confusion","title":"Plot a confusion matrix — plot.confusion","text":"Several graphical representations confusion objects possible: image matrix colored squares, barplot comparing recall precision, stars plot also comparing two metrics, possibly also comparing two different classifiers dataset, dendrogram grouping classes relative errors observed confusion matrix (classes errors pooled together rapidly).","code":""},{"path":"https://www.sciviews.org/mlearning/reference/plot.confusion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a confusion matrix — plot.confusion","text":"","code":"# S3 method for confusion plot(   x,   y = NULL,   type = c(\"image\", \"barplot\", \"stars\", \"dendrogram\"),   stat1 = \"Recall\",   stat2 = \"Precision\",   names,   ... )  confusion_image(   x,   y = NULL,   labels = names(dimnames(x)),   sort = \"ward.D2\",   numbers = TRUE,   digits = 0,   mar = c(3.1, 10.1, 3.1, 3.1),   cex = 1,   asp = 1,   colfun,   ncols = 41,   col0 = FALSE,   grid.col = \"gray\",   ... )  confusionImage(   x,   y = NULL,   labels = names(dimnames(x)),   sort = \"ward.D2\",   numbers = TRUE,   digits = 0,   mar = c(3.1, 10.1, 3.1, 3.1),   cex = 1,   asp = 1,   colfun,   ncols = 41,   col0 = FALSE,   grid.col = \"gray\",   ... )  confusion_barplot(   x,   y = NULL,   col = c(\"PeachPuff2\", \"green3\", \"lemonChiffon2\"),   mar = c(1.1, 8.1, 4.1, 2.1),   cex = 1,   cex.axis = cex,   cex.legend = cex,   main = \"F-score (precision versus recall)\",   numbers = TRUE,   min.width = 17,   ... )  confusionBarplot(   x,   y = NULL,   col = c(\"PeachPuff2\", \"green3\", \"lemonChiffon2\"),   mar = c(1.1, 8.1, 4.1, 2.1),   cex = 1,   cex.axis = cex,   cex.legend = cex,   main = \"F-score (precision versus recall)\",   numbers = TRUE,   min.width = 17,   ... )  confusion_stars(   x,   y = NULL,   stat1 = \"Recall\",   stat2 = \"Precision\",   names,   main,   col = c(\"green2\", \"blue2\", \"green4\", \"blue4\"),   ... )  confusionStars(   x,   y = NULL,   stat1 = \"Recall\",   stat2 = \"Precision\",   names,   main,   col = c(\"green2\", \"blue2\", \"green4\", \"blue4\"),   ... )  confusion_dendrogram(   x,   y = NULL,   labels = rownames(x),   sort = \"ward.D2\",   main = \"Groups clustering\",   ... )  confusionDendrogram(   x,   y = NULL,   labels = rownames(x),   sort = \"ward.D2\",   main = \"Groups clustering\",   ... )"},{"path":"https://www.sciviews.org/mlearning/reference/plot.confusion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a confusion matrix — plot.confusion","text":"x confusion object y NULL (used), second confusion object two different classifications compared plot (\"stars\" type). type kind plot produce (\"image\", default, \"barplot\", \"stars\", \"dendrogram\"). stat1 first metric plot \"stars\" type (Recall default). stat2 second metric plot \"stars\" type (Precision default). names names two classifiers compare ... arguments passed function. can arguments corresponding plot. labels labels use two classifications. default, vars, one confusion matrix. sort rows columns confusion matrix sorted classes larger confusion closer together? Sorting done using hierarchical clustering hclust(). clustering method \"ward.D2\" default, see hclust() help options). FALSE NULL, sorting done. numbers actual numbers indicated confusion matrix image? digits number digits decimal point print confusion matrix. default zero leads compact presentation suitable frequencies, relative frequencies. mar graph margins. cex text magnification factor. asp graph aspect ratio. little reasons change default value 1. colfun function calculates series colors, like e.g., cm.colors() accepts one argument number colors generated. ncols number colors generate. preferably 2 * number levels + 1, levels number frequencies want evidence plot. Default 41. col0 null values colored (, default)? grid.col color use grid lines, NULL drawing grid lines. col color(s) use plot. cex.axis idem axes. NULL, axis drawn. cex.legend idem legend text. NULL, legend added. main main title plot. min.width minimum bar width required add numbers.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/plot.confusion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a confusion matrix — plot.confusion","text":"Data calculate create plots returned invisibly. functions mostly used side-effect producing plot.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/plot.confusion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot a confusion matrix — plot.confusion","text":"","code":"data(\"Glass\", package = \"mlbench\") # Use a little bit more informative labels for Type Glass$Type <- as.factor(paste(\"Glass\", Glass$Type))  # Use learning vector quantization to classify the glass types # (using default parameters) summary(glass_lvq <- ml_lvq(Type ~ ., data = Glass)) #> Codebook: #>       Class       RI       Na         Mg        Al       Si           K #> 52  Glass 1 1.517587 13.28383  3.6267597 1.0844480 73.27093  0.58301543 #> 19  Glass 1 1.519656 13.68053  3.9576673 1.0737817 72.09487  0.17855195 #> 66  Glass 1 1.521222 13.16605  4.0158354 0.7779961 72.34701  0.12313837 #> 20  Glass 1 1.519487 12.88883  4.1584855 1.8885698 73.05621  0.30102838 #> 37  Glass 1 1.520243 13.41644  4.2643859 0.7809437 72.32901  0.39032602 #> 51  Glass 1 1.523579 13.60401  4.0741686 0.3161763 72.16985  0.03772843 #> 30  Glass 1 1.517670 12.85907  3.5462287 1.2255746 73.12634  0.60405215 #> 54  Glass 1 1.514249 13.63938  3.1548696 0.7212818 73.17292  0.64804192 #> 138 Glass 2 1.516330 13.16385  3.6306432 1.4898087 72.97339  0.45828713 #> 113 Glass 2 1.528203 11.95054  0.0000000 1.1133036 71.76714  0.19607143 #> 120 Glass 2 1.517513 13.72800  4.6410000 1.0050000 72.97500 -1.03100000 #> 139 Glass 2 1.517274 12.64250  3.8740353 1.5782136 73.58350  0.04156875 #> 73  Glass 2 1.515291 12.77167  3.6692554 1.3495262 73.54321  0.55444923 #> 140 Glass 2 1.516658 12.60834  3.7009893 1.3986225 73.69755  0.47041893 #> 116 Glass 2 1.518434 13.31928  3.9615004 1.2960403 72.42758  0.49444294 #> 93  Glass 2 1.516386 12.54969  3.1536448 1.2577365 73.50275  0.42494827 #> 153 Glass 3 1.517635 13.64600  3.6548947 0.6717895 73.00347  0.05905263 #> 162 Glass 3 1.518047 13.73238  3.5147942 0.4314259 72.80701  0.14350720 #> 170 Glass 5 1.520470 12.01456  0.4963314 1.9306212 73.18111  0.64154891 #> 180 Glass 6 1.517440 14.65930  1.8830694 1.4012584 73.23657 -0.28905578 #> 196 Glass 7 1.515643 14.56206 -0.0139737 2.7440082 73.35501 -0.26906938 #> 214 Glass 7 1.517318 14.58996  0.0000000 2.0104634 73.19444 -0.08271457 #> 193 Glass 7 1.516704 11.91430  0.0000000 1.5229900 76.43517  1.26300551 #>            Ca           Ba           Fe #> 52   8.179518 -0.077010818 -0.025241541 #> 19   8.843803  0.031945915 -0.063182372 #> 66   9.481523  0.000000000  0.097254536 #> 20   8.589478 -0.072000000 -0.105676559 #> 37   8.953499 -0.290485140 -0.369404755 #> 51   9.617862  0.000000000  0.089839135 #> 30   8.577116 -0.002428826  0.070600884 #> 54   8.480408 -0.239912400 -0.191929920 #> 138  8.075058 -0.046942383  0.013797065 #> 113 14.342679  0.421875000  0.103928571 #> 120  8.269000  0.000000000  0.000000000 #> 139  8.120423  0.000000000 -0.069097045 #> 73   8.271785 -0.376153846 -0.062030769 #> 140  8.434071 -0.507692308 -0.047715976 #> 116  8.341131 -0.129201738  0.131334290 #> 93   8.786087  0.033744980  0.168361057 #> 153  8.896684  0.000000000  0.000000000 #> 162  8.822687  0.282739200  0.452382720 #> 170 11.610939  0.000000000  0.021658224 #> 180  9.283795 -0.188536313 -0.071493388 #> 196  8.791390  0.797433301  0.019055122 #> 214  8.435528  1.795602212  0.006466783 #> 193  8.745619  0.212625653  0.047840772  # Calculate cross-validated confusion matrix and plot it in different ways (glass_conf <- confusion(cvpredict(glass_lvq), Glass$Type)) #> 214 items classified with 143 true positives (error rate = 33.2%) #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 3   0  11   6   0   0   0    17    100 #>   02 Glass 1   1  55  14   0   0   0    70     21 #>   03 Glass 2   0  13  59   3   1   0    76     22 #>   04 Glass 5   0   0   6   6   0   1    13     54 #>   05 Glass 6   1   2   1   1   1   3     9     89 #>   06 Glass 7   0   2   3   1   1  22    29     24 #>   (sum)        2  83  89  11   3  26   214     33 # Raw confusion matrix: no sort and no margins print(glass_conf, sums = FALSE, sort = FALSE) #> 214 items classified with 143 true positives (error rate = 33.2%) #>             Predicted #> Actual       01 02 03 04 05 06 #>   01 Glass 1 55 14  1  0  0  0 #>   02 Glass 2 13 59  0  3  1  0 #>   03 Glass 3 11  6  0  0  0  0 #>   04 Glass 5  0  6  0  6  0  1 #>   05 Glass 6  2  1  1  1  1  3 #>   06 Glass 7  2  3  0  1  1 22 # Plots plot(glass_conf) # Image by default  plot(glass_conf, sort = FALSE) # No sorting  plot(glass_conf, type = \"barplot\") #> Warning: argument 1 does not name a graphical parameter  plot(glass_conf, type = \"stars\")  plot(glass_conf, type = \"dendrogram\")   # Build another classifier and make a comparison summary(glass_naive_bayes <- ml_naive_bayes(Type ~ ., data = Glass)) #> A mlearning object of class mlNaiveBayes (naive Bayes classifier): #> Initial call: mlNaiveBayes.formula(formula = Type ~ ., data = Glass) #>  #> Naive Bayes Classifier for Discrete Predictors #>  #> Call: #> naiveBayes.default(x = train, y = response, laplace = laplace,  #>     .args. = ..1) #>  #> A-priori probabilities: #> response #>    Glass 1    Glass 2    Glass 3    Glass 5    Glass 6    Glass 7  #> 0.32710280 0.35514019 0.07943925 0.06074766 0.04205607 0.13551402  #>  #> Conditional probabilities: #>          RI #> response      [,1]        [,2] #>   Glass 1 1.518718 0.002268097 #>   Glass 2 1.518619 0.003802126 #>   Glass 3 1.517964 0.001916360 #>   Glass 5 1.518928 0.003345355 #>   Glass 6 1.517456 0.003115783 #>   Glass 7 1.517116 0.002545069 #>  #>          Na #> response      [,1]      [,2] #>   Glass 1 13.24229 0.4993015 #>   Glass 2 13.11171 0.6641594 #>   Glass 3 13.43706 0.5068871 #>   Glass 5 12.82769 0.7770366 #>   Glass 6 14.64667 1.0840203 #>   Glass 7 14.44207 0.6863588 #>  #>          Mg #> response       [,1]      [,2] #>   Glass 1 3.5524286 0.2470430 #>   Glass 2 3.0021053 1.2156615 #>   Glass 3 3.5435294 0.1627859 #>   Glass 5 0.7738462 0.9991458 #>   Glass 6 1.3055556 1.0971339 #>   Glass 7 0.5382759 1.1176828 #>  #>          Al #> response      [,1]      [,2] #>   Glass 1 1.163857 0.2731581 #>   Glass 2 1.408158 0.3183403 #>   Glass 3 1.201176 0.3474889 #>   Glass 5 2.033846 0.6939205 #>   Glass 6 1.366667 0.5718610 #>   Glass 7 2.122759 0.4427261 #>  #>          Si #> response      [,1]      [,2] #>   Glass 1 72.61914 0.5694842 #>   Glass 2 72.59803 0.7245726 #>   Glass 3 72.40471 0.5122758 #>   Glass 5 72.36615 1.2823191 #>   Glass 6 73.20667 1.0794675 #>   Glass 7 72.96586 0.9402337 #>  #>          K #> response       [,1]      [,2] #>   Glass 1 0.4474286 0.2148790 #>   Glass 2 0.5210526 0.2137262 #>   Glass 3 0.4064706 0.2298897 #>   Glass 5 1.4700000 2.1386951 #>   Glass 6 0.0000000 0.0000000 #>   Glass 7 0.3251724 0.6684931 #>  #>          Ca #> response       [,1]      [,2] #>   Glass 1  8.797286 0.5748066 #>   Glass 2  9.073684 1.9216353 #>   Glass 3  8.782941 0.3801112 #>   Glass 5 10.123846 2.1837908 #>   Glass 6  9.356667 1.4499483 #>   Glass 7  8.491379 0.9735052 #>  #>          Ba #> response         [,1]       [,2] #>   Glass 1 0.012714286 0.08383769 #>   Glass 2 0.050263158 0.36234044 #>   Glass 3 0.008823529 0.03638034 #>   Glass 5 0.187692308 0.60825096 #>   Glass 6 0.000000000 0.00000000 #>   Glass 7 1.040000000 0.66534094 #>  #>          Fe #> response        [,1]       [,2] #>   Glass 1 0.05700000 0.08907496 #>   Glass 2 0.07973684 0.10643275 #>   Glass 3 0.05705882 0.10786361 #>   Glass 5 0.06076923 0.15558821 #>   Glass 6 0.00000000 0.00000000 #>   Glass 7 0.01344828 0.02979404 #>  (glass_conf2 <- confusion(cvpredict(glass_naive_bayes), Glass$Type)) #> 214 items classified with 88 true positives (error rate = 58.9%) #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 3   3  13   0   0   1   0    17     82 #>   02 Glass 1  15  49   5   0   1   0    70     30 #>   03 Glass 2  13  40  12   5   6   0    76     84 #>   04 Glass 5   0   0   3   0   9   1    13    100 #>   05 Glass 6   0   0   0   0   8   1     9     11 #>   06 Glass 7   0   1   0   2  10  16    29     45 #>   (sum)       31 103  20   7  35  18   214     59  # Comparison plot for two classifiers plot(glass_conf, glass_conf2)"},{"path":"https://www.sciviews.org/mlearning/reference/prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Get or set priors on a confusion matrix — prior","title":"Get or set priors on a confusion matrix — prior","text":"metrics supervised classifications sensitive relative proportion items different classes. confusion matrix calculated test set, uses proportions observed test set. representative proportions population, metrics biased. case, priors confusion object can adjusted better reflect proportions supposed observed different classes order get accurate metrics.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get or set priors on a confusion matrix — prior","text":"","code":"prior(object, ...)  # S3 method for confusion prior(object, ...)  prior(object, ...) <- value  # S3 method for confusion prior(object, ...) <- value"},{"path":"https://www.sciviews.org/mlearning/reference/prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get or set priors on a confusion matrix — prior","text":"object confusion object (another class method implemented) ... arguments passed methods value (named) vector positive numbers zeros length number classes confusion object. can also single >= 0 number case, equal probabilities applied classes (use 1 relative frequencies 100 relative frequencies percent). value zero length NULL, original prior probabilities (test set) used. vector named, names must correspond existing class names confusion object.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get or set priors on a confusion matrix — prior","text":"prior() returns current class frequencies associated first classification tabulated confusion object, .e., rows confusion matrix.","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get or set priors on a confusion matrix — prior","text":"","code":"data(\"Glass\", package = \"mlbench\") # Use a little bit more informative labels for Type Glass$Type <- as.factor(paste(\"Glass\", Glass$Type)) # Use learning vector quantization to classify the glass types # (using default parameters) summary(glass_lvq <- ml_lvq(Type ~ ., data = Glass)) #> Codebook: #>       Class       RI       Na         Mg        Al       Si           K #> 39  Glass 1 1.521277 14.36424  4.0239650 0.2303602 72.02336  0.05438544 #> 32  Glass 1 1.519196 12.65659  4.8674990 0.4665660 73.85181 -0.70557100 #> 42  Glass 1 1.517072 13.05397  3.5624747 1.1790037 73.04297  0.36584618 #> 62  Glass 1 1.519485 13.65144  3.9084643 1.1892587 72.02706  0.17018101 #> 17  Glass 1 1.517766 12.73950  3.5976780 1.2351181 73.12186  0.58917126 #> 53  Glass 1 1.512427 13.32208  3.9524905 0.4489736 73.89021 -0.41723689 #> 7   Glass 1 1.517945 13.24449  3.6731433 1.0806355 73.10465  0.51905537 #> 65  Glass 1 1.521897 13.29258  3.9273652 0.7334249 72.17823  0.12908872 #> 99  Glass 2 1.517391 12.49545  3.4441722 1.0178870 73.56065  0.20786576 #> 112 Glass 2 1.527390 11.02000  0.0000000 0.7500000 73.08000  0.00000000 #> 90  Glass 2 1.516678 12.63550  4.0566619 1.7910617 73.35970 -0.21845344 #> 123 Glass 2 1.517596 13.34163  3.8999283 1.3457780 72.48911  0.51160215 #> 146 Glass 2 1.518161 12.94614  3.8968416 1.1444819 72.40279  0.61233719 #> 74  Glass 2 1.514592 14.31787  3.7701862 1.4697558 72.65758  0.01233319 #> 140 Glass 2 1.516436 13.01225  3.5639963 1.5689297 73.06020  0.44685888 #> 111 Glass 2 1.529245 12.18383  0.0000000 1.3056921 71.14729  0.23481414 #> 147 Glass 3 1.516647 13.38319  4.2593376 0.9504240 72.69514  0.12440480 #> 153 Glass 3 1.518183 13.64508  3.6166110 0.6917897 72.88964  0.09196932 #> 168 Glass 5 1.520283 11.71126  0.8515020 2.1059832 73.20731  0.56499288 #> 178 Glass 6 1.517372 14.38524  2.2219272 1.2808706 73.32370 -0.14151907 #> 203 Glass 7 1.515268 14.47779 -0.1009342 2.7167221 73.54135 -0.24604345 #> 209 Glass 7 1.514531 14.25746 -0.6027840 3.6910000 73.68328 -1.82460000 #> 214 Glass 7 1.518406 14.80140  0.1955234 1.8482661 72.96822 -0.15294447 #>            Ca           Ba          Fe #> 39   9.201240 -0.082865256 -0.02213713 #> 32   8.766666  0.000000000 -0.27342900 #> 42   8.617063 -0.016637285  0.07246322 #> 62   8.772069  0.177102475 -0.04529924 #> 17   8.595615 -0.001611879  0.04024410 #> 53   9.165447 -0.272307692 -0.14355668 #> 7    8.361845 -0.077109478 -0.01304638 #> 65   9.642040  0.000000000  0.08271237 #> 99   8.930357  0.015530409  0.07132736 #> 112 14.960000  0.000000000  0.00000000 #> 90   8.192072  0.000000000  0.03076215 #> 123  8.218452 -0.104701969  0.06487314 #> 146  8.644896  0.000000000  0.27817524 #> 74   8.354815 -0.925046496  0.09880781 #> 140  8.164653 -0.044621318 -0.01078383 #> 111 14.260043  0.681301667  0.15537564 #> 147  8.560674  0.000000000  0.00000000 #> 153  8.895123  0.051116040  0.08178566 #> 168 11.387505  0.012140868  0.04744348 #> 178  9.101822 -0.229954212 -0.05084183 #> 203  8.726922  0.840061733  0.02440280 #> 209  9.261000  1.530912000  0.00000000 #> 214  8.307199  1.956745508  0.02198064  # Calculate cross-validated confusion matrix (glass_conf <- confusion(cvpredict(glass_lvq), Glass$Type)) #> 214 items classified with 141 true positives (error rate = 34.1%) #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 3   0  11   6   0   0   0    17    100 #>   02 Glass 1   0  58  12   0   0   0    70     17 #>   03 Glass 2   0  21  51   3   0   1    76     33 #>   04 Glass 5   0   0   4   7   0   2    13     46 #>   05 Glass 6   0   2   1   0   2   4     9     78 #>   06 Glass 7   0   2   3   1   0  23    29     21 #>   (sum)        0  94  77  11   2  30   214     34  # When the probabilities in each class do not match the proportions in the # training set, all these calculations are useless. Having an idea of # the real proportions (so-called, priors), one should first reweight the # confusion matrix before calculating statistics, for instance: prior1 <- c(10, 10, 10, 100, 100, 100) # Glass types 1-3 are rare prior(glass_conf) <- prior1 glass_conf #> 214 items classified with 141 true positives (error rate = 34.1%) #> with initial row frequencies: #> Glass 1 Glass 2 Glass 3 Glass 5 Glass 6 Glass 7  #>      70      76      17      13       9      29  #> Rescaled to: #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 2   7   0   0   0   3   0    10     33 #>   02 Glass 5  31  54   0  15   0   0   100     46 #>   03 Glass 6  11   0  22  44  22   0   100     78 #>   04 Glass 7  10   3   0  79   7   0   100     21 #>   05 Glass 1   2   0   0   0   8   0    10     17 #>   06 Glass 3   4   0   0   0   6   0    10    100 #>   (sum)       64  58  22 139  47   0   330     48 summary(glass_conf, type = c(\"Fscore\", \"Recall\", \"Precision\")) #> 214 items classified with 141 true positives (error = 34.1%) #>  #> Global statistics on reweighted data: #> Error rate: 48.4%, F(micro-average): 0.486, F(macro-average): 0.364 #>  #>            Fscore    Recall Precision #> Glass 5 0.6829404 0.5384615 0.9333842 #> Glass 7 0.6629332 0.7931034 0.5694678 #> Glass 6 0.3636364 0.2222222 1.0000000 #> Glass 1 0.2925838 0.8285714 0.1776593 #> Glass 2 0.1809270 0.6710526 0.1045589 #> Glass 3 0.0000000 0.0000000       NaN  # This is very different than if glass types 1-3 are abundants! prior2 <- c(100, 100, 100, 10, 10, 10) # Glass types 1-3 are abundants prior(glass_conf) <- prior2 glass_conf #> 214 items classified with 141 true positives (error rate = 34.1%) #> with initial row frequencies: #> Glass 1 Glass 2 Glass 3 Glass 5 Glass 6 Glass 7  #>      70      76      17      13       9      29  #> Rescaled to: #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 2  67  28   0   4   0   1   100     33 #>   02 Glass 1  17  83   0   0   0   0   100     17 #>   03 Glass 3  35  65   0   0   0   0   100    100 #>   04 Glass 5   3   0   0   5   0   2    10     46 #>   05 Glass 6   1   2   0   0   2   4    10     78 #>   06 Glass 7   1   1   0   0   0   8    10     21 #>   (sum)      125 178   0  10   2  15   330     50 summary(glass_conf, type = c(\"Fscore\", \"Recall\", \"Precision\")) #> 214 items classified with 141 true positives (error = 34.1%) #>  #> Global statistics on reweighted data: #> Error rate: 49.8%, F(micro-average): 0.511, F(macro-average): 0.455 #>  #>            Fscore    Recall Precision #> Glass 7 0.6287055 0.7931034 0.5207600 #> Glass 2 0.5971155 0.6710526 0.5378543 #> Glass 1 0.5958663 0.8285714 0.4652113 #> Glass 5 0.5473057 0.5384615 0.5564452 #> Glass 6 0.3636364 0.2222222 1.0000000 #> Glass 3 0.0000000 0.0000000       NaN  # Weight can also be used to construct a matrix of relative frequencies # In this case, all rows sum to one prior(glass_conf) <- 1 print(glass_conf, digits = 2) #> 214 items classified with 141 true positives (error rate = 34.1%) #> with initial row frequencies: #> Glass 1 Glass 2 Glass 3 Glass 5 Glass 6 Glass 7  #>      70      76      17      13       9      29  #> Rescaled to: #>             Predicted #> Actual           01     02     03     04     05     06  (sum) (FNR%) #>   01 Glass 6   0.22   0.44   0.22   0.00   0.11   0.00   1.00  78.00 #>   02 Glass 7   0.00   0.79   0.07   0.00   0.10   0.03   1.00  21.00 #>   03 Glass 1   0.00   0.00   0.83   0.00   0.17   0.00   1.00  17.00 #>   04 Glass 3   0.00   0.00   0.65   0.00   0.35   0.00   1.00 100.00 #>   05 Glass 2   0.00   0.01   0.28   0.00   0.67   0.04   1.00  33.00 #>   06 Glass 5   0.00   0.15   0.00   0.00   0.31   0.54   1.00  46.00 #>   (sum)        0.22   1.40   2.04   0.00   1.72   0.61   6.00  49.00 # However, it is easier to work with relative frequencies in percent # and one gets a more compact presentation prior(glass_conf) <- 100 glass_conf #> 214 items classified with 141 true positives (error rate = 34.1%) #> with initial row frequencies: #> Glass 1 Glass 2 Glass 3 Glass 5 Glass 6 Glass 7  #>      70      76      17      13       9      29  #> Rescaled to: #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 6  22  44  22   0  11   0   100     78 #>   02 Glass 7   0  79   7   0  10   3   100     21 #>   03 Glass 1   0   0  83   0  17   0   100     17 #>   04 Glass 3   0   0  65   0  35   0   100    100 #>   05 Glass 2   0   1  28   0  67   4   100     33 #>   06 Glass 5   0  15   0   0  31  54   100     46 #>   (sum)       22 140 204   0 172  61   600     49  # To reset row class frequencies to original propotions, just assign NULL prior(glass_conf) <- NULL glass_conf #> 214 items classified with 141 true positives (error rate = 34.1%) #>             Predicted #> Actual        01  02  03  04  05  06 (sum) (FNR%) #>   01 Glass 3   0  11   6   0   0   0    17    100 #>   02 Glass 1   0  58  12   0   0   0    70     17 #>   03 Glass 2   0  21  51   3   0   1    76     33 #>   04 Glass 5   0   0   4   7   0   2    13     46 #>   05 Glass 6   0   2   1   0   2   4     9     78 #>   06 Glass 7   0   2   3   1   0  23    29     21 #>   (sum)        0  94  77  11   2  30   214     34 prior(glass_conf) #> Glass 1 Glass 2 Glass 3 Glass 5 Glass 6 Glass 7  #>      70      76      17      13       9      29"},{"path":"https://www.sciviews.org/mlearning/reference/response.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the response variable for a mlearning object — response","title":"Get the response variable for a mlearning object — response","text":"response either class predicted classification problem (factor), dependent variable regression model (numeric case). unsupervised classification, response provided return NULL.","code":""},{"path":"https://www.sciviews.org/mlearning/reference/response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the response variable for a mlearning object — response","text":"","code":"response(object, ...)  # S3 method for default response(object, ...)"},{"path":"https://www.sciviews.org/mlearning/reference/response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the response variable for a mlearning object — response","text":"object object response variable. ... parameter (depends method).","code":""},{"path":"https://www.sciviews.org/mlearning/reference/response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the response variable for a mlearning object — response","text":"response variable training set, NULL unsupervised classification.","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the response variable for a mlearning object — response","text":"","code":"data(\"HouseVotes84\", package = \"mlbench\") house_rf <- ml_rforest(data = HouseVotes84, Class ~ .) house_rf #> A mlearning object of class mlRforest (random forest): #> Call: mlRforest.formula(formula = Class ~ ., data = HouseVotes84) #> Trained using 232 out of 435 cases: #>   democrat republican  #>        124        108  response(house_rf) #>   [1] democrat   republican democrat   democrat   democrat   democrat   #>   [7] democrat   republican democrat   republican democrat   republican #>  [13] democrat   republican republican republican democrat   democrat   #>  [19] democrat   democrat   democrat   democrat   republican republican #>  [25] republican republican republican republican democrat   republican #>  [31] republican republican democrat   republican republican democrat   #>  [37] democrat   democrat   democrat   republican republican republican #>  [43] democrat   republican republican democrat   democrat   democrat   #>  [49] democrat   democrat   democrat   democrat   democrat   democrat   #>  [55] democrat   republican democrat   democrat   republican democrat   #>  [61] republican republican democrat   republican republican republican #>  [67] democrat   democrat   republican republican republican democrat   #>  [73] republican democrat   republican democrat   democrat   republican #>  [79] republican republican democrat   democrat   democrat   republican #>  [85] republican republican democrat   democrat   democrat   republican #>  [91] democrat   democrat   democrat   democrat   democrat   democrat   #>  [97] democrat   democrat   democrat   democrat   democrat   republican #> [103] democrat   republican democrat   democrat   republican democrat   #> [109] democrat   republican republican democrat   republican democrat   #> [115] republican republican republican republican republican democrat   #> [121] democrat   republican democrat   democrat   democrat   republican #> [127] republican democrat   republican democrat   democrat   republican #> [133] democrat   democrat   democrat   democrat   republican republican #> [139] democrat   democrat   republican republican republican republican #> [145] republican democrat   republican democrat   democrat   democrat   #> [151] democrat   republican democrat   democrat   republican republican #> [157] republican republican republican republican democrat   republican #> [163] democrat   democrat   republican republican democrat   democrat   #> [169] democrat   democrat   republican democrat   republican democrat   #> [175] republican democrat   democrat   democrat   republican republican #> [181] republican democrat   republican republican republican republican #> [187] democrat   republican republican republican republican republican #> [193] democrat   democrat   republican republican democrat   democrat   #> [199] republican republican democrat   republican republican democrat   #> [205] democrat   democrat   democrat   republican democrat   democrat   #> [211] republican republican republican democrat   democrat   republican #> [217] republican democrat   republican democrat   republican democrat   #> [223] democrat   democrat   republican democrat   democrat   democrat   #> [229] democrat   republican republican democrat   #> Levels: democrat republican"},{"path":"https://www.sciviews.org/mlearning/reference/train.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the training variable for a mlearning object — train","title":"Get the training variable for a mlearning object — train","text":"training variables (train) variables used train classifier, excepted prediction (class dependent variable).","code":""},{"path":"https://www.sciviews.org/mlearning/reference/train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the training variable for a mlearning object — train","text":"","code":"train(object, ...)  # S3 method for default train(object, ...)"},{"path":"https://www.sciviews.org/mlearning/reference/train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the training variable for a mlearning object — train","text":"object object train attribute. ... parameter (depends method).","code":""},{"path":"https://www.sciviews.org/mlearning/reference/train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the training variable for a mlearning object — train","text":"data frame containing training variables model.","code":""},{"path":[]},{"path":"https://www.sciviews.org/mlearning/reference/train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the training variable for a mlearning object — train","text":"","code":"data(\"HouseVotes84\", package = \"mlbench\") house_rf <- ml_rforest(data = HouseVotes84, Class ~ .) house_rf #> A mlearning object of class mlRforest (random forest): #> Call: mlRforest.formula(formula = Class ~ ., data = HouseVotes84) #> Trained using 232 out of 435 cases: #>   democrat republican  #>        124        108  train(house_rf) #>     V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 #> 6    n  y  y  n  y  y  n  n  n   n   n   n   y   y   y   y #> 9    n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 20   y  y  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 24   y  y  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 26   y  n  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 27   y  n  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 28   y  y  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 29   y  n  n  y  y  n  y  y  y   n   n   y   y   y   n   y #> 30   y  y  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 31   n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 33   y  y  y  n  n  n  y  y  y   y   n   n   y   n   y   y #> 34   n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 35   y  y  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 36   n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 38   y  y  n  y  y  y  n  n  n   n   n   n   y   y   n   y #> 39   n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 40   y  n  y  n  n  n  y  y  y   y   y   n   y   n   y   y #> 43   y  n  y  n  n  n  y  y  y   n   n   n   n   n   n   y #> 44   y  n  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 47   y  y  y  n  n  n  y  y  y   n   n   n   n   n   n   y #> 49   y  y  y  n  n  n  y  y  n   n   n   n   n   y   n   y #> 51   y  y  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 54   y  y  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 56   n  y  n  y  y  y  n  n  n   y   y   y   y   y   n   n #> 57   n  y  n  y  y  y  n  n  n   y   y   y   y   y   n   y #> 58   n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 59   n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 62   n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 64   y  y  y  n  n  n  y  y  y   n   y   n   n   n   n   y #> 66   y  y  n  y  y  y  y  n  n   n   n   y   y   y   n   y #> 67   n  y  n  y  y  y  y  n  n   n   y   y   y   y   n   y #> 68   n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 70   y  y  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 72   y  y  y  y  n  n  y  y  y   y   y   n   n   y   n   y #> 74   y  n  y  y  y  n  y  n  y   y   n   n   y   y   n   y #> 75   y  n  y  n  n  y  y  y  y   y   y   n   n   y   y   y #> 76   n  y  y  y  y  y  n  n  n   y   y   n   y   y   n   n #> 78   n  y  y  y  y  y  n  y  y   y   y   y   y   y   n   y #> 79   y  y  y  n  y  y  n  n  n   y   y   n   y   y   n   y #> 80   n  n  n  y  y  n  n  n  n   y   n   y   y   y   n   n #> 83   n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 84   n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 86   n  n  y  n  y  y  n  n  n   y   y   y   y   y   n   y #> 87   n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 88   n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 89   n  y  y  n  y  y  y  n  y   y   y   n   y   y   n   y #> 91   y  n  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 92   y  n  y  n  n  n  y  y  y   y   y   n   n   n   y   y #> 94   y  n  y  n  n  n  y  n  y   y   y   n   n   n   y   y #> 95   y  n  y  n  y  y  n  n  n   n   n   n   n   n   n   y #> 98   y  n  n  n  y  y  y  n  n   y   y   n   n   y   n   y #> 99   y  y  y  n  n  y  y  y  y   y   n   n   n   n   n   y #> 101  y  n  n  n  y  y  n  n  n   n   y   y   n   y   n   y #> 102  y  n  y  n  y  y  y  n  n   n   y   n   n   y   n   y #> 106  y  y  y  n  n  n  n  y  y   n   y   n   n   n   y   y #> 107  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 111  n  n  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 117  y  n  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 118  y  y  y  y  y  n  y  n  n   n   n   y   y   y   n   y #> 119  n  y  y  n  n  n  n  y  y   y   y   n   n   n   y   y #> 120  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 123  n  n  n  y  y  y  n  n  n   y   n   y   n   y   n   y #> 132  n  n  y  n  n  y  n  y  y   y   n   n   n   y   n   y #> 134  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 136  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 137  n  y  n  y  y  y  n  n  n   y   y   y   y   n   n   y #> 139  n  n  y  n  n  y  y  y  y   y   n   n   n   y   n   y #> 140  y  n  y  n  n  y  y  y  y   n   n   n   n   n   y   y #> 141  n  n  n  y  n  n  y  y  y   y   n   n   y   y   n   y #> 143  n  n  n  y  y  y  y  y  y   y   n   y   y   y   n   y #> 147  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 148  n  n  n  n  n  n  y  y  y   y   n   y   y   y   y   y #> 149  n  y  n  y  y  y  n  n  n   y   y   y   y   y   n   y #> 150  n  n  y  n  n  n  y  y  y   y   n   n   y   n   y   y #> 151  y  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 153  n  y  y  n  n  y  n  y  y   y   y   n   y   n   y   y #> 154  n  n  y  n  n  y  y  y  y   y   y   n   y   y   n   y #> 155  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 157  y  y  n  y  y  y  y  n  n   n   n   y   y   y   n   n #> 159  n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 161  n  y  n  n  y  y  n  n  n   n   n   y   y   y   y   y #> 162  n  n  n  n  y  y  y  n  n   n   n   y   y   y   n   y #> 163  n  y  y  n  y  y  y  n  n   n   y   y   y   y   n   y #> 164  n  y  n  y  y  y  y  n  n   n   n   y   y   y   n   y #> 167  y  n  y  y  y  y  y  y  n   y   n   y   n   y   y   y #> 168  y  n  y  y  y  y  y  y  n   y   y   y   n   y   y   y #> 170  y  n  y  n  n  n  y  y  y   y   y   n   n   y   n   y #> 174  n  n  n  n  y  y  n  n  n   y   y   y   y   y   n   y #> 176  n  y  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 177  n  n  y  y  n  n  y  y  y   y   n   n   n   y   y   y #> 180  y  n  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 182  n  n  y  n  n  n  y  y  y   y   y   n   n   n   y   y #> 185  n  n  y  n  n  n  y  y  y   y   y   n   n   n   y   y #> 187  n  y  y  n  n  n  y  y  y   y   y   n   n   n   y   y #> 190  y  n  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 193  n  n  n  n  n  y  y  y  y   n   y   n   n   y   y   y #> 194  n  n  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 201  n  n  y  n  n  n  y  y  y   n   n   n   n   y   y   y #> 202  y  y  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 203  y  n  y  n  n  y  y  y  y   y   y   n   n   n   y   y #> 204  y  n  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 205  n  n  y  y  y  y  y  n  n   n   n   y   y   y   n   y #> 206  n  n  y  n  n  y  y  y  y   y   n   y   n   n   n   y #> 207  n  n  n  y  y  y  n  n  n   y   n   y   n   y   n   y #> 210  y  y  y  n  n  n  y  y  y   y   y   n   n   n   n   y #> 211  n  n  y  n  n  y  y  y  y   n   n   n   n   n   y   y #> 212  n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 213  n  n  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 214  n  y  y  n  n  y  n  y  y   n   y   n   y   n   y   y #> 215  y  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 218  n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 220  n  y  y  n  n  n  n  y  y   n   y   n   n   y   y   y #> 224  n  n  n  y  y  n  n  n  n   n   n   y   y   y   n   y #> 227  n  n  y  n  n  y  y  y  y   n   y   n   n   y   y   y #> 230  n  y  y  y  y  y  y  n  y   y   n   y   y   y   n   y #> 231  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 232  n  y  n  y  y  y  n  n  y   y   n   y   y   y   n   y #> 234  n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 236  n  n  n  y  y  y  n  n  n   y   n   y   n   y   n   y #> 237  n  n  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 238  y  n  y  n  n  y  y  y  n   n   n   y   y   n   n   y #> 240  n  n  n  y  y  y  y  n  n   y   n   n   n   y   y   y #> 242  y  n  y  n  n  n  y  y  y   y   y   n   n   y   y   y #> 245  y  n  y  n  n  n  n  y  y   y   n   n   n   n   y   y #> 246  y  n  y  n  n  n  y  y  y   y   y   n   n   n   y   y #> 251  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 252  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 253  y  y  y  n  n  y  y  y  y   n   n   n   n   n   y   y #> 254  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 255  y  n  y  n  n  n  y  y  y   y   n   n   n   n   n   y #> 256  y  n  y  n  n  n  y  y  y   y   n   n   n   y   y   y #> 257  n  n  n  y  y  n  n  n  n   n   n   y   n   y   n   n #> 259  n  n  y  n  n  n  y  y  y   n   y   n   n   n   y   y #> 260  y  n  y  n  n  n  y  y  y   n   n   n   n   n   n   y #> 261  y  n  y  n  n  n  y  y  y   y   n   n   n   n   n   y #> 266  y  n  y  n  n  n  y  y  y   y   n   n   n   n   n   y #> 267  n  n  n  y  y  y  n  n  n   y   n   y   n   y   n   y #> 268  y  n  n  n  n  n  y  y  y   y   n   n   n   y   n   y #> 270  y  n  y  n  n  n  y  y  y   n   n   n   n   n   n   y #> 271  y  y  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 275  y  n  n  y  y  n  y  n  n   y   n   n   n   y   y   y #> 277  n  n  n  y  y  y  n  n  n   n   n   y   y   y   y   n #> 278  n  n  y  y  y  y  y  y  n   y   n   n   n   y   n   y #> 279  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 280  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 281  n  n  y  n  n  n  y  y  y   y   n   n   n   y   n   y #> 284  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 286  n  n  y  n  n  y  y  y  y   y   y   n   n   n   y   y #> 289  y  y  y  n  y  y  n  y  n   y   y   n   y   y   y   y #> 293  y  n  y  n  n  y  y  y  n   y   y   n   y   y   y   y #> 294  y  y  y  n  n  y  y  y  y   y   y   n   y   y   y   y #> 297  n  n  y  y  y  y  n  n  n   y   n   y   y   y   y   y #> 299  n  y  n  n  n  n  y  y  y   y   y   n   n   n   y   y #> 300  n  y  y  n  n  y  y  y  y   y   n   n   y   y   y   y #> 301  n  n  n  y  y  n  y  y  y   y   n   y   y   y   n   y #> 303  n  n  n  y  y  y  y  n  n   y   n   y   y   y   n   y #> 304  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 306  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 307  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 309  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 310  y  n  n  n  n  y  y  y  y   y   n   n   n   y   y   y #> 311  n  n  n  y  y  y  n  n  n   y   n   y   y   y   y   n #> 312  n  n  y  n  n  y  y  y  y   y   n   n   y   n   n   y #> 313  y  y  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 314  n  y  y  y  y  y  n  n  n   y   n   y   y   y   n   y #> 315  n  y  n  y  y  y  y  y  n   n   y   y   y   y   y   y #> 317  n  n  n  n  n  y  n  y  y   n   y   y   y   y   y   n #> 318  y  n  n  n  n  n  y  y  y   y   n   n   n   n   y   y #> 321  n  y  y  n  n  y  n  y  y   y   n   n   y   y   n   y #> 322  y  y  y  n  n  n  y  y  y   y   n   n   y   n   n   y #> 325  n  y  n  y  y  y  n  n  n   n   y   y   y   y   n   n #> 327  y  y  n  y  n  n  y  y  y   n   y   n   n   y   n   y #> 328  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 329  y  y  y  n  n  n  y  y  y   n   y   n   n   n   n   y #> 331  n  y  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 334  n  n  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 338  y  n  y  n  n  n  y  y  y   n   n   n   n   n   y   y #> 339  y  n  y  n  n  n  y  y  y   y   n   n   n   y   y   y #> 340  y  n  n  y  y  y  n  n  n   n   y   y   y   y   n   n #> 341  n  n  n  y  y  y  n  n  n   y   y   y   n   y   n   y #> 344  n  n  n  y  y  n  y  n  y   y   n   n   n   y   n   y #> 345  n  n  y  n  n  n  y  y  y   y   y   n   n   n   y   y #> 346  n  n  n  y  y  y  y  n  n   y   n   y   n   y   y   y #> 347  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 348  y  n  n  y  y  y  n  n  n   y   n   y   y   y   n   n #> 350  n  y  y  y  y  y  y  y  y   n   n   y   y   y   n   y #> 351  n  y  n  n  n  y  y  n  y   n   y   n   n   n   y   y #> 352  n  n  y  y  y  y  y  y  y   y   n   y   y   y   y   y #> 354  n  n  y  y  y  y  y  n  n   y   y   y   y   y   n   y #> 356  y  n  y  y  n  n  n  y  y   y   n   n   n   y   y   y #> 357  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 358  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 361  y  n  y  n  n  y  y  y  y   y   n   n   y   n   n   y #> 363  y  y  y  n  n  y  y  y  y   y   y   y   y   n   n   y #> 364  y  y  n  y  y  y  n  n  n   y   y   n   y   n   n   n #> 365  y  y  n  y  y  y  n  n  n   n   y   n   y   y   n   y #> 366  n  y  n  n  y  y  n  n  n   y   y   n   y   y   n   n #> 369  n  y  y  n  n  y  y  y  n   y   n   n   n   n   y   y #> 370  n  y  n  y  y  y  n  n  n   n   n   n   y   y   n   y #> 375  n  y  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 376  n  y  n  y  y  y  n  n  n   n   y   y   n   y   n   n #> 379  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 380  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 384  y  y  y  n  y  y  n  y  y   y   y   n   n   n   n   y #> 385  y  y  y  y  y  y  n  n  n   n   y   y   y   y   n   y #> 386  y  y  n  n  y  y  n  n  n   n   y   y   y   y   y   n #> 392  y  y  n  n  n  n  n  y  y   n   y   n   n   n   y   n #> 393  y  y  n  y  y  y  n  n  n   n   y   y   y   y   n   y #> 397  y  y  y  n  y  y  n  y  n   n   y   n   y   n   y   y #> 399  n  y  y  n  y  y  n  y  n   n   n   n   n   n   n   y #> 402  n  y  n  y  y  y  n  n  n   y   y   y   y   y   n   n #> 405  y  y  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 406  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   y #> 407  y  n  y  n  y  y  n  n  y   y   n   n   y   y   n   y #> 408  n  n  n  y  y  y  n  n  n   n   y   y   y   y   n   n #> 410  n  n  n  y  y  y  n  n  n   n   n   y   y   y   n   n #> 411  n  n  n  y  y  y  n  n  n   n   y   y   y   y   n   y #> 412  y  n  y  n  n  y  y  y  y   y   y   n   n   n   n   y #> 413  n  n  n  y  y  y  n  n  n   y   n   y   y   y   n   y #> 415  y  y  y  n  n  n  y  y  y   n   n   n   n   n   n   y #> 417  y  y  n  y  y  y  n  n  n   y   n   n   y   y   n   y #> 418  y  y  y  n  n  n  y  y  y   y   y   n   y   n   n   y #> 419  y  y  y  n  n  n  y  y  n   y   n   n   n   n   n   y #> 420  y  y  y  n  n  n  y  y  y   n   n   n   n   n   n   y #> 421  y  y  y  y  y  y  y  y  n   y   n   n   y   y   n   y #> 422  n  y  y  n  y  y  y  y  n   n   y   n   y   n   y   y #> 423  n  n  y  n  n  y  y  y  y   n   y   n   n   n   y   y #> 424  n  y  y  n  n  y  y  y  y   n   y   n   n   y   y   y #> 427  y  n  y  n  n  n  y  y  y   y   n   n   n   n   y   y #> 428  n  n  n  y  y  y  y  y  n   y   n   y   y   y   n   y #> 431  n  n  y  y  y  y  n  n  y   y   n   y   y   y   n   y #> 432  n  n  y  n  n  n  y  y  y   y   n   n   n   n   n   y"},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-121","dir":"Changelog","previous_headings":"","what":"mlearning 1.2.1","title":"mlearning 1.2.1","text":"CRAN release: 2023-08-30 Documentation refactored using Roxygen2 considerably enhanced. camelCase function names now equivalence snake_case, e.g., mlRforest -> ml_rforest(), confusionImage() -> confusion_image() order adapt coding preferences user.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-120","dir":"Changelog","previous_headings":"","what":"mlearning 1.2.0","title":"mlearning 1.2.0","text":"mlRpart() function implements rpart::rpart() using decision trees.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-111","dir":"Changelog","previous_headings":"","what":"mlearning 1.1.1","title":"mlearning 1.1.1","text":"CRAN release: 2022-04-26 description extended. {pkgdown} site added.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-110","dir":"Changelog","previous_headings":"","what":"mlearning 1.1.0","title":"mlearning 1.1.0","text":"mlKnn() implemented K-nearest neighbors. Several adjustments required compatibility R 4.2.0 (allowed use vectors > 1 || &&).","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-107","dir":"Changelog","previous_headings":"","what":"mlearning 1.0.7","title":"mlearning 1.0.7","text":"predict() applied mlearning object build full formula (short one var ~ .), dependent variable newdata =, error message raised (although variable necessary point). Bug identified Damien Dumont, corrected.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-106","dir":"Changelog","previous_headings":"","what":"mlearning 1.0.6","title":"mlearning 1.0.6","text":"mlSvm.formula(), arguments scale=, type=, kernel= classwt= correctly used. Corrected.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-105","dir":"Changelog","previous_headings":"","what":"mlearning 1.0.5","title":"mlearning 1.0.5","text":"mlLvq() providing size = prior = led lvq object found message. Corrected.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-104","dir":"Changelog","previous_headings":"","what":"mlearning 1.0.4","title":"mlearning 1.0.4","text":"Sometimes, data found (e.g., called inside {learnr} tutorial). mlearning(), data forced .data.frame() (tibbles supported internally). mlXXX() function, possible indicate something like mlLda(data = iris, Species ~ .). Solved adding train = argument mlXXX(). summary.confusion() produced error one type = provided.","code":""},{"path":"https://www.sciviews.org/mlearning/news/index.html","id":"mlearning-103","dir":"Changelog","previous_headings":"","what":"mlearning 1.0.3","title":"mlearning 1.0.3","text":"NEWS.md file added. Repository moved GitHub.","code":""}]
